#!/bin/bash
# =============================================================================
# OMO (oh-my-ollama or Ollama Models Organizer)
# =============================================================================
#
# ü§ñ ÂäüËÉΩÊ¶ÇËßàÔºö
#   üì• Ê®°Âûã‰∏ãËΩΩÔºö
#       ‚Ä¢ ‰ªéOllamaÂÆòÊñπ‰ªìÂ∫ì‰∏ãËΩΩÊ®°Âûã
#       ‚Ä¢ ‰ªéHuggingFace‰ªìÂ∫ì‰∏ãËΩΩÊ®°ÂûãÂπ∂Ëá™Âä®ËΩ¨Êç¢ÈáèÂåñ
#       ‚Ä¢ Áõ¥Êé•‰∏ãËΩΩHuggingFaceÁöÑGGUFÊ†ºÂºèÊ®°Âûã
#       ‚Ä¢ ÊîØÊåÅÊñ≠ÁÇπÁª≠‰º†ÂíåÁºìÂ≠òÂ§çÁî®
#       ‚Ä¢ Êô∫ËÉΩHuggingFaceÈïúÂÉèÁ´ØÁÇπÊ£ÄÊµã
#
#   üíæ Ê®°ÂûãÂ§á‰ªΩÔºö
#       ‚Ä¢ ÂÆåÊï¥Â§á‰ªΩOllamaÊ®°ÂûãÔºàmanifest + blobsÔºâ
#       ‚Ä¢ Â§á‰ªΩHuggingFaceÂéüÂßãÊ®°ÂûãÊñá‰ª∂
#       ‚Ä¢ MD5Ê†°È™åÁ°Æ‰øùÊï∞ÊçÆÂÆåÊï¥ÊÄß
#       ‚Ä¢ ÁîüÊàêËØ¶ÁªÜÂ§á‰ªΩ‰ø°ÊÅØÊñá‰ª∂
#
#   üîÑ Ê®°ÂûãÊÅ¢Â§çÔºö
#       ‚Ä¢ ‰ªéÂ§á‰ªΩÊÅ¢Â§çOllamaÊ®°Âûã
#       ‚Ä¢ ÊÅ¢Â§çHuggingFaceÂéüÂßãÊ®°ÂûãÂà∞ÁºìÂ≠ò
#       ‚Ä¢ ÊîØÊåÅÂº∫Âà∂Ë¶ÜÁõñÊ®°Âºè
#       ‚Ä¢ Ëá™Âä®È™åËØÅÊñá‰ª∂ÂÆåÊï¥ÊÄß
#
#   üìã Ê®°ÂûãÁÆ°ÁêÜÔºö
#       ‚Ä¢ ÂàóÂá∫Â∑≤ÂÆâË£ÖÊ®°ÂûãÂèäËØ¶ÁªÜ‰ø°ÊÅØ
#       ‚Ä¢ Êô∫ËÉΩÂà†Èô§Ê®°ÂûãÔºàÂçï‰∏™/ÊâπÈáèÔºâ
#       ‚Ä¢ Ê®°ÂûãÂÆåÊï¥ÊÄßÊ£ÄÊü•ÂíåÈ™åËØÅ
#       ‚Ä¢ Á£ÅÁõò‰ΩøÁî®ÊÉÖÂÜµÁªüËÆ°
#
#   üê≥ ÂÆπÂô®ÂåñÈÉ®ÁΩ≤Ôºö
#       ‚Ä¢ ÁîüÊàêDocker ComposeÈÖçÁΩÆ
#       ‚Ä¢ ÈõÜÊàêOllama„ÄÅOne-API„ÄÅPrompt-OptimizerÁ≠âÊúçÂä°
#       ‚Ä¢ Ëá™Âä®GPUÊîØÊåÅÂíåÊó∂Âå∫ÈÖçÁΩÆ
#       ‚Ä¢ Êô∫ËÉΩÁ´ØÂè£ÂíåÁΩëÁªúÈÖçÁΩÆ
#
#   ‚öôÔ∏è  È´òÁ∫ßÁâπÊÄßÔºö
#       ‚Ä¢ ÊîØÊåÅËá™ÂÆö‰πâÈáèÂåñÁ±ªÂûãÔºàq4_0, q5_0, q8_0Á≠âÔºâ
#       ‚Ä¢ Âä®ÊÄÅDockerÈïúÂÉèÊûÑÂª∫
#       ‚Ä¢ Âπ∂Ë°åÂ§ÑÁêÜÂíåÁºìÂ≠ò‰ºòÂåñ
#       ‚Ä¢ ËØ¶ÁªÜÊó•ÂøóÂíåÈîôËØØÂ§ÑÁêÜ
#
# üìù ÊîØÊåÅÁöÑÊ®°ÂûãÊ†ºÂºèÔºö
#   ‚Ä¢ ollama [model]:[tag]     - OllamaÂÆòÊñπÊ®°Âûã
#   ‚Ä¢ huggingface [model] [quant] - HuggingFaceÊ®°Âûã(ÈúÄËΩ¨Êç¢)
#   ‚Ä¢ hf-gguf [model]:[tag]    - HuggingFace GGUFÊ®°Âûã(Áõ¥Êé•ÂØºÂÖ•)
#
# üîß ÁéØÂ¢ÉË¶ÅÊ±ÇÔºö
#   ‚Ä¢ Docker (ÊîØÊåÅGPUÂèØÈÄâ)
#   ‚Ä¢ Bash 4.0+
#   ‚Ä¢ curl, jq (Ëá™Âä®ÂÆâË£ÖÂà∞ÂÆπÂô®)
#
# üë®‚Äçüíª ‰ΩúËÄÖÔºöChain Lai
# üìñ ËØ¶ÁªÜ‰ΩøÁî®ËØ¥ÊòéËØ∑ËøêË°åÔºö./omo.sh --help
# =============================================================================

set -euo pipefail  # ÂêØÁî®‰∏•Ê†ºÁöÑÈîôËØØÂ§ÑÁêÜ

# ÂáΩÊï∞‰ºòÂåñÊèêÁ§∫ËØç
# ‰ºòÂåñÊ≠•È™§
#   1. ÂàÜÊûêÂáΩÊï∞ÔºöÊâæÂá∫Â§çÊùÇÂµåÂ•ó„ÄÅÈáçÂ§ç‰ª£Á†Å„ÄÅÂÜó‰ΩôÈÄªËæë
#   2. ÊèêÂèñËæÖÂä©ÂáΩÊï∞ÔºöÂ∞ÜÂ§çÊùÇÈÄªËæëÊãÜÂàÜ‰∏∫Áã¨Á´ãÂáΩÊï∞
#   3. ÂàõÂª∫Áªü‰∏ÄÊ°ÜÊû∂ÔºöÁî®ÈÄöÁî®Ê®°ÂºèÊõø‰ª£ÈáçÂ§çÁöÑÊù°‰ª∂ÂàÜÊîØ
#   4. ‰ºòÂåñËæìÂá∫ÔºöÁªü‰∏ÄËæìÂá∫Ê†ºÂºèÔºåÊ∂àÈô§ÈáçÂ§ç‰ø°ÊÅØ
#   5. È™åËØÅÊïàÊûúÔºöËØ≠Ê≥ïÊ£ÄÊü• + ÂäüËÉΩÊµãËØï

#==============================================================================
# ÂÖ®Â±ÄÈÖçÁΩÆÂíåÂèòÈáèÂÆö‰πâ
#==============================================================================
SCRIPT_DIR=""
readonly SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]:-$0}")" && pwd)"
readonly MODELS_LIST_FILE="${SCRIPT_DIR}/models.list"
# Âü∫Á°ÄË∑ØÂæÑÈÖçÁΩÆÔºàÂèØÂú®mainÂáΩÊï∞‰∏≠Ë¢´Ë¶ÜÁõñÔºâ
OLLAMA_DATA_DIR="${SCRIPT_DIR}/ollama"
OLLAMA_MODELS_DIR="${OLLAMA_DATA_DIR}/models"
BACKUP_OUTPUT_DIR="${SCRIPT_DIR}/backups"
HF_DOWNLOAD_CACHE_DIR="${SCRIPT_DIR}/hf_download_cache"
HF_ORIGINAL_BACKUP_DIR="${SCRIPT_DIR}/hf_originals"

# È¢ÑËÆ°ÁÆóÁöÑÁªùÂØπË∑ØÂæÑÔºàÊÄßËÉΩ‰ºòÂåñÔºâ
ABS_OLLAMA_DATA_DIR=""
ABS_HF_DOWNLOAD_CACHE_DIR=""
ABS_HF_ORIGINAL_BACKUP_DIR=""

# HuggingFaceÈïúÂÉèÈÖçÁΩÆ
HF_ENDPOINT=""  # ÂàùÂßã‰∏∫Á©∫Ôºå‰ºöÂú®ÈúÄË¶ÅÊó∂Âä®ÊÄÅÊ£ÄÊµãÊúÄ‰ºòÁ´ØÁÇπ


# DockerÈïúÂÉèÈÖçÁΩÆ
readonly DOCKER_IMAGE_LLAMA_CPP="ghcr.io/ggml-org/llama.cpp:full-cuda"
readonly DOCKER_IMAGE_OLLAMA="ollama/ollama:latest"
readonly DOCKER_IMAGE_ONE_API="justsong/one-api:latest"
readonly DOCKER_IMAGE_PROMPT_OPTIMIZER="linshen/prompt-optimizer:latest"
readonly DOCKER_IMAGE_CHATGPT_NEXT_WEB="yidadaa/chatgpt-next-web:latest"

# Â§á‰ªΩÈÖçÁΩÆ

# ËøêË°åÊó∂ÈÖçÁΩÆ
VERBOSE="false"  # ËØ¶ÁªÜÊ®°ÂºèÂºÄÂÖ≥

#==============================================================================
# Â∑•ÂÖ∑ÂáΩÊï∞
#==============================================================================

# ÊòæÁ§∫ÂÆπÂô®Êó•ÂøóÁöÑÂ∑•ÂÖ∑ÂáΩÊï∞
show_container_logs() {
    local container_name="$1"
    log_error "ÂÆπÂô®Êó•Âøó:"
    docker logs "$container_name" 2>&1 | tail -10
}

# Ëé∑Âèñ‰∏ªÊú∫Êó∂Âå∫
get_host_timezone() {
    # Â∞ùËØïÂ§öÁßçÊñπÊ≥ïËé∑Âèñ‰∏ªÊú∫Êó∂Âå∫
    if command_exists timedatectl; then
        # ‰ºòÂÖà‰ΩøÁî® timedatectlÔºàsystemd Á≥ªÁªüÔºâ
        timedatectl show --property=Timezone --value 2>/dev/null
    elif [[ -L /etc/localtime ]]; then
        # ÈÄöËøáÁ¨¶Âè∑ÈìæÊé•Ëé∑ÂèñÊó∂Âå∫
        readlink /etc/localtime | sed 's|.*/zoneinfo/||'
    elif [[ -f /etc/timezone ]]; then
        # ‰ªé /etc/timezone Êñá‰ª∂ËØªÂèñ
        cat /etc/timezone
    else
        # ÈªòËÆ§ÂõûÈÄÄÂà∞ UTC
        echo "UTC"
    fi
}

#==============================================================================
# DockerÈõÜÊàêÊ®°ÂùóÔºàHuggingFaceÊ®°ÂûãËΩ¨Êç¢Ôºâ
#==============================================================================
readonly IMAGE_NAME="hf_downloader"
readonly IMAGE_TAG="latest"
readonly FULL_IMAGE_NAME="${IMAGE_NAME}:${IMAGE_TAG}"

# DockerÈõÜÊàêÂÜÖÂÆπÔºàÂµåÂÖ•ÂºèÊñá‰ª∂Ôºâ
# ÂàõÂª∫‰∏¥Êó∂ÊûÑÂª∫ÁõÆÂΩïÂπ∂ÂÜôÂÖ•ÂøÖË¶ÅÊñá‰ª∂
create_docker_build_context() {
    local build_dir="$1"
    
    mkdir -p "$build_dir"
    
    # Ëé∑Âèñ‰∏ªÊú∫Êó∂Âå∫
    local host_timezone=$(get_host_timezone)
    [[ -z "$host_timezone" ]] && host_timezone="UTC"
    
    # ÂÜôÂÖ•Dockerfile
    cat > "$build_dir/Dockerfile" << EOF
FROM $DOCKER_IMAGE_LLAMA_CPP
WORKDIR /app
ENV DEBIAN_FRONTEND=noninteractive TZ=${host_timezone}
RUN apt-get update && apt-get install -y --no-install-recommends curl aria2 jq tzdata && \
    apt-get clean && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* && \
    ln -snf /usr/share/zoneinfo/${host_timezone} /etc/localtime && echo ${host_timezone} > /etc/timezone && \
    curl -fsSL https://hf-mirror.com/hfd/hfd.sh -o /app/hfd.sh && chmod +x /app/hfd.sh
COPY convert_model.sh /app/
RUN chmod +x /app/convert_model.sh
ENTRYPOINT ["/app/convert_model.sh"]
EOF
    
    # ÂÜôÂÖ•convert_model.sh
    cat > "$build_dir/convert_model.sh" << 'EOF'
#!/bin/bash
set -euo pipefail
# ÁÆÄÂåñÊó•ÂøóÂáΩÊï∞ÔºàDockerÁéØÂ¢É‰∏ìÁî®Ôºâ
log_info() { printf "[INFO] %s\n" "$1" >&2; }
log_success() { printf "[SUCCESS] %s\n" "$1" >&2; }
log_error() { printf "[ERROR] %s\n" "$1" >&2; }
run_command() {
    local description="$1" verbose="$2"
    shift 2
    log_info "$description"
    if [[ "$verbose" == "true" ]]; then
        "$@"
    else
        if "$@" >/dev/null 2>&1; then
            log_success "${description}ÂÆåÊàê"
        else
            log_error "${description}Â§±Ë¥•"
            "$@" 2>&1 | tail -5
            exit 1
        fi
    fi
}
download_model() {
    local model_name="$1" download_dir="$2"
    # log_info "‰∏ãËΩΩÊ®°Âûã: $model_name"
    if [[ -n "${HF_ENDPOINT:-}" ]]; then
        export HF_ENDPOINT="${HF_ENDPOINT}"
    fi
    local cache_dir="/app/download_cache"
    if [[ -d "$cache_dir" ]]; then
        local model_safe_name=$(echo "$model_name" | sed 's/[\/:]/_/g')
        local cached_model_dir="${cache_dir}/${model_safe_name}"
        
        # Ê£ÄÊü•ÊòØÂê¶ÊúâÊú™ÂÆåÊàêÁöÑ‰∏ãËΩΩÔºàÂ≠òÂú®.aria2Êñá‰ª∂Ôºâ
        if [[ -d "$cached_model_dir" ]] && [[ -n "$(find "$cached_model_dir" -name "*.aria2" 2>/dev/null)" ]]; then
            log_info "Ê£ÄÊµãÂà∞Êú™ÂÆåÊàêÁöÑ‰∏ãËΩΩÔºåÁªßÁª≠‰∏ãËΩΩ..."
            export ARIA2C_OPTS="--continue=true --max-tries=10 --retry-wait=3 --split=8 --max-connection-per-server=8 --auto-file-renaming=false"
            if /app/hfd.sh "$model_name" --local-dir "$cached_model_dir" --tool aria2c; then
                rm -f "$cached_model_dir"/*.aria2 2>/dev/null || true
                if [[ -n "$(ls -A "$cached_model_dir" 2>/dev/null)" ]]; then
                    cp -r "$cached_model_dir"/* "$download_dir"/ 2>/dev/null || true
                fi
            else
                log_error "Ê®°Âûã‰∏ãËΩΩÂ§±Ë¥•"
                exit 1
            fi
        elif [[ -d "$cached_model_dir" ]] && [[ -n "$(ls -A "$cached_model_dir" 2>/dev/null)" ]]; then
            log_info "‰ΩøÁî®Â∑≤ÁºìÂ≠òÁöÑÂÆåÊï¥Ê®°Âûã"
            if [[ -n "$(ls -A "$cached_model_dir" 2>/dev/null)" ]]; then
                cp -r "$cached_model_dir"/* "$download_dir"/ 2>/dev/null || true
            fi
            return 0
        else
            # ÂÖ®Êñ∞‰∏ãËΩΩ
            mkdir -p "$cached_model_dir"
            export ARIA2C_OPTS="--continue=true --max-tries=10 --retry-wait=3 --split=8 --max-connection-per-server=8 --auto-file-renaming=false"
            if /app/hfd.sh "$model_name" --local-dir "$cached_model_dir" --tool aria2c; then
                rm -f "$cached_model_dir"/*.aria2 2>/dev/null || true
                if [[ -n "$(ls -A "$cached_model_dir" 2>/dev/null)" ]]; then
                    cp -r "$cached_model_dir"/* "$download_dir"/ 2>/dev/null || true
                fi
            else
                log_error "Ê®°Âûã‰∏ãËΩΩÂ§±Ë¥•"
                exit 1
            fi
        fi
    else
        if ! /app/hfd.sh "$model_name" --local-dir "$download_dir" --tool aria2c; then
            log_error "Ê®°Âûã‰∏ãËΩΩÂ§±Ë¥•"
            exit 1
        fi
    fi
}
convert_to_gguf() {
    local model_dir="$1" output_file="$2" verbose="$3"
    run_command "ËΩ¨Êç¢‰∏∫GGUFÊ†ºÂºè" "$verbose" \
        python3 /app/convert_hf_to_gguf.py "$model_dir" --outfile "$output_file" --outtype f16
}
quantize_model() {
    local input_file="$1" output_file="$2" quantize_type="$3" verbose="$4"
    run_command "ÈáèÂåñÊ®°Âûã (${quantize_type})" "$verbose" \
        /app/llama-quantize "$input_file" "$output_file" "$quantize_type"
    rm -f "$input_file"
}
convert_main() {
    local model_name="" quantize_type="q4_0" gguf_dir="/app/models" verbose=false
    while [[ $# -gt 0 ]]; do
        case $1 in
            --quantize)
                [[ -z "${2:-}" ]] && { log_error "Áº∫Â∞ë --quantize ÂèÇÊï∞ÂÄº"; exit 1; }
                quantize_type="$2"; shift 2 ;;
            --gguf-dir)
                [[ -z "${2:-}" ]] && { log_error "Áº∫Â∞ë --gguf-dir ÂèÇÊï∞ÂÄº"; exit 1; }
                gguf_dir="$2"; shift 2 ;;
            --verbose) verbose=true; shift ;;
            -*) log_error "Êú™Áü•ÂèÇÊï∞: $1"; exit 1 ;;
            *)
                if [[ -z "$model_name" ]]; then
                    model_name="$1"
                else
                    log_error "Â§ö‰ΩôÁöÑÂèÇÊï∞: $1"; exit 1
                fi
                shift ;;
        esac
    done
    if [[ -z "$model_name" ]]; then
        log_error "Áº∫Â∞ëÊ®°ÂûãÂêçÁß∞ÂèÇÊï∞"; exit 1
    fi
    log_info "Â§ÑÁêÜÊ®°Âûã: $model_name (${quantize_type})"
    mkdir -p "$gguf_dir"
    local temp_dir="/tmp/model_download_$$"
    mkdir -p "$temp_dir"
    local model_basename=$(echo "$model_name" | sed 's/\//-/g')
    local final_gguf_file="${gguf_dir}/${model_basename}-${quantize_type}.gguf"
    if [[ -f "$final_gguf_file" ]]; then
        log_info "ËæìÂá∫Êñá‰ª∂Â∑≤Â≠òÂú®ÔºåË∑≥ËøáËΩ¨Êç¢"; exit 0
    fi
    local temp_gguf_file="${temp_dir}/${model_basename}.gguf"
    download_model "$model_name" "$temp_dir"
    convert_to_gguf "$temp_dir" "$temp_gguf_file" "$verbose"
    quantize_model "$temp_gguf_file" "$final_gguf_file" "$quantize_type" "$verbose"
    log_success "ËΩ¨Êç¢ÂÆåÊàê: $final_gguf_file"
    if [[ -f "$final_gguf_file" ]]; then
        local file_size=$(du -h "$final_gguf_file" | cut -f1)
        log_info "Êñá‰ª∂Â§ßÂ∞è: $file_size"
    fi
}
convert_main "$@"
EOF
    
    chmod +x "$build_dir/convert_model.sh"
    log_verbose_success "DockerÊûÑÂª∫‰∏ä‰∏ãÊñáÂàõÂª∫ÂÆåÊàê"
}

# Ê∏ÖÁêÜ‰∏¥Êó∂ÊûÑÂª∫ÁõÆÂΩï
cleanup_docker_build_context() {
    local build_dir="$1"
    if [[ -d "$build_dir" ]]; then
        rm -rf "$build_dir"
    fi
}

# È¢úËâ≤ÂÆö‰πâ
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly CYAN='\033[0;36m'
readonly MAGENTA='\033[0;95m'
readonly NC='\033[0m' # No Color

#==============================================================================
# ‰ªªÂä°ÊâßË°åÊ®°Âùó  
#==============================================================================
# Áªü‰∏ÄÁöÑ‰ªªÂä°ÊâßË°åÂáΩÊï∞
execute_task() {
    local task_name="$1"
    local task_function="$2"
    shift 2
    local task_args=("$@")
    
    log_info "ÊâßË°å${task_name}..."
    if "${task_function}" "${task_args[@]}"; then
        log_success "${task_name}ÂÆåÊàê"
        exit 0
    else
        local exit_code=$?
        if [[ $exit_code -eq 2 ]]; then
            # Áî®Êà∑ÂèñÊ∂àÊìç‰ΩúÔºå‰∏çÊòæÁ§∫ÈîôËØØ‰ø°ÊÅØ
            exit 0
        else
            log_error "${task_name}Â§±Ë¥•"
            exit 1
        fi
    fi
}

# Â§á‰ªΩÂçï‰∏™Ê®°ÂûãÁöÑÂåÖË£ÖÂáΩÊï∞
backup_single_model() {
    local backup_model="$1"
    local backup_dir="$2"
    
    # Â§ÑÁêÜ‰∏çÂêåÁ±ªÂûãÁöÑÊ®°ÂûãÂâçÁºÄ
    local model_to_backup="$backup_model"
    if [[ "$backup_model" =~ ^hf-gguf:(.+)$ ]]; then
        model_to_backup="${BASH_REMATCH[1]}"
    elif [[ "$backup_model" =~ ^ollama:(.+)$ ]]; then
        model_to_backup="${BASH_REMATCH[1]}"
    elif [[ "$backup_model" =~ ^huggingface:([^:]+):(.+)$ ]]; then
        # HuggingFaceÊ®°ÂûãÈúÄË¶ÅËΩ¨Êç¢‰∏∫OllamaÊ†ºÂºè
        local model_name="${BASH_REMATCH[1]}"
        local quantize_type="${BASH_REMATCH[2]}"
        model_to_backup=$(generate_ollama_model_name "$model_name" "$quantize_type")
    fi
    
    backup_ollama_model "$model_to_backup" "$backup_dir"
}

# ÊÅ¢Â§çÊ®°ÂûãÁöÑÂåÖË£ÖÂáΩÊï∞
restore_model() {
    local restore_file="$1"
    local force_restore="$2"
    
    # Â¶ÇÊûúÊÅ¢Â§çÊñá‰ª∂‰∏çÊòØÁªùÂØπË∑ØÂæÑÔºåÂàôÂú®BACKUP_OUTPUT_DIR‰∏≠Êü•Êâæ
    local restore_path="$restore_file"
    if [[ "$restore_file" != /* ]]; then
        restore_path="$BACKUP_OUTPUT_DIR/$restore_file"
    fi
    
    restore_ollama_model "$restore_path" "$force_restore"
}

# Ê®°ÂûãÂ§ÑÁêÜÂô® - Ëß£ÊûêÊ®°ÂûãÊù°ÁõÆÂπ∂ËøîÂõûÂ§ÑÁêÜÂáΩÊï∞
parse_model_entry() {
    local model_entry="$1"
    local -n result_ref="$2"
    
    # Ê∏ÖÁ©∫ÁªìÊûúÊï∞ÁªÑ
    result_ref=()
    
    if [[ "$model_entry" =~ ^ollama:([^:]+):(.+)$ ]]; then
        result_ref[type]="ollama"
        result_ref[name]="${BASH_REMATCH[1]}"
        result_ref[tag]="${BASH_REMATCH[2]}"
        result_ref[display]="${result_ref[name]}:${result_ref[tag]} (Ollama)"
        
    elif [[ "$model_entry" =~ ^huggingface:([^:]+):(.+)$ ]]; then
        result_ref[type]="huggingface"
        result_ref[name]="${BASH_REMATCH[1]}"
        result_ref[quantize]="${BASH_REMATCH[2]}"
        result_ref[display]="${result_ref[name]} (ÈáèÂåñ: ${result_ref[quantize]})"
        
    elif [[ "$model_entry" =~ ^hf-gguf:(.+)$ ]]; then
        result_ref[type]="hf-gguf"
        local model_full_name="${BASH_REMATCH[1]}"
        if [[ "$model_full_name" =~ ^(.+):(.+)$ ]]; then
            result_ref[name]="${BASH_REMATCH[1]}"
            result_ref[tag]="${BASH_REMATCH[2]}"
        else
            result_ref[name]="$model_full_name"
            result_ref[tag]="latest"
        fi
        result_ref[display]="${result_ref[name]}:${result_ref[tag]} (HF-GGUF)"
    else
        return 1
    fi
    
    return 0
}

# Ê£ÄÊü•Ê®°ÂûãÊòØÂê¶Â≠òÂú®
check_model_exists() {
    local -n model_info_ref=$1
    
    case "${model_info_ref[type]}" in
        "ollama")
            check_ollama_model "${model_info_ref[name]}" "${model_info_ref[tag]}"
            ;;
        "huggingface")
            check_huggingface_model_in_ollama "${model_info_ref[name]}" "${model_info_ref[quantize]}"
            ;;
        "hf-gguf")
            check_hf_gguf_model "${model_info_ref[name]}" "${model_info_ref[tag]}"
            ;;
        *)
            return 1
            ;;
    esac
}

# ‰∏ãËΩΩÊ®°Âûã
download_model() {
    local -n model_info_ref=$1
    
    case "${model_info_ref[type]}" in
        "ollama")
            download_ollama_model "${model_info_ref[name]}" "${model_info_ref[tag]}"
            ;;
        "huggingface")
            download_huggingface_model "${model_info_ref[name]}" "${model_info_ref[quantize]}"
            ;;
        "hf-gguf")
            download_hf_gguf_model "${model_info_ref[name]}" "${model_info_ref[tag]}"
            ;;
        *)
            return 1
            ;;
    esac
}

# Â∞ùËØï‰ªéÂ§á‰ªΩÊÅ¢Â§çÊ®°Âûã
try_restore_model() {
    local -n model_info_ref=$1
    
    case "${model_info_ref[type]}" in
        "ollama")
            try_restore_ollama_from_backup "${model_info_ref[name]}" "${model_info_ref[tag]}"
            ;;
        "hf-gguf")
            try_restore_ollama_from_backup "${model_info_ref[name]}" "${model_info_ref[tag]}"
            ;;
        "huggingface")
            # HuggingFaceÊ®°ÂûãÁöÑÊÅ¢Â§çÈÄªËæëËæÉÂ§çÊùÇÔºåÂÖàÂ∞ùËØïOllamaÂ§á‰ªΩÔºåÂÜçÂ∞ùËØïÂéüÂßãÂ§á‰ªΩ
            local expected_ollama_name=$(generate_ollama_model_name "${model_info_ref[name]}" "${model_info_ref[quantize]}")
            local ollama_model_name="${expected_ollama_name%:*}"
            local ollama_model_tag="${expected_ollama_name#*:}"
            
            if try_restore_ollama_from_backup "$ollama_model_name" "$ollama_model_tag"; then
                return 0
            fi
            
            # Â∞ùËØï‰ªéÂéüÂßãÂ§á‰ªΩÊÅ¢Â§ç
            if try_restore_hf_from_original "${model_info_ref[name]}"; then
                log_verbose "‰ªéÂéüÂßãÂ§á‰ªΩÊÅ¢Â§çÔºåÂºÄÂßãËΩ¨Êç¢..."
                if restore_and_reconvert_hf_model "${model_info_ref[name]}" "${model_info_ref[quantize]}" "true"; then
                    return 0
                fi
            fi
            return 1
            ;;
        *)
            return 1
            ;;
    esac
}

#==============================================================================
# Êó•ÂøóÁ≥ªÁªüÊ®°Âùó
#==============================================================================
# Êó•ÂøóËßÑÂàô:
# 1. ‰∏ªÊµÅÁ®ãÂáΩÊï∞: ‰ΩøÁî®Ê†áÂáÜÊó•ÂøóÂáΩÊï∞(log_info, log_success, log_warning, log_error)
#    - Âú®ÊôÆÈÄöÊ®°ÂºèÂíåverboseÊ®°Âºè‰∏ãÈÉΩÊòæÁ§∫, Áî®‰∫éÁî®Êà∑ÂÖ≥ÂøÉÁöÑÊ†∏ÂøÉÊìç‰ΩúËøõÂ∫¶
# 2. Â∑•ÂÖ∑ÂáΩÊï∞: Ê≠£Â∏∏ËøΩË∏™Êó•Âøó‰ΩøÁî®verboseÁâàÊú¨(log_verbose, log_verbose_success)
#    - ‰ªÖÂú®verboseÊ®°ÂºèÊòæÁ§∫, Ë≠¶ÂëäÂíåÈîôËØØ(log_warning, log_error)Âú®‰ªª‰ΩïÊ®°ÂºèÈÉΩÊòæÁ§∫
# 3. ÈÅøÂÖçÊó•ÂøóÈáçÂ§ç: Â∑•ÂÖ∑ÂáΩÊï∞ÁöÑËøΩË∏™‰ø°ÊÅØÂè™Âú®verboseÊ®°ÂºèÊòæÁ§∫, ‰∏ªÊµÅÁ®ã‰øùÊåÅÁÆÄÊ¥Å
#==============================================================================

log_info() {
    printf "${BLUE}[INFO]${NC} %s\n" "$1"
}

log_success() {
    printf "${GREEN}[SUCCESS]${NC} %s\n" "$1"
}

log_warning() {
    printf "${YELLOW}[WARNING]${NC} %s\n" "$1"
}

log_error() {
    printf "${RED}[ERROR]${NC} %s\n" "$1"
}


# Verbose-only logging functions
log_verbose() {
    if [[ "${VERBOSE}" == "true" ]]; then
        printf "${BLUE}[INFO]${NC} %s\n" "$1"
    fi
    return 0
}

log_verbose_success() {
    [[ "${VERBOSE}" == "true" ]] && printf "${GREEN}[SUCCESS]${NC} %s\n" "$1"
    return 0
}

log_verbose_warning() {
    [[ "${VERBOSE}" == "true" ]] && printf "${YELLOW}[WARNING]${NC} %s\n" "$1"
    return 0
}


# HuggingFaceÁ´ØÁÇπÊô∫ËÉΩÊ£ÄÊµãÂáΩÊï∞
detect_optimal_hf_endpoint() {
    # Â¶ÇÊûúÂ∑≤ÁªèÊ£ÄÊµãËøáÔºåÁõ¥Êé•ËøîÂõû
    if [[ "$HF_ENDPOINT_DETECTED" == "true" ]]; then
        return 0
    fi
    
    local cache_file="/tmp/.hf_endpoint_cache"
    local cache_timeout=3600  # ÁºìÂ≠ò1Â∞èÊó∂
    
    # Ê£ÄÊü•ÁºìÂ≠òÊòØÂê¶ÊúâÊïà
    if [[ -f "$cache_file" ]]; then
        local cache_time=$(stat -c %Y "$cache_file" 2>/dev/null || echo 0)
        local current_time=$(date +%s)
        if [[ $((current_time - cache_time)) -lt $cache_timeout ]]; then
            HF_ENDPOINT=$(cat "$cache_file")
            HF_ENDPOINT_DETECTED="true"
            log_verbose "‰ΩøÁî®ÁºìÂ≠òÁöÑHuggingFaceÁ´ØÁÇπ: $HF_ENDPOINT"
            return 0
        fi
    fi
    
    local hf_official="https://huggingface.co"
    local hf_mirror="https://hf-mirror.com"
    local timeout=3
    
    log_verbose "Ê£ÄÊµãÊúÄ‰ºòHuggingFaceÁ´ØÁÇπ..."
    
    # ÊµãËØïÂçï‰∏™Á´ØÁÇπÁöÑÂáΩÊï∞
    test_endpoint() {
        local endpoint="$1"
        local host=$(echo "$endpoint" | sed 's|https\?://||' | cut -d'/' -f1)
        
        # ‰ΩøÁî®pingÊµãËØïËøûÈÄöÊÄßÂíåÂª∂Ëøü
        local ping_result=$(ping -c 1 -W $timeout "$host" 2>/dev/null | grep 'time=' | sed 's/.*time=\([0-9.]*\).*/\1/')
        
        if [[ -n "$ping_result" ]]; then
            # Â∞ÜÂª∂ËøüËΩ¨Êç¢‰∏∫ÊØ´ÁßíÊï¥Êï∞
            local latency=$(echo "$ping_result" | cut -d'.' -f1)
            [[ -z "$latency" || ! "$latency" =~ ^[0-9]+$ ]] && latency=0
            echo "$endpoint|$latency"  # ‰ΩøÁî® | ÂàÜÈöîÁ¨¶ÈÅøÂÖç‰∏éURL‰∏≠ÁöÑ:ÂÜ≤Á™Å
        else
            echo "$endpoint|999999"  # Ë°®Á§∫Êó†Ê≥ïËÆøÈóÆ
        fi
    }
    
    # Âπ∂Ë°åÊµãËØï
    local official_result="" mirror_result=""
    {
        official_result=$(test_endpoint "$hf_official")
    } &
    local pid1=$!
    
    {
        mirror_result=$(test_endpoint "$hf_mirror")
    } &
    local pid2=$!
    
    # Á≠âÂæÖÊµãËØïÂÆåÊàê
    wait $pid1 $pid2
    
    # Ëß£ÊûêÁªìÊûú
    local official_latency="${official_result#*|}"
    local mirror_latency="${mirror_result#*|}"
    
    # Êî∂ÈõÜÂèØÁî®Á´ØÁÇπ
    local available_endpoints=()
    [[ "$official_latency" != "999999" ]] && available_endpoints+=("$hf_official|$official_latency")
    [[ "$mirror_latency" != "999999" ]] && available_endpoints+=("$hf_mirror|$mirror_latency")
    
    # Ê£ÄÊü•ÊòØÂê¶ÊúâÂèØÁî®Á´ØÁÇπ
    if [[ ${#available_endpoints[@]} -eq 0 ]]; then
        log_error "Êó†Ê≥ïËÆøÈóÆ‰ªª‰ΩïHuggingFaceÁ´ØÁÇπÔºåËÑöÊú¨‰∏≠Ê≠¢"
        exit 1
    fi
    
    # ÈÄâÊã©Âª∂ËøüÊúÄ‰ΩéÁöÑÁ´ØÁÇπ
    local best_endpoint=""
    local best_latency=999999
    for endpoint_info in "${available_endpoints[@]}"; do
        local endpoint="${endpoint_info%|*}"
        local latency="${endpoint_info#*|}"
        if [[ -n "$latency" && "$latency" =~ ^[0-9]+$ && $latency -lt $best_latency ]]; then
            best_latency=$latency
            best_endpoint=$endpoint
        fi
    done
    
    local selected_endpoint="$best_endpoint"
    log_verbose "ÈÄâÊã©ÊúÄ‰ºòÁ´ØÁÇπ: $selected_endpoint (${best_latency}ms)"
    
    # Êõ¥Êñ∞ÂÖ®Â±ÄÂèòÈáèÂπ∂ÁºìÂ≠òÁªìÊûú
    HF_ENDPOINT="$selected_endpoint"
    HF_ENDPOINT_DETECTED="true"
    echo "$selected_endpoint" > "$cache_file"
    
    return 0
}

# Ê†ºÂºèÂåñÂ≠óËäÇÂ§ßÂ∞è‰∏∫‰∫∫Á±ªÂèØËØªÊ†ºÂºè
format_bytes() {
    local bytes="$1"
    
    # ‰ΩøÁî®ÂçïÊ¨°awkË∞ÉÁî®ÂáèÂ∞ëÂºÄÈîÄÔºåÈ¢ÑÂÆö‰πâÂ∏∏ÈáèÊèêÈ´òÂèØËØªÊÄß
    awk -v b="$bytes" '
    BEGIN {
        if (b >= 1073741824) printf "%.1fGB", b / 1073741824
        else if (b >= 1048576) printf "%.1fMB", b / 1048576  
        else printf "%.1fKB", b / 1024
    }'
}

# È™åËØÅÊ®°ÂûãÊ†ºÂºèÊòØÂê¶Ê≠£Á°Æ
validate_model_format() {
    local model_spec="$1"
    if [[ "$model_spec" != *":"* ]]; then
        log_error "Ê®°ÂûãÊ†ºÂºèÈîôËØØÔºåÂ∫î‰∏∫ 'Ê®°ÂûãÂêç:ÁâàÊú¨'Ôºå‰æãÂ¶Ç 'llama2:7b'"
        return 1
    fi
    return 0
}

# Á≠âÂæÖOllamaÂÆπÂô®Â∞±Áª™
wait_for_ollama_ready() {
    local container_name="$1"
    local max_attempts=120  # Â¢ûÂä†Âà∞120Áßí
    local attempt=0
    
    log_verbose "Á≠âÂæÖOllamaÊúçÂä°ÂêØÂä®..."
    
    while (( attempt < max_attempts )); do
        # È¶ñÂÖàÊ£ÄÊü•ÂÆπÂô®ÊòØÂê¶ËøòÂú®ËøêË°å
        if ! docker ps -q --filter "name=^${container_name}$" | grep -q .; then
            log_error "ÂÆπÂô® $container_name Â∑≤ÂÅúÊ≠¢ËøêË°å"
            show_container_logs "$container_name"
            return 1
        fi
        
        # Ê£ÄÊü•ollamaÊúçÂä°ÊòØÂê¶Â∞±Áª™
        if docker exec "$container_name" ollama list &>/dev/null; then
            log_verbose_success "OllamaÊúçÂä°Â∑≤Â∞±Áª™"
            return 0
        fi
        
        # ÊØè10ÁßíÊòæÁ§∫‰∏ÄÊ¨°ËøõÂ∫¶
        if (( attempt % 10 == 0 && attempt > 0 )); then
            log_verbose "Á≠âÂæÖ‰∏≠... ($attempt/$max_attempts Áßí)"
        fi
        
        sleep 1
        ((attempt++))
    done
    
    log_error "Á≠âÂæÖOllamaÊúçÂä°Â∞±Áª™Ë∂ÖÊó∂ ($max_attempts Áßí)"
    show_container_logs "$container_name"
    return 1
}

# ÊûÑÂª∫ÂÆåÊï¥ÁöÑDockerËøêË°åÂëΩ‰ª§
build_full_docker_cmd() {
    local container_name="$1"
    local use_gpu="${2:-true}"
    local include_hf_token="${3:-false}"
    local extra_env=()
    local extra_volumes=()
    
    # Â§ÑÁêÜÈ¢ùÂ§ñÁöÑÁéØÂ¢ÉÂèòÈáèÂíåÊåÇËΩΩÂç∑ÂèÇÊï∞
    shift 3
    while [[ $# -gt 0 ]]; do
        case "$1" in
            --env)
                extra_env+=("$2")
                shift 2
                ;;
            --volume)
                extra_volumes+=("$2")
                shift 2
                ;;
            *)
                break
                ;;
        esac
    done
    
    local docker_cmd=("docker" "run" "--name" "$container_name" "--rm" "-t")
    
    # GPUÊîØÊåÅ
    if [[ "$use_gpu" == "true" ]]; then
        docker_cmd+=("--gpus" "all")
    fi
    
    # HF TokenÊîØÊåÅ
    if [[ "$include_hf_token" == "true" && -n "${HF_TOKEN:-}" ]]; then
        docker_cmd+=("-e" "HF_TOKEN=${HF_TOKEN}")
    fi
    
    # Âü∫Á°ÄÁéØÂ¢ÉÂèòÈáè
    docker_cmd+=("-e" "HF_ENDPOINT=${HF_ENDPOINT:-https://hf-mirror.com}")
    docker_cmd+=("-e" "PYTHONUNBUFFERED=1")
    docker_cmd+=("-e" "TERM=xterm-256color")
    docker_cmd+=("-v" "/etc/localtime:/etc/localtime:ro")
    docker_cmd+=("-e" "TZ=${HOST_TIMEZONE:-UTC}")
    
    # Ê∑ªÂä†È¢ùÂ§ñÁöÑÁéØÂ¢ÉÂèòÈáè
    for env_var in "${extra_env[@]}"; do
        docker_cmd+=("-e" "$env_var")
    done
    
    # Ê∑ªÂä†È¢ùÂ§ñÁöÑÊåÇËΩΩÂç∑
    for volume in "${extra_volumes[@]}"; do
        docker_cmd+=("-v" "$volume")
    done
    
    printf '%s\n' "${docker_cmd[@]}"
}

# ÈÄöÁî®cleanupÂáΩÊï∞ÁîüÊàêÂô®
create_cleanup_function() {
    local cleanup_name="$1"
    shift
    local cleanup_items_str="$*"
    
    # Âä®ÊÄÅÂàõÂª∫cleanupÂáΩÊï∞
    eval "${cleanup_name}() {
        local item
        local cleanup_items=($cleanup_items_str)
        for item in \"\${cleanup_items[@]}\"; do
            if [[ -f \"\$item\" ]]; then
                rm -f \"\$item\"
            elif [[ -d \"\$item\" ]]; then
                rm -rf \"\$item\"
            elif [[ \"\$item\" =~ ^[a-zA-Z0-9_-]+\$ ]]; then
                # ÂÅáËÆæÊòØÂÆπÂô®Âêç
                docker rm -f \"\$item\" &>/dev/null || true
            fi
        done
    }"
}

# ËÆæÁΩÆcleanup trapÁöÑÈÄöÁî®ÂáΩÊï∞
setup_cleanup_trap() {
    local cleanup_function="$1"
    local signals="${2:-EXIT INT TERM}"
    trap '$cleanup_function' "$signals"
}

# OllamaÊ®°ÂûãÂàóË°®ÁºìÂ≠ò
declare -g OLLAMA_MODELS_CACHE=""
declare -g OLLAMA_CACHE_INITIALIZED="false"

# ‰∏¥Êó∂OllamaÂÆπÂô®ÁÆ°ÁêÜ
declare -g TEMP_OLLAMA_CONTAINER=""
declare -g EXISTING_OLLAMA_CONTAINER=""

# ÂÖ®Â±ÄÊ∏ÖÁêÜÂáΩÊï∞ÁÆ°ÁêÜ
declare -g GLOBAL_CLEANUP_FUNCTIONS=()
declare -g GLOBAL_CLEANUP_INITIALIZED="false"

# HuggingFaceÁ´ØÁÇπÊ£ÄÊµãÁä∂ÊÄÅÁÆ°ÁêÜ
declare -g HF_ENDPOINT_DETECTED="false"

# ÂÖ®Â±ÄÊ∏ÖÁêÜÂáΩÊï∞ÁÆ°ÁêÜ
add_cleanup_function() {
    local func_name="$1"
    if [[ -z "$func_name" ]]; then
        log_error "Ê∏ÖÁêÜÂáΩÊï∞ÂêçÁß∞‰∏çËÉΩ‰∏∫Á©∫"
        return 1
    fi
    
    # Ê£ÄÊü•ÂáΩÊï∞ÊòØÂê¶Â∑≤Â≠òÂú®ÔºåÈÅøÂÖçÈáçÂ§çÊ∑ªÂä†
    local func
    for func in "${GLOBAL_CLEANUP_FUNCTIONS[@]}"; do
        if [[ "$func" == "$func_name" ]]; then
            return 0  # Â∑≤Â≠òÂú®ÔºåÁõ¥Êé•ËøîÂõû
        fi
    done
    
    GLOBAL_CLEANUP_FUNCTIONS+=("$func_name")
    
    # Â¶ÇÊûúÊòØÁ¨¨‰∏ÄÊ¨°Ê∑ªÂä†ÔºåËÆæÁΩÆÂÖ®Â±Ä trap
    if [[ "$GLOBAL_CLEANUP_INITIALIZED" == "false" ]]; then
        trap 'execute_global_cleanup' EXIT INT TERM
        GLOBAL_CLEANUP_INITIALIZED="true"
        log_verbose "ÂàùÂßãÂåñÂÖ®Â±ÄÊ∏ÖÁêÜÊú∫Âà∂"
    fi
}

# ÊâßË°åÊâÄÊúâÊ∏ÖÁêÜÂáΩÊï∞
execute_global_cleanup() {
    local exit_code=$?
    local func
    
    # Â¶ÇÊûúÊòØ‰∏≠Êñ≠‰ø°Âè∑ÔºåÊòæÁ§∫‰∏≠Êñ≠Ê∂àÊÅØ
    if [[ $exit_code -eq 130 ]]; then  # Ctrl+C
        log_warning "Ê£ÄÊµãÂà∞‰∏≠Êñ≠‰ø°Âè∑ (Ctrl+C)"
    elif [[ $exit_code -eq 143 ]]; then  # SIGTERM
        log_warning "Ê£ÄÊµãÂà∞ÁªàÊ≠¢‰ø°Âè∑ (SIGTERM)"
    fi
    
    for func in "${GLOBAL_CLEANUP_FUNCTIONS[@]}"; do
        if declare -f "$func" >/dev/null 2>&1; then
            log_verbose "ÊâßË°åÊ∏ÖÁêÜÂáΩÊï∞: $func"
            "$func"
        fi
    done
    
    # Â¶ÇÊûúÊòØ‰∏≠Êñ≠ÔºåÈÄÄÂá∫
    if [[ $exit_code -eq 130 || $exit_code -eq 143 ]]; then
        exit $exit_code
    fi
}

# ÁßªÈô§Ê∏ÖÁêÜÂáΩÊï∞
remove_cleanup_function() {
    local func_name="$1"
    local new_array=()
    local func
    
    for func in "${GLOBAL_CLEANUP_FUNCTIONS[@]}"; do
        if [[ "$func" != "$func_name" ]]; then
            new_array+=("$func")
        fi
    done
    
    GLOBAL_CLEANUP_FUNCTIONS=("${new_array[@]}")
}

# ÂàùÂßãÂåñOllamaÊ®°ÂûãÂàóË°®ÁºìÂ≠ò
init_ollama_cache() {
    if [[ "$OLLAMA_CACHE_INITIALIZED" == "true" ]]; then
        return 0
    fi
    
    log_verbose "ÂàùÂßãÂåñOllamaÊ®°ÂûãÂàóË°®ÁºìÂ≠ò..."
    
    # ‰ΩøÁî®Áªü‰∏ÄÁöÑÂÆπÂô®ÈÄªËæëËé∑ÂèñÊ®°ÂûãÂàóË°®
    log_verbose "Ëé∑ÂèñOllamaÊ®°ÂûãÂàóË°®..."
    
    # Ëé∑ÂèñÊ®°ÂûãÂàóË°®Âπ∂ÁºìÂ≠ò
    OLLAMA_MODELS_CACHE=$(execute_ollama_command_with_output "list" | awk 'NR>1 {print $1}' | sort)
    if [[ -n "$OLLAMA_MODELS_CACHE" ]]; then
        OLLAMA_CACHE_INITIALIZED="true"
        log_verbose_success "OllamaÊ®°ÂûãÂàóË°®ÁºìÂ≠òÂàùÂßãÂåñÂÆåÊàê"
    else
        log_verbose "OllamaÊ®°ÂûãÂàóË°®‰∏∫Á©∫"
        OLLAMA_MODELS_CACHE=""
        OLLAMA_CACHE_INITIALIZED="true"
    fi
    
    return 0
}

# Ê£ÄÊü•OllamaÊ®°ÂûãÊòØÂê¶Â≠òÂú®Ôºà‰ΩøÁî®ÁºìÂ≠òÔºâ
check_ollama_model_exists() {
    local model_name="$1"
    
    # Á°Æ‰øùÁºìÂ≠òÂ∑≤ÂàùÂßãÂåñ
    if ! init_ollama_cache; then
        log_error "Êó†Ê≥ïÂàùÂßãÂåñOllamaÊ®°ÂûãÁºìÂ≠ò"
        return 1
    fi
    
    # Âú®ÁºìÂ≠ò‰∏≠Êü•ÊâæÊ®°Âûã
    if echo "$OLLAMA_MODELS_CACHE" | grep -q "^${model_name}$"; then
        return 0
    else
        return 1
    fi
}


# È™åËØÅÊ®°Âûã‰∏öÂä°ÈÄªËæëÂÆåÊï¥ÊÄß
validate_model_business_integrity() {
    local backup_file="$1"
    
    # ÂàõÂª∫‰∏¥Êó∂ÁõÆÂΩïÊèêÂèñÂ§á‰ªΩÊñá‰ª∂
    local temp_dir=$(mktemp -d) || { log_error "Êó†Ê≥ïÂàõÂª∫‰∏¥Êó∂ÁõÆÂΩï"; return 1; }
    
    # Ê∏ÖÁêÜÂáΩÊï∞
    cleanup_temp_business() {
        [[ -d "${temp_dir:-}" ]] && docker_rm_rf "$temp_dir"
    }
    add_cleanup_function "cleanup_temp_business"
    
    # ÊèêÂèñÂ§á‰ªΩÊñá‰ª∂Âà∞‰∏¥Êó∂ÁõÆÂΩï
    if ! docker run --rm --entrypoint="" -v "$(dirname "$backup_file"):/data" -v "$temp_dir:/temp" hf_downloader:latest sh -c "
        cd /data && tar -xf '$(basename "$backup_file")' -C /temp 2>/dev/null
    "; then
        log_error "Êó†Ê≥ïÊèêÂèñÂ§á‰ªΩÊñá‰ª∂ËøõË°å‰∏öÂä°ÈÄªËæëÊ£ÄÊü•"
        cleanup_temp_business
        return 1
    fi
    
    # Êü•ÊâæmanifestÊñá‰ª∂
    local manifest_files=()
    while IFS= read -r -d '' manifest; do
        manifest_files+=("$manifest")
    done < <(find "$temp_dir" -path "*/manifests/*" -type f -print0 2>/dev/null)
    
    if [[ ${#manifest_files[@]} -eq 0 ]]; then
        log_error "Â§á‰ªΩ‰∏≠Êú™ÊâæÂà∞manifestÊñá‰ª∂"
        cleanup_temp_business
        return 1
    fi
    
    # Ê£ÄÊü•ÊØè‰∏™manifestÂºïÁî®ÁöÑblobÊñá‰ª∂
    local missing_blobs=0
    local total_blobs=0
    
    for manifest_file in "${manifest_files[@]}"; do
        if [[ -f "$manifest_file" ]]; then
            # Ëß£ÊûêmanifestÊñá‰ª∂‰∏≠ÁöÑblobÂºïÁî®
            local blob_digests
            blob_digests=$(grep -o '"digest":"sha256:[a-f0-9]\{64\}"' "$manifest_file" 2>/dev/null | sed 's/"digest":"sha256:\([a-f0-9]\{64\}\)"/\1/g')
            
            for digest in $blob_digests; do
                ((total_blobs++))
                local blob_path="$temp_dir/blobs/sha256-$digest"
                if [[ ! -f "$blob_path" ]]; then
                    log_error "Áº∫Â∞ëblobÊñá‰ª∂: sha256-$digest"
                    ((missing_blobs++))
                fi
            done
        fi
    done
    
    cleanup_temp_business
    remove_cleanup_function "cleanup_temp_business"
    
    if [[ $missing_blobs -gt 0 ]]; then
        log_error "ÂèëÁé∞ $missing_blobs/$total_blobs ‰∏™blobÊñá‰ª∂Áº∫Â§±"
        return 1
    fi
    
    log_verbose_success "Ê®°Âûã‰∏öÂä°ÈÄªËæëÂÆåÊï¥ÊÄßÈ™åËØÅÈÄöËøá ($total_blobs ‰∏™blobÊñá‰ª∂)"
    return 0
}


# Ê∏ÖÁêÜ‰∏çÂÆåÊï¥ÁöÑÊ®°Âûã
cleanup_incomplete_model() {
    local model_name="$1"
    local model_tag="$2"
    local full_model_name="${model_name}:${model_tag}"
    
    log_verbose_warning "Ê£ÄÊµãÂà∞‰∏çÂÆåÊï¥ÁöÑÊ®°ÂûãÔºåÊ≠£Âú®Ê∏ÖÁêÜ: $full_model_name"
    
    # Á°ÆÂÆömanifestÊñá‰ª∂Ë∑ØÂæÑ
    local manifest_file
    if [[ "$model_name" == hf.co/* ]]; then
        # HuggingFace GGUFÊ®°Âûã
        manifest_file="$OLLAMA_MODELS_DIR/manifests/$model_name/$model_tag"
    elif [[ "$model_name" == *"/"* ]]; then
        # Áî®Êà∑ÂàÜ‰∫´ÁöÑÊ®°Âûã
        local user_name="${model_name%/*}"
        local repo_name="${model_name#*/}"
        manifest_file="$OLLAMA_MODELS_DIR/manifests/registry.ollama.ai/$user_name/$repo_name/$model_tag"
    else
        # ÂÆòÊñπÊ®°Âûã
        manifest_file="$OLLAMA_MODELS_DIR/manifests/registry.ollama.ai/library/$model_name/$model_tag"
    fi
    
    # Âà†Èô§manifestÊñá‰ª∂
    if [[ -f "$manifest_file" ]]; then
        if docker_rm_rf "$manifest_file"; then
            log_verbose "Â∑≤Âà†Èô§‰∏çÂÆåÊï¥ÁöÑmanifestÊñá‰ª∂: $manifest_file"
        else
            log_warning "Êó†Ê≥ïÂà†Èô§manifestÊñá‰ª∂: $manifest_file"
        fi
    fi
    
    # Ê∏ÖÈô§ÁºìÂ≠òÔºåÂº∫Âà∂ÈáçÊñ∞Ê£ÄÊü•
    OLLAMA_CACHE_INITIALIZED="false"
    OLLAMA_MODELS_CACHE=""
    
    log_verbose_success "‰∏çÂÆåÊï¥Ê®°ÂûãÊ∏ÖÁêÜÂÆåÊàê: $full_model_name"
}

# È™åËØÅÊ®°ÂûãÂÆâË£ÖÂêéÁöÑÂÆåÊï¥ÊÄß
verify_model_after_installation() {
    local model_name="$1"
    local model_tag="$2"
    local full_model_name="${model_name}:${model_tag}"
    
    log_verbose "È™åËØÅÊ®°ÂûãÂÆâË£ÖÂÆåÊï¥ÊÄß: $full_model_name"
    
    # ÂàùÂßãÂåñÁºìÂ≠ò‰ª•ÊèêÈ´òÂÆåÊï¥ÊÄßÊ£ÄÊü•ÊÄßËÉΩ
    ensure_cache_initialized
    
    # Á≠âÂæÖ‰∏Ä‰∏ãËÆ©Êñá‰ª∂Á≥ªÁªüÂêåÊ≠•
    sleep 2
    
    # Ê£ÄÊü•Ê®°ÂûãÂÆåÊï¥ÊÄßÔºà‰ΩøÁî®ÁºìÂ≠ò‰ºòÂåñÔºâ
    local model_spec="${model_name}:${model_tag}"
    if verify_integrity "model" "$model_spec" "use_cache:true,check_blobs:true"; then
        log_verbose_success "Ê®°ÂûãÂÆâË£ÖÂÆåÊï¥ÊÄßÈ™åËØÅÈÄöËøá: $full_model_name"
        return 0
    else
        log_error "Ê®°ÂûãÂÆâË£Ö‰∏çÂÆåÊï¥ÔºåÊ≠£Âú®Ê∏ÖÁêÜ: $full_model_name"
        cleanup_incomplete_model "$model_name" "$model_tag"
        return 1
    fi
}

# ÁÆÄÂåñÁöÑÊ®°ÂûãÊ£ÄÊü•ÂáΩÊï∞
check_ollama_model() {
    local model_name="$1"
    local model_tag="$2"
    local full_model_name="${model_name}:${model_tag}"
    
    # È¶ñÂÖàÂ∞ùËØïÈÄöËøáOllamaÂÆπÂô®Ê£ÄÊü•ÔºàÊúÄÂáÜÁ°ÆÔºâ
    if check_ollama_model_exists "$full_model_name"; then
        log_verbose_success "OllamaÊ®°ÂûãÂ∑≤Â≠òÂú®: $full_model_name"
        return 0
    fi
    
    # Â¶ÇÊûúOllamaÂÆπÂô®Ê£ÄÊü•Â§±Ë¥•ÔºåËøõË°åÂÆåÊï¥ÊÄßÊ£ÄÊü•Ôºà‰ΩøÁî®ÁºìÂ≠ò‰ºòÂåñÔºâ
    local model_spec="${model_name}:${model_tag}"
    if verify_integrity "model" "$model_spec" "use_cache:true,check_blobs:true"; then
        log_verbose_success "OllamaÊ®°ÂûãÂ∑≤Â≠òÂú®ÔºàÊñá‰ª∂Á≥ªÁªüÈ™åËØÅÔºâ: $full_model_name"
        return 0
    else
        log_verbose_warning "OllamaÊ®°Âûã‰∏çÂ≠òÂú®Êàñ‰∏çÂÆåÊï¥: $full_model_name"
        return 1
    fi
}

# Ëß£ÊûêÊ®°ÂûãËßÑÊ†ºÔºàmodel:versionÊ†ºÂºèÔºâ
parse_model_spec() {
    local model_spec="$1"
    local -n name_var="$2"
    local -n version_var="$3"
    
    if ! validate_model_format "$model_spec"; then
        return 1
    fi
    
    name_var="${model_spec%:*}"
    version_var="${model_spec#*:}"
    return 0
}

# ÂàùÂßãÂåñÁªùÂØπË∑ØÂæÑ
init_paths() {
    # Ëé∑ÂèñÁªùÂØπË∑ØÂæÑÔºåÂ¶ÇÊûúÁõÆÂΩï‰∏çÂ≠òÂú®ÂàôÂÖàÂàõÂª∫Áà∂ÁõÆÂΩï
    mkdir -p "${OLLAMA_DATA_DIR}" "${HF_DOWNLOAD_CACHE_DIR}" "${HF_ORIGINAL_BACKUP_DIR}" || {
        log_error "Êó†Ê≥ïÂàõÂª∫ÂøÖË¶ÅÁõÆÂΩï"
        return 1
    }
    
    ABS_OLLAMA_DATA_DIR="$(realpath "${OLLAMA_DATA_DIR}")"
    ABS_HF_DOWNLOAD_CACHE_DIR="$(realpath "${HF_DOWNLOAD_CACHE_DIR}")"
    ABS_HF_ORIGINAL_BACKUP_DIR="$(realpath "${HF_ORIGINAL_BACKUP_DIR}")"
    
}

# Docker backup helper functions

# DockerËæÖÂä©ÂáΩÊï∞ - ÈáçÂëΩÂêçÂàÜÂç∑Êñá‰ª∂Ôºà‰ªé.000,.001,.002Ê†ºÂºèÂà∞.001,.002,.003Ê†ºÂºèÔºâ

# Docker helper function - list tar content directly

# DockerÊñá‰ª∂Á≥ªÁªüÊìç‰ΩúËæÖÂä©ÂáΩÊï∞
docker_rm_rf() {
    local target_path="$1"
    local parent_dir
    local target_name
    
    # ÂÆâÂÖ®Ê£ÄÊü•ÔºöÈò≤Ê≠¢Âà†Èô§Á©∫Ë∑ØÂæÑÊàñÊ†πÁõÆÂΩï
    if [[ -z "$target_path" || "$target_path" == "/" ]]; then
        log_error "ÂÆâÂÖ®Âà†Èô§: Ë∑ØÂæÑ‰∏∫Á©∫ÊàñÊ†πÁõÆÂΩïÔºåÊãíÁªùÂà†Èô§"
        return 1
    fi
    
    # Ëé∑ÂèñÁà∂ÁõÆÂΩïÂíåÁõÆÊ†áÂêçÁß∞
    parent_dir="$(dirname "$target_path")"
    target_name="$(basename "$target_path")"
    
    # log_info "‰ΩøÁî®DockerÂà†Èô§: $target_path"
    
    # ‰ΩøÁî®DockerÂÆπÂô®‰ª•rootÊùÉÈôêÂà†Èô§Êñá‰ª∂/ÁõÆÂΩïÔºåË¶ÜÁõñENTRYPOINT
    docker run --rm --entrypoint="" \
        -v "$parent_dir:/work" \
        "$FULL_IMAGE_NAME" \
        rm -rf "/work/$target_name" 2>/dev/null
}

docker_mkdir_p() {
    local target_path="$1"
    local parent_dir
    local target_name
    
    # Â¶ÇÊûúÁõÆÂΩïÂ∑≤Â≠òÂú®ÔºåÁõ¥Êé•ËøîÂõû
    [[ -d "$target_path" ]] && return 0
    
    # Ëé∑ÂèñÁà∂ÁõÆÂΩïÂíåÁõÆÊ†áÂêçÁß∞
    parent_dir="$(dirname "$target_path")"
    target_name="$(basename "$target_path")"
    
    
    # ‰ΩøÁî®DockerÂÆπÂô®‰ª•rootÊùÉÈôêÂàõÂª∫ÁõÆÂΩïÔºåË¶ÜÁõñENTRYPOINT
    if docker run --rm --entrypoint="" --user root \
        -v "$parent_dir:/work" \
        "$FULL_IMAGE_NAME" \
        sh -c "mkdir -p /work/$target_name" 2>/dev/null; then
        return 0
    else 
        log_error "DockerÂàõÂª∫ÁõÆÂΩïÂ§±Ë¥•: $target_path" >&2
        return 1
    fi
}


# Á°Æ‰øùhf_downloaderÈïúÂÉèÂ≠òÂú®
ensure_hf_downloader_image() {
    if ! docker image inspect "$FULL_IMAGE_NAME" &>/dev/null; then
        log_verbose "ÊûÑÂª∫ $FULL_IMAGE_NAME ÈïúÂÉè..."
        if ! build_docker_image; then
            log_error "$FULL_IMAGE_NAME ÈïúÂÉèÊûÑÂª∫Â§±Ë¥•"
            return 1
        fi
        log_verbose_success "$FULL_IMAGE_NAME ÈïúÂÉèÊûÑÂª∫ÂÆåÊàê"
    fi
    return 0
}


# Á°Æ‰øùollama/ollamaÈïúÂÉèÂ≠òÂú®
ensure_ollama_image() {
    if ! docker image inspect "$DOCKER_IMAGE_OLLAMA" &>/dev/null; then
        log_verbose "ÊãâÂèñ $DOCKER_IMAGE_OLLAMA ÈïúÂÉè..."
        if ! docker pull "$DOCKER_IMAGE_OLLAMA"; then
            log_error "$DOCKER_IMAGE_OLLAMA ÈïúÂÉèÊãâÂèñÂ§±Ë¥•"
            return 1
        fi
        log_verbose_success "$DOCKER_IMAGE_OLLAMA ÈïúÂÉèÊãâÂèñÂÆåÊàê"
    fi
    return 0
}

# Êü•ÊâæËøêË°å‰∏≠ÁöÑOllamaÂÆπÂô®
find_running_ollama_container() {
    # Ê£ÄÊü•ÊòØÂê¶ÊúâËøêË°å‰∏≠ÁöÑ Ollama ÂÆπÂô®
    local running_containers
    running_containers=$(docker ps --format "{{.Names}}" --filter "ancestor=ollama/ollama")
    
    if [[ -n "$running_containers" ]]; then
        # ÊâæÂà∞Á¨¨‰∏Ä‰∏™ËøêË°å‰∏≠ÁöÑÂÆπÂô®
        EXISTING_OLLAMA_CONTAINER=$(echo "$running_containers" | head -n1)
        log_verbose "ÊâæÂà∞ËøêË°å‰∏≠ÁöÑOllamaÂÆπÂô®: $EXISTING_OLLAMA_CONTAINER"
        return 0
    fi
    
    # Ê£ÄÊü•Êú¨Âú∞11434Á´ØÂè£ÊòØÂê¶ÊúâÊúçÂä°ÂìçÂ∫îÔºàÂèØËÉΩÊòØÂ§ñÈÉ®ÂÆπÂô®Ôºâ
    if command -v curl >/dev/null 2>&1; then
        if curl -s --connect-timeout 2 http://localhost:11434/api/version >/dev/null 2>&1; then
            # ÊâæÂà∞‰ΩøÁî®11434Á´ØÂè£ÁöÑÂÆπÂô®
            local port_container
            port_container=$(docker ps --format "{{.Names}}" --filter "publish=11434")
            if [[ -n "$port_container" ]]; then
                EXISTING_OLLAMA_CONTAINER=$(echo "$port_container" | head -n1)
                log_verbose "ÊâæÂà∞‰ΩøÁî®11434Á´ØÂè£ÁöÑOllamaÂÆπÂô®: $EXISTING_OLLAMA_CONTAINER"
                return 0
            fi
        fi
    fi
    
    EXISTING_OLLAMA_CONTAINER=""
    return 1
}

# ÂêØÂä®‰∏¥Êó∂OllamaÂÆπÂô®
start_temp_ollama_container() {
    if [[ -n "$TEMP_OLLAMA_CONTAINER" ]]; then
        # Ê£ÄÊü•‰∏¥Êó∂ÂÆπÂô®ÊòØÂê¶ËøòÂú®ËøêË°å
        if docker ps -q --filter "name=^${TEMP_OLLAMA_CONTAINER}$" | grep -q .; then
            log_verbose "‰∏¥Êó∂OllamaÂÆπÂô®‰ªçÂú®ËøêË°å: $TEMP_OLLAMA_CONTAINER"
            return 0
        else
            log_verbose "‰∏¥Êó∂OllamaÂÆπÂô®Â∑≤ÂÅúÊ≠¢ÔºåÈáçÊñ∞ÂêØÂä®"
            TEMP_OLLAMA_CONTAINER=""
        fi
    fi
    
    # Á°Æ‰øù Ollama ÈïúÂÉèÂ≠òÂú®
    ensure_ollama_image || return 1
    
    TEMP_OLLAMA_CONTAINER="ollama-temp-$$"
    
    log_verbose "ÂêØÂä®‰∏¥Êó∂OllamaÂÆπÂô®: $TEMP_OLLAMA_CONTAINER"
    
    # ÊûÑÂª∫ÂÆπÂô®ÂêØÂä®ÂëΩ‰ª§
    local cmd=("docker" "run" "-d" "--name" "$TEMP_OLLAMA_CONTAINER")
    cmd+=("-e" "HF_ENDPOINT=${HF_ENDPOINT}")
    cmd+=("--gpus" "all")
    cmd+=("-v" "${ABS_OLLAMA_DATA_DIR}:/root/.ollama")
    cmd+=("-p" "11435:11434")  # ‰ΩøÁî®‰∏çÂêåÁ´ØÂè£ÈÅøÂÖçÂÜ≤Á™Å
    cmd+=("$DOCKER_IMAGE_OLLAMA")
    
    # ÂêØÂä®ÂÆπÂô®
    local start_output
    if start_output=$("${cmd[@]}" 2>&1); then
        log_verbose "‰∏¥Êó∂ÂÆπÂô®ÂêØÂä®ÊàêÂäüÔºåID: ${start_output:0:12}"
        
        # Á≠âÂæÖÊúçÂä°Â∞±Áª™
        if wait_for_ollama_ready "$TEMP_OLLAMA_CONTAINER"; then
            log_verbose_success "‰∏¥Êó∂OllamaÂÆπÂô®Â∞±Áª™: $TEMP_OLLAMA_CONTAINER"
            # ËÆæÁΩÆÊ∏ÖÁêÜÈô∑Èò±
            setup_temp_container_cleanup
            return 0
        else
            log_error "‰∏¥Êó∂OllamaÂÆπÂô®ÂêØÂä®Â§±Ë¥•"
            docker rm -f "$TEMP_OLLAMA_CONTAINER" &>/dev/null
            TEMP_OLLAMA_CONTAINER=""
            return 1
        fi
    else
        log_error "Êó†Ê≥ïÂêØÂä®‰∏¥Êó∂OllamaÂÆπÂô®"
        log_error "DockerÂêØÂä®ÈîôËØØ: $start_output"
        TEMP_OLLAMA_CONTAINER=""
        return 1
    fi
}

# Ê∏ÖÁêÜ‰∏¥Êó∂OllamaÂÆπÂô®
cleanup_temp_ollama_container() {
    if [[ -n "$TEMP_OLLAMA_CONTAINER" ]]; then
        log_verbose "Ê∏ÖÁêÜ‰∏¥Êó∂OllamaÂÆπÂô®: $TEMP_OLLAMA_CONTAINER"
        docker rm -f "$TEMP_OLLAMA_CONTAINER" &>/dev/null
        TEMP_OLLAMA_CONTAINER=""
    fi
}

# ËÆæÁΩÆ‰∏¥Êó∂ÂÆπÂô®Ê∏ÖÁêÜÈô∑Èò±
setup_temp_container_cleanup() {
    add_cleanup_function "cleanup_temp_ollama_container"
}

# Áªü‰∏ÄÁöÑOllamaÂëΩ‰ª§ÊâßË°åÂáΩÊï∞
execute_ollama_command() {
    local action="$1"
    shift
    local args=("$@")
    
    log_verbose "ÊâßË°åOllamaÂëΩ‰ª§: $action ${args[*]}"
    
    # È¶ñÂÖàÊü•ÊâæËøêË°å‰∏≠ÁöÑOllamaÂÆπÂô®
    if find_running_ollama_container; then
        log_verbose "‰ΩøÁî®Áé∞ÊúâOllamaÂÆπÂô®: $EXISTING_OLLAMA_CONTAINER"
        log_verbose "ÊâßË°åÂëΩ‰ª§: docker exec $EXISTING_OLLAMA_CONTAINER ollama $action ${args[*]}"
        if docker exec "$EXISTING_OLLAMA_CONTAINER" ollama "$action" "${args[@]}"; then
            return 0
        else
            log_error "Âú®Áé∞ÊúâÂÆπÂô®‰∏≠ÊâßË°åOllamaÂëΩ‰ª§Â§±Ë¥•: $action ${args[*]}"
            return 1
        fi
    else
        # Ê≤°ÊúâÊâæÂà∞ËøêË°å‰∏≠ÁöÑÂÆπÂô®ÔºåÂêØÂä®‰∏¥Êó∂ÂÆπÂô®
        log_verbose "Êú™ÊâæÂà∞ËøêË°å‰∏≠ÁöÑOllamaÂÆπÂô®ÔºåÂêØÂä®‰∏¥Êó∂ÂÆπÂô®"
        if start_temp_ollama_container; then
            log_verbose "Âú®‰∏¥Êó∂ÂÆπÂô®‰∏≠ÊâßË°åÂëΩ‰ª§: docker exec $TEMP_OLLAMA_CONTAINER ollama $action ${args[*]}"
            if docker exec "$TEMP_OLLAMA_CONTAINER" ollama "$action" "${args[@]}"; then
                return 0
            else
                log_error "Âú®‰∏¥Êó∂ÂÆπÂô®‰∏≠ÊâßË°åOllamaÂëΩ‰ª§Â§±Ë¥•: $action ${args[*]}"
                return 1
            fi
        else
            log_error "Êó†Ê≥ïÂêØÂä®‰∏¥Êó∂OllamaÂÆπÂô®"
            return 1
        fi
    fi
}

# ÊâßË°åOllamaÂëΩ‰ª§Âπ∂Ëé∑ÂèñËæìÂá∫
execute_ollama_command_with_output() {
    local action="$1"
    shift
    local args=("$@")
    
    # È¶ñÂÖàÊü•ÊâæËøêË°å‰∏≠ÁöÑOllamaÂÆπÂô®
    if find_running_ollama_container; then
        docker exec "$EXISTING_OLLAMA_CONTAINER" ollama "$action" "${args[@]}" 2>/dev/null
    else
        # Ê≤°ÊúâÊâæÂà∞ËøêË°å‰∏≠ÁöÑÂÆπÂô®ÔºåÂêØÂä®‰∏¥Êó∂ÂÆπÂô®
        if start_temp_ollama_container; then
            docker exec "$TEMP_OLLAMA_CONTAINER" ollama "$action" "${args[@]}" 2>/dev/null
        else
            return 1
        fi
    fi
}


# ÊòæÁ§∫‰ΩøÁî®Â∏ÆÂä©
show_help() {
    cat << 'EOF'
ü§ñ OMO - Oh My Ollama / Ollama Models Organizer

‰ΩøÁî®ÊñπÊ≥ï:
  ./omo.sh [OPTIONS]

ÈÄâÈ°π:
  --models-file FILE    ÊåáÂÆöÊ®°ÂûãÂàóË°®Êñá‰ª∂ (ÈªòËÆ§: ./models.list)
  --ollama-dir DIR      ÊåáÂÆöOllamaÊï∞ÊçÆÁõÆÂΩï (ÈªòËÆ§: ./ollama)
  --hf-backup-dir DIR   ÊåáÂÆöHuggingFaceÂéüÂßãÊ®°ÂûãÂ§á‰ªΩÁõÆÂΩï (ÈªòËÆ§: ./hf_originals)
  --install             ÂÆâË£Ö/‰∏ãËΩΩÊ®°Âûã (Ë¶ÜÁõñÈªòËÆ§ÁöÑ‰ªÖÊ£ÄÊü•Ë°å‰∏∫)
  --check-only          ‰ªÖÊ£ÄÊü•Ê®°ÂûãÁä∂ÊÄÅÔºå‰∏ç‰∏ãËΩΩ (ÈªòËÆ§Ë°å‰∏∫)
  --force-download      Âº∫Âà∂ÈáçÊñ∞‰∏ãËΩΩÊâÄÊúâÊ®°Âûã (Ëá™Âä®ÂêØÁî®ÂÆâË£ÖÊ®°Âºè)
  --verbose             ÊòæÁ§∫ËØ¶ÁªÜÊó•Âøó
  --hf-token TOKEN      HuggingFaceËÆøÈóÆ‰ª§Áâå
  --rebuild             Âº∫Âà∂ÈáçÊñ∞ÊûÑÂª∫DockerÈïúÂÉè
  --list                ÂàóÂá∫Â∑≤ÂÆâË£ÖÁöÑOllamaÊ®°ÂûãÂèäËØ¶ÁªÜ‰ø°ÊÅØ
  --backup MODEL        Â§á‰ªΩÊåáÂÆöÊ®°Âûã (Ê†ºÂºè: Ê®°ÂûãÂêç:ÁâàÊú¨)
  --backup-all          Â§á‰ªΩÊâÄÊúâÊ®°Âûã
  --restore FILE        ÊÅ¢Â§çÊåáÂÆöÂ§á‰ªΩÊñá‰ª∂
  --remove MODEL        Âà†Èô§ÊåáÂÆöÊ®°Âûã
  --remove-all          Âà†Èô§ÊâÄÊúâÊ®°Âûã
  --backup-dir DIR      Â§á‰ªΩÁõÆÂΩï (ÈªòËÆ§: ./backups)
  --force               Âº∫Âà∂Êìç‰ΩúÔºàË∑≥ËøáÁ°ÆËÆ§Ôºâ
  --generate-compose    ÁîüÊàêdocker-compose.yamlÊñá‰ª∂ÔºàÂü∫‰∫émodels.listÔºâ
  --help                ÊòæÁ§∫Â∏ÆÂä©‰ø°ÊÅØ

Ê®°ÂûãÂàóË°®Êñá‰ª∂Ê†ºÂºè:
  ollama deepseek-r1:1.5b
  huggingface microsoft/DialoGPT-medium q4_0
  hf-gguf hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF:latest

‰∏ãËΩΩÁºìÂ≠ò:
  HuggingFaceÊ®°Âûã‰∏ãËΩΩÊîØÊåÅÊñ≠ÁÇπÁª≠‰º†ÂíåÁºìÂ≠òÂ§çÁî®
  ÁºìÂ≠òÁõÆÂΩï: ./hf_download_cache (Ëá™Âä®ÂàõÂª∫)
  ÊØè‰∏™Ê®°ÂûãÊúâÁã¨Á´ãÁöÑÁºìÂ≠òÂ≠êÁõÆÂΩï
  ‰∏≠Êñ≠ÂêéÈáçÊñ∞ËøêË°åËÑöÊú¨Â∞ÜÊÅ¢Â§ç‰∏ãËΩΩÔºåÂÆåÊàêÂêéËá™Âä®ÁºìÂ≠ò

ÂéüÂßãÂ§á‰ªΩ:
  HuggingFaceÊ®°ÂûãËΩ¨Êç¢ÂÆåÊàêÂêéËá™Âä®Â§á‰ªΩÂéüÂßãÊñá‰ª∂
  Â§á‰ªΩÁõÆÂΩï: ./hf_originals (Ëá™Âä®ÂàõÂª∫)  
  Â§á‰ªΩÊ†ºÂºè: 
EOF
    cat << 'EOF'
  Â§á‰ªΩÊ†ºÂºè: ÁõÆÂΩïÂ§çÂà∂ (Ê®°ÂûãÂêç_original/)
  Ëá™Âä®ÁîüÊàê: MD5Ê†°È™åÊñá‰ª∂ (Ê®°ÂûãÂêç_original.md5)
  Â§á‰ªΩ‰ø°ÊÅØÊñá‰ª∂: Ê®°ÂûãÂêç_original_info.txt (ÂåÖÂê´Êñá‰ª∂ÂàóË°®ÂíåMD5Ê†°È™å)
  Áî®ÈÄî: HuggingFace APIË∞ÉÁî®„ÄÅÈáçÊñ∞ÈáèÂåñ„ÄÅÊ®°ÂûãÊÅ¢Â§çÁ≠â

OllamaÊ®°ÂûãÂ§á‰ªΩ:
  ÊîØÊåÅÂÆåÊï¥ÁöÑOllamaÊ®°ÂûãÂ§á‰ªΩÂíåÊÅ¢Â§ç
  Â§á‰ªΩÁõÆÂΩï: ./backups (ÈªòËÆ§ÔºåÂèØÈÄöËøá--backup-dirÊåáÂÆö)
  Â§á‰ªΩÊ†ºÂºè: ÁõÆÂΩïÂ§çÂà∂ (Ê®°ÂûãÂêç/)
  ÂåÖÂê´ÂÜÖÂÆπ: manifestÊñá‰ª∂ÂíåÊâÄÊúâblobÊï∞ÊçÆ
  Ëá™Âä®ÁîüÊàê: MD5Ê†°È™åÊñá‰ª∂ÂíåËØ¶ÁªÜ‰ø°ÊÅØÊñá‰ª∂
  
Â§á‰ªΩÁâπÊÄß:
  - Áõ¥Êé•Â§çÂà∂Â§á‰ªΩÔºåÊó†ÂéãÁº©Â§ÑÁêÜÔºåÂ§á‰ªΩÂíåÊÅ¢Â§çÈÄüÂ∫¶ÊûÅÂø´
  - MD5Ê†°È™åÁ°Æ‰øùÊñá‰ª∂ÂÆåÊï¥ÊÄß
  - ÊØè‰∏™Ê®°ÂûãÁã¨Á´ãÊñá‰ª∂Â§πÔºå‰æø‰∫éÁÆ°ÁêÜ

Á§∫‰æã:
  # Ê£ÄÊü•Ê®°ÂûãÁä∂ÊÄÅ (ÈªòËÆ§Ë°å‰∏∫)
  ./omo.sh
  
  # ÂÆâË£Ö/‰∏ãËΩΩÁº∫Â§±ÁöÑÊ®°Âûã
  ./omo.sh --install
  
  # ‰ªÖÊ£ÄÊü•Áä∂ÊÄÅ (ÂêåÈªòËÆ§Ë°å‰∏∫)
  ./omo.sh --check-only
  
  # ÂàóÂá∫Â∑≤ÂÆâË£ÖÁöÑÊ®°Âûã
  ./omo.sh --list
  
  # Â§á‰ªΩÊ®°Âûã
  ./omo.sh --backup tinyllama:latest
  
  # Âà†Èô§Ê®°Âûã
  ./omo.sh --remove llama2:7b --force

EOF
}

# Ê£ÄÊü•‰æùËµñ
# Ê£ÄÊü•GPUÊîØÊåÅ
check_gpu_support() {
    # Ê£ÄÊü•ÊòØÂê¶ÊîØÊåÅNVIDIA GPU
    if command_exists nvidia-smi && nvidia-smi &>/dev/null; then
        return 0  # ÊîØÊåÅGPU
    fi
    return 1  # ‰∏çÊîØÊåÅGPU
}

check_dependencies() {
    local missing_deps=()
    
    # Ê£ÄÊü• docker
    if ! command_exists docker; then
        missing_deps+=("docker")
        log_error "Docker Êú™ÂÆâË£ÖÊàñ‰∏çÂú® PATH ‰∏≠"
    else
        # Ê£ÄÊü• Docker ÂÆàÊä§ËøõÁ®ãÊòØÂê¶ËøêË°å
        if ! docker info &> /dev/null; then
            log_error "Docker Â∑≤ÂÆâË£Ö‰ΩÜÂÆàÊä§ËøõÁ®ãÊú™ËøêË°åÔºåËØ∑ÂêØÂä® Docker ÊúçÂä°"
            return 1
        fi
    fi
    
    # Ê£ÄÊü• tar
    if ! command_exists tar; then
        missing_deps+=("tar")
        log_error "tar Êú™ÂÆâË£ÖÔºåÁî®‰∫éÊ®°ÂûãÊñá‰ª∂ÊâìÂåÖ/Ëß£ÂåÖ"
    fi
    
    # Â¶ÇÊûúÊúâÁº∫Â§±ÁöÑ‰æùËµñÔºåÁªôÂá∫ÊèêÁ§∫Âπ∂ÈÄÄÂá∫
    if [ ${#missing_deps[@]} -gt 0 ]; then
        log_error "Áº∫Â∞ëÂøÖÈúÄÁöÑÁ≥ªÁªü‰æùËµñ: ${missing_deps[*]}"
        log_error "ËØ∑ÂÆâË£ÖÁº∫Â§±ÁöÑ‰æùËµñÂêéÈáçÊñ∞ËøêË°åËÑöÊú¨"
        return 1
    fi
    
    # Ê£ÄÊü•GPUÊîØÊåÅÔºàÂøÖÈúÄÈ°πÔºâ
    if ! check_gpu_support; then
        log_error "Êú™Ê£ÄÊµãÂà∞NVIDIA GPUÊîØÊåÅÔºåÊ≠§ËÑöÊú¨ÈúÄË¶ÅGPUÁéØÂ¢É"
        log_error "ËØ∑Á°Æ‰øùÔºö1) ÂÆâË£Ö‰∫ÜNVIDIAÈ©±Âä®  2) ÂÆâË£Ö‰∫Ünvidia-smiÂ∑•ÂÖ∑"
        return 1
    fi
    
    log_verbose "Ê£ÄÊµãÂà∞GPUÊîØÊåÅÔºåÂ∞ÜÂêØÁî®GPUÂä†ÈÄü"
    
    # ÊâÄÊúâ‰æùËµñÊ£ÄÊü•ÈÄöËøáÔºåÈùôÈªòËøîÂõû
    return 0
}

# ÊûÑÂª∫DockerÈïúÂÉè - ÈõÜÊàêÁâàÊú¨
build_docker_image() {
    log_verbose "ÊûÑÂª∫DockerÈïúÂÉè: $FULL_IMAGE_NAME"
    
    # ÂàõÂª∫‰∏¥Êó∂ÊûÑÂª∫ÁõÆÂΩï
    local temp_build_dir="/tmp/docker_build_$$"
    
    # ‰øùÂ≠òÂΩìÂâçÁöÑtrapËÆæÁΩÆ
    local original_trap
    original_trap=$(trap -p EXIT | sed "s/trap -- '//" | sed "s/' EXIT//")
    
    # ËÆæÁΩÆÂ§çÂêàtrap - ‰øÆÂ§çshellcheck SC2089/SC2090
    if [[ -n "$original_trap" ]]; then
        trap "cleanup_docker_build_context '$temp_build_dir'; $original_trap" EXIT
    else
        trap "cleanup_docker_build_context '$temp_build_dir'" EXIT
    fi
    
    # ÂàõÂª∫ÊûÑÂª∫‰∏ä‰∏ãÊñá
    create_docker_build_context "$temp_build_dir"
    
    # ÊûÑÂª∫ÊîØÊåÅCUDAÁöÑÈïúÂÉè
    local build_args=()
    log_verbose "ÊûÑÂª∫ÊîØÊåÅCUDAÁöÑÈïúÂÉèÔºåËØ∑ËÄêÂøÉÁ≠âÂæÖ..."
    build_args+=("--build-arg" "USE_CUDA=true")
    
    # ÊâßË°åÊûÑÂª∫ÂëΩ‰ª§
    local docker_build_cmd=("docker" "build" "${build_args[@]}" "-t" "$FULL_IMAGE_NAME" "$temp_build_dir")
    
    if "${docker_build_cmd[@]}"; then
        log_verbose_success "DockerÈïúÂÉèÊûÑÂª∫ÂÆåÊàê: $FULL_IMAGE_NAME"
        cleanup_docker_build_context "$temp_build_dir"
        # ÊÅ¢Â§çÂéüÂßãtrap
        if [[ -n "$original_trap" ]]; then
            trap '$original_trap' EXIT
        else
            trap - EXIT
        fi
        return 0
    else
        log_error "DockerÈïúÂÉèÊûÑÂª∫Â§±Ë¥•"
        cleanup_docker_build_context "$temp_build_dir"
        # ÊÅ¢Â§çÂéüÂßãtrap
        if [[ -n "$original_trap" ]]; then
            trap '$original_trap' EXIT
        else
            trap - EXIT
        fi
        exit 1
    fi
}


# Ëß£ÊûêÊ®°ÂûãÂàóË°®Êñá‰ª∂
parse_models_list() {
    local models_file="$1"
    local -n models_array=${2:-models}
    
    if [[ ! -f "$models_file" ]]; then
        log_error "Ê®°ÂûãÂàóË°®Êñá‰ª∂‰∏çÂ≠òÂú®: $models_file"
        return 1
    fi
    
    log_verbose "Ëß£ÊûêÊ®°ÂûãÂàóË°®Êñá‰ª∂: $models_file"
    
    while IFS= read -r line || [[ -n "$line" ]]; do
        # Ë∑≥ËøáÁ©∫Ë°åÂíåÊ≥®ÈáäË°å
        [[ -z "$line" || "$line" =~ ^[[:space:]]*# ]] && continue
        
        # ‰ΩøÁî®Á©∫Ê†ºÂàÜÈöîËß£ÊûêÊ®°Âûã‰ø°ÊÅØ: Ê®°ÂûãÁ±ªÂûã Ê®°ÂûãÂêçÁß∞ [ÈáèÂåñÁ±ªÂûã]
        read -r model_type model_name quantization <<< "$line"
        
        if [[ -n "$model_type" && -n "$model_name" ]]; then
            if [[ "$model_type" == "ollama" || "$model_type" == "huggingface" || "$model_type" == "hf-gguf" ]]; then
                # Â¶ÇÊûúÊúâÈáèÂåñÁ±ªÂûãÔºåÊ∑ªÂä†Âà∞Ê®°Âûã‰ø°ÊÅØ‰∏≠
                if [[ -n "$quantization" ]]; then
                    models_array+=("$model_type:$model_name:$quantization")
                    log_verbose "Ê∑ªÂä†Ê®°Âûã: $model_type -> $model_name:$quantization"
                else
                    models_array+=("$model_type:$model_name")
                    log_verbose "Ê∑ªÂä†Ê®°Âûã: $model_type -> $model_name"
                fi
            else
                log_warning "Êú™Áü•Ê®°ÂûãÁ±ªÂûã: $model_type (Ë°å: $line)"
            fi
        else
            log_warning "ÂøΩÁï•Êó†ÊïàË°å: $line"
        fi
    done < "$models_file"
    
    # Ê£ÄÊü•ÊòØÂê¶ÊâæÂà∞ÊúâÊïàÊ®°Âûã
    if [[ ${#models_array[@]} -eq 0 ]]; then
        log_warning "================================ WARNING ================================"
        log_warning "No valid models found in models.list file!"
        log_warning "All model entries are either commented out or invalid."
        log_warning ""
        log_warning "Please edit the models.list file:"
        log_warning "1. Uncomment (remove # at the beginning) the models you need"
        log_warning "2. Add your own model configurations"
        log_warning "3. Check model search URLs in the comments for available models"
        log_warning ""
        log_warning "Examples:"
        log_warning "  ollama deepseek-r1:1.5b"
        log_warning "  huggingface Qwen/Qwen3-0.6B q4_0"
        log_warning "  hf-gguf hf.co/MaziyarPanahi/gemma-3-1b-it-GGUF"
        log_warning "====================================================================="
        echo
    else
        log_verbose "ÂÖ±Ëß£ÊûêÂà∞ ${#models_array[@]} ‰∏™Ê®°Âûã"
    fi
}

# Ê£ÄÊü•HuggingFace GGUFÊ®°ÂûãÊòØÂê¶Â≠òÂú®ÔºàÈÄöËøáOllamaÊ£ÄÊü•Ôºâ
check_hf_gguf_model() {
    local model_name="$1"
    local model_tag="$2"
    local full_model_name="${model_name}:${model_tag}"
    
    
    # ‰ΩøÁî®ÂÆπÂô®Ê£ÄÊü•
    if check_ollama_model_exists "$full_model_name"; then
        log_verbose_success "HuggingFace GGUFÊ®°ÂûãÂ∑≤Â≠òÂú®: $full_model_name"
        return 0
    fi
    
    log_verbose_warning "HuggingFace GGUFÊ®°Âûã‰∏çÂ≠òÂú®: $full_model_name"
    return 1
}

# ÁîüÊàêOllamaÊ®°ÂûãÂêçÁß∞
generate_ollama_model_name() {
    local model_name="$1"
    local quantize_type="$2"
    
    # Ê∏ÖÁêÜÈáèÂåñÁ±ªÂûã
    local clean_quant=$(echo "${quantize_type}" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9]//g')
    
    # ‰ΩøÁî®Áªü‰∏ÄÁöÑÂëΩÂêçÂáΩÊï∞ËøõË°åOllamaÊ®°ÂûãÂêçÁß∞ËΩ¨Êç¢
    local full_name_clean=$(get_safe_model_name "$model_name" "ollama")
    
    # ‰∏∫‰ªéHuggingFace‰∏ãËΩΩÁöÑÊ®°ÂûãÊ∑ªÂä†ËØÜÂà´ÂâçÁºÄÂíåÈáèÂåñÂêéÁºÄ
    echo "hf-${full_name_clean}:${clean_quant}"
}

# ‰∏ãËΩΩOllamaÊ®°Âûã
download_ollama_model() {
    local model_name="$1"
    local model_tag="$2"
    
    log_info "‰∏ãËΩΩÊ®°Âûã: ${model_name}:${model_tag}"
    
    if execute_ollama_command "pull" "${model_name}:${model_tag}"; then
        log_verbose_success "OllamaÊ®°Âûã‰∏ãËΩΩÂÆåÊàê: ${model_name}:${model_tag}"
        
        # È™åËØÅ‰∏ãËΩΩÂêéÁöÑÊ®°ÂûãÂÆåÊï¥ÊÄß
        if verify_model_after_installation "$model_name" "$model_tag"; then
            log_verbose_success "Ê®°ÂûãÂÆåÊï¥ÊÄßÈ™åËØÅÈÄöËøá: ${model_name}:${model_tag}"
            return 0
        else
            log_verbose_warning "Ê®°ÂûãÂÆåÊï¥ÊÄßÈ™åËØÅÂ§±Ë¥•ÔºåÊ®°ÂûãÂ∑≤Ë¢´Ê∏ÖÁêÜ: ${model_name}:${model_tag}"
            return 1
        fi
    else
        log_error "OllamaÊ®°Âûã‰∏ãËΩΩÂ§±Ë¥•: ${model_name}:${model_tag}"
        return 1
    fi
}

# ‰∏ãËΩΩHuggingFace GGUFÊ®°ÂûãÔºàÈÄöËøáOllamaÁõ¥Êé•‰∏ãËΩΩÔºâ
download_hf_gguf_model() {
    local model_name="$1"
    local model_tag="$2"
    local full_model_name="${model_name}:${model_tag}"
    
    log_verbose "ÂºÄÂßã‰∏ãËΩΩHuggingFace GGUFÊ®°Âûã: $full_model_name"
    
    if execute_ollama_command "pull" "$full_model_name"; then
        log_verbose_success "HuggingFace GGUFÊ®°Âûã‰∏ãËΩΩÂÆåÊàê: $full_model_name"
        
        # È™åËØÅ‰∏ãËΩΩÂêéÁöÑÊ®°ÂûãÂÆåÊï¥ÊÄß
        if verify_model_after_installation "$model_name" "$model_tag"; then
            log_verbose_success "Ê®°ÂûãÂÆåÊï¥ÊÄßÈ™åËØÅÈÄöËøá: $full_model_name"
            return 0
        else
            log_error "Ê®°ÂûãÂÆåÊï¥ÊÄßÈ™åËØÅÂ§±Ë¥•ÔºåÊ®°ÂûãÂ∑≤Ë¢´Ê∏ÖÁêÜ: $full_model_name"
            return 1
        fi
    else
        log_error "HuggingFace GGUFÊ®°Âûã‰∏ãËΩΩÂ§±Ë¥•: $full_model_name"
        return 1
    fi
}

# Âà†Èô§OllamaÊ®°Âûã
remove_ollama_model() {
    local model_spec="$1"
    local force_delete="${2:-false}"
    
    # Ëß£ÊûêÊ®°ÂûãÂêçÁß∞ÂíåÁâàÊú¨
    if ! validate_model_format "$model_spec"; then
        return 1
    fi
    
    log_verbose "ÂáÜÂ§áÂà†Èô§OllamaÊ®°Âûã: $model_spec"
    
    # Ê£ÄÊü•Ê®°ÂûãÊòØÂê¶Â≠òÂú®
    local model_name model_version
    if ! parse_model_spec "$model_spec" model_name model_version; then
        return 1
    fi
    if ! check_ollama_model "$model_name" "$model_version"; then
        log_warning "Ê®°Âûã‰∏çÂ≠òÂú®ÔºåÊó†ÈúÄÂà†Èô§: $model_spec"
        return 0
    fi
    
    # Â¶ÇÊûú‰∏çÊòØÂº∫Âà∂Âà†Èô§ÔºåËØ¢ÈóÆÁî®Êà∑Á°ÆËÆ§
    if [[ "$force_delete" != "true" ]]; then
        log_warning "Âç≥Â∞ÜÂà†Èô§Ê®°Âûã: $model_spec"
        echo -n "Á°ÆËÆ§Âà†Èô§Ôºü[y/N]: "
        read -r confirm
        if [[ "$confirm" != "y" && "$confirm" != "Y" ]]; then
            log_verbose "ÂèñÊ∂àÂà†Èô§Êìç‰Ωú"
            return 0
        fi
    fi
    
    if execute_ollama_command "rm" "$model_spec"; then
        log_verbose_success "OllamaÊ®°ÂûãÂà†Èô§ÂÆåÊàê: $model_spec"
        return 0
    else
        log_error "OllamaÊ®°ÂûãÂà†Èô§Â§±Ë¥•: $model_spec"
        return 1
    fi
}

# Ëé∑ÂèñÊ®°ÂûãÁõ∏ÂÖ≥ÁöÑblobÊñá‰ª∂Ë∑ØÂæÑ
get_model_blob_paths() {
    local manifest_file="$1"
    local models_dir="$2"
    local blob_paths=()
    
    if [[ ! -f "$manifest_file" ]]; then
        log_error "Ê®°ÂûãmanifestÊñá‰ª∂‰∏çÂ≠òÂú®: $manifest_file"
        return 1
    fi
    
    # ‰ΩøÁî®hf_downloaderÈïúÂÉè‰∏≠ÁöÑjqËß£ÊûêJSONÊñá‰ª∂
    local layers
    layers=$(docker run --rm --entrypoint="" -v "$(dirname "$manifest_file"):/data" hf_downloader jq -r '.layers[].digest, .config.digest' "/data/$(basename "$manifest_file")" 2>/dev/null | sort -u)
    
    # ÊûÑÂª∫blobÊñá‰ª∂Ë∑ØÂæÑ
    while IFS= read -r digest; do
        if [[ -n "$digest" ]]; then
            # Â∞Ü sha256:xxx Ê†ºÂºèËΩ¨Êç¢‰∏∫ sha256-xxx
            local blob_name="${digest//:/-}"
            local blob_file="$models_dir/blobs/$blob_name"
            blob_paths+=("$blob_file")
        fi
    done <<< "$layers"
    
    # ËæìÂá∫Ë∑ØÂæÑ
    printf '%s\n' "${blob_paths[@]}"
}

# ===== Â§á‰ªΩÂ∑•ÂÖ∑ÂáΩÊï∞ =====

# ÈÄöÁî®ÂëΩ‰ª§Ê£ÄÊü•ÂáΩÊï∞
command_exists() {
    command -v "$1" >/dev/null 2>&1
}

# Ê®°ÂûãÂêçÁß∞ÂÆâÂÖ®ÂåñÂ§ÑÁêÜ
# Áªü‰∏ÄÁöÑÊ®°ÂûãÂêçÁß∞ËΩ¨Êç¢ÂáΩÊï∞
# ÂèÇÊï∞1: Ê®°ÂûãÂêçÁß∞
# ÂèÇÊï∞2: ËΩ¨Êç¢Á±ªÂûã (backup|ollama|filesystem)
get_safe_model_name() {
    local model_spec="$1"
    local conversion_type="${2:-backup}"
    
    case "$conversion_type" in
        "backup")
            # Áî®‰∫éÂ§á‰ªΩÁõÆÂΩïÂëΩÂêçÔºö/ Âíå : ‚Üí _
            echo "$model_spec" | sed 's/[\/:]/_/g'
            ;;
        "ollama")
            # Áî®‰∫éOllamaÊ®°ÂûãÂëΩÂêçÔºöÂ§çÊùÇËΩ¨Êç¢ËßÑÂàôÔºà‰∏ÄÊ¨°ÊÄßÂ§ÑÁêÜÔºâ
            local full_name_clean
            full_name_clean=$(echo "$model_spec" | tr '[:upper:]' '[:lower:]' | sed -e 's/\//_/g' -e 's/[^a-z0-9_-]/_/g' -e 's/__*/_/g' -e 's/--*/-/g' -e 's/^[-_]\+\|[-_]\+$//g')
            # ÈïøÂ∫¶ÈôêÂà∂
            if [[ ${#full_name_clean} -gt 50 ]]; then
                local prefix="${full_name_clean:0:30}"
                local suffix="${full_name_clean: -15}"
                full_name_clean="${prefix}_${suffix}"
            fi
            echo "$full_name_clean"
            ;;
        "filesystem")
            # Áî®‰∫éÊñá‰ª∂Á≥ªÁªüÂÆâÂÖ®ÂëΩÂêçÔºö/ ‚Üí _ÔºåÂÖ∂‰ªñÈùûÊ≥ïÂ≠óÁ¨¶ ‚Üí -
            echo "$model_spec" | sed -e 's/\//_/g' -e 's/[^a-zA-Z0-9._-]/-/g'
            ;;
        *)
            # ÈªòËÆ§‰ΩøÁî®backupËßÑÂàô
            echo "$model_spec" | sed 's/[\/:]/_/g'
            ;;
    esac
}

# Êñá‰ª∂Â§ßÂ∞èÂ∑•ÂÖ∑ÂáΩÊï∞
get_file_size() {
    local file_path="$1"
    local format="${2:-mb}"  # mb, human
    
    case "$format" in
        "mb")
            du -sm "$file_path" 2>/dev/null | cut -f1
            ;;
        "human")
            du -sh "$file_path" 2>/dev/null | cut -f1 
            ;;
        *)
            log_error "Unknown format: $format. Use 'mb' or 'human'"
            return 1
            ;;
    esac
}

# ÂêëÂêéÂÖºÂÆπÁöÑÂåÖË£ÖÂáΩÊï∞
get_file_size_mb() {
    get_file_size "$1" "mb"
}

get_file_size_human() {
    get_file_size "$1" "human"
}

# ËÆ°ÁÆóÁõÆÂΩïÁöÑMD5Ê†°È™åÂÄº
calculate_directory_md5() {
    local dir_path="$1"
    local md5_file="$2"
    
    if [[ ! -d "$dir_path" ]]; then
        log_error "ÁõÆÂΩï‰∏çÂ≠òÂú®: $dir_path"
        return 1
    fi
    
    log_verbose "Ê≠£Âú®ËÆ°ÁÆóÁõÆÂΩïMD5Ê†°È™åÂÄº: $dir_path"
    
    # ‰ΩøÁî®findÂíåmd5sumËÆ°ÁÆóÊâÄÊúâÊñá‰ª∂ÁöÑMD5ÂÄºÔºå‰ΩøÁî®Áõ∏ÂØπË∑ØÂæÑ
    # ÊåâÊñá‰ª∂Ë∑ØÂæÑÊéíÂ∫è‰ª•Á°Æ‰øùÁªìÊûú‰∏ÄËá¥ÊÄß
    if (cd "$dir_path" && find . -type f -print0 | sort -z | xargs -0 md5sum) > "$md5_file" 2>/dev/null; then
        log_verbose "MD5Ê†°È™åÊñá‰ª∂Â∑≤ÁîüÊàê: $md5_file"
        return 0
    else
        log_error "MD5Ê†°È™åËÆ°ÁÆóÂ§±Ë¥•"
        return 1
    fi
}

# È™åËØÅÁõÆÂΩïÁöÑMD5Ê†°È™åÂÄº
verify_directory_md5() {
    local dir_path="$1"
    local md5_file="$2"
    
    if [[ ! -d "$dir_path" ]]; then
        log_error "ÁõÆÂΩï‰∏çÂ≠òÂú®: $dir_path"
        return 1
    fi
    
    if [[ ! -f "$md5_file" ]]; then
        log_error "MD5Ê†°È™åÊñá‰ª∂‰∏çÂ≠òÂú®: $md5_file"
        return 1
    fi
    
    log_verbose "Ê≠£Âú®È™åËØÅÁõÆÂΩïMD5Ê†°È™åÂÄº: $dir_path"
    
    # ‰∏¥Êó∂ËÆ°ÁÆóÂΩìÂâçÁõÆÂΩïÁöÑMD5ÂÄº
    local temp_md5=$(mktemp)
    if ! calculate_directory_md5 "$dir_path" "$temp_md5"; then
        rm -f "$temp_md5"
        return 1
    fi
    
    # ÊØîËæÉMD5Êñá‰ª∂
    if diff "$md5_file" "$temp_md5" >/dev/null 2>&1; then
        log_verbose "MD5Ê†°È™åÈÄöËøá"
        rm -f "$temp_md5"
        return 0
    else
        log_error "MD5Ê†°È™åÂ§±Ë¥•"
        rm -f "$temp_md5"
        return 1
    fi
}

# ÂÖ®Â±ÄÁºìÂ≠òÂèòÈáè
declare -A BACKUP_CONTENT_CACHE
declare -A MODEL_BLOB_CACHE

# Ê£ÄÊü•Â§á‰ªΩÂÆåÊï¥ÊÄßÔºàÊ£ÄÊü•Â§á‰ªΩ‰∏≠ÊòØÂê¶ÂåÖÂê´ÊâÄÊúâÂøÖÈúÄÁöÑblobÊñá‰ª∂Ôºâ

# Ëé∑ÂèñÊ®°ÂûãblobÂàóË°®ÔºàÂ∏¶ÁºìÂ≠òÔºâ
get_model_blobs_cached() {
    local model_spec="$1"
    
    # Ê£ÄÊü•ÁºìÂ≠ò
    if [[ -n "${MODEL_BLOB_CACHE[$model_spec]:-}" ]]; then
        echo "${MODEL_BLOB_CACHE[$model_spec]}"
        return 0
    fi
    
    # Ëß£ÊûêÊ®°ÂûãÂêçÁß∞ÂíåÁâàÊú¨
    local model_name model_version
    if ! parse_model_spec "$model_spec" model_name model_version; then
        return 1
    fi
    
    # Á°ÆÂÆömanifestÊñá‰ª∂Ë∑ØÂæÑ
    local manifest_file
    if [[ "$model_name" == hf.co/* ]]; then
        manifest_file="$OLLAMA_MODELS_DIR/manifests/$model_name/$model_version"
    elif [[ "$model_name" == *"/"* ]]; then
        local user_name="${model_name%/*}"
        local repo_name="${model_name#*/}"
        manifest_file="$OLLAMA_MODELS_DIR/manifests/registry.ollama.ai/$user_name/$repo_name/$model_version"
    else
        manifest_file="$OLLAMA_MODELS_DIR/manifests/registry.ollama.ai/library/$model_name/$model_version"
    fi
    
    # Ëé∑ÂèñblobÊñá‰ª∂ÂàóË°®
    if [[ -f "$manifest_file" ]]; then
        local blobs=$(get_model_blob_paths "$manifest_file" "$OLLAMA_MODELS_DIR" | sed "s|^$OLLAMA_MODELS_DIR/||")
        if [[ -n "$blobs" ]]; then
            # ÁºìÂ≠òÁªìÊûú
            MODEL_BLOB_CACHE[$model_spec]="$blobs"
            echo "$blobs"
            return 0
        fi
    fi
    
    return 1
}

# Âø´ÈÄüÊ£ÄÊü•ÂçïÊñá‰ª∂Â§á‰ªΩÂÆåÊï¥ÊÄß


# Ê∏ÖÁêÜÂÆåÊï¥ÊÄßÊ£ÄÊü•ÁºìÂ≠ò
clear_integrity_cache() {
    [[ -n "${VERBOSE}" ]] && log_verbose "Ê∏ÖÁêÜÂÆåÊï¥ÊÄßÊ£ÄÊü•ÁºìÂ≠ò"
    unset BACKUP_CONTENT_CACHE
    unset MODEL_BLOB_CACHE
    declare -g -A BACKUP_CONTENT_CACHE
    declare -g -A MODEL_BLOB_CACHE
}

# Á°Æ‰øùÂÆåÊï¥ÊÄßÊ£ÄÊü•ÁºìÂ≠òÂ∑≤ÂàùÂßãÂåñ
ensure_cache_initialized() {
    # Â¶ÇÊûúÁºìÂ≠òÊï∞ÁªÑ‰∏çÂ≠òÂú®ÔºåÂàùÂßãÂåñÂÆÉ‰ª¨
    if [[ ! -v BACKUP_CONTENT_CACHE ]] || [[ ! -v MODEL_BLOB_CACHE ]]; then
        declare -g -A BACKUP_CONTENT_CACHE
        declare -g -A MODEL_BLOB_CACHE
        [[ -n "${VERBOSE}" ]] && log_verbose "ÂÆåÊï¥ÊÄßÊ£ÄÊü•ÁºìÂ≠òÂ∑≤ÂàùÂßãÂåñ"
    fi
}

# ==================================================================================
#                           Áªü‰∏ÄÂÆåÊï¥ÊÄßÈ™åËØÅÊû∂ÊûÑ
# ==================================================================================

# ÈÄöÁî®ÂÆåÊï¥ÊÄßÈ™åËØÅÂáΩÊï∞ - Áªü‰∏ÄÊâÄÊúâÈ™åËØÅÈÄªËæëÁöÑÂÖ•Âè£ÁÇπ
verify_integrity() {
    local verification_type="$1"  # model, backup, hf_model
    local target="$2"             # ÁõÆÊ†áÊñá‰ª∂/Ë∑ØÂæÑ/Ê®°ÂûãËßÑÊ†º
    local options="${3:-}"        # ÈôÑÂä†ÈÄâÈ°π (use_cache:true, check_blobs:true, etc.)
    
    # Ëß£ÊûêÈÄâÈ°π
    local use_cache="true"
    local check_blobs="true"
    local model_spec=""
    
    # Ëß£ÊûêÈÄâÈ°πÂ≠óÁ¨¶‰∏≤
    if [[ -n "$options" ]]; then
        while IFS=',' read -ra ADDR; do
            for i in "${ADDR[@]}"; do
                case "$i" in
                    use_cache:*)
                        use_cache="${i#*:}"
                        ;;
                    check_blobs:*)
                        check_blobs="${i#*:}"
                        ;;
                    model_spec:*)
                        model_spec="${i#*:}"
                        ;;
                esac
            done
        done <<< "$options"
    fi
    
    # Á°Æ‰øùÁºìÂ≠òÂ∑≤ÂàùÂßãÂåñ
    [[ "$use_cache" == "true" ]] && ensure_cache_initialized
    
    # Ê†πÊçÆÈ™åËØÅÁ±ªÂûãË∞ÉÁî®Áõ∏Â∫îÁöÑÈ™åËØÅÈÄªËæë
    case "$verification_type" in
        "model")
            _verify_local_model "$target" "$use_cache" "$check_blobs"
            ;;
        "backup")
            _verify_backup_target "$target" "$model_spec" "$use_cache" "$check_blobs"
            ;;
        "hf_model")
            _verify_hf_model "$target" "$check_blobs"
            ;;
        "backup_file")
            _verify_backup_file "$target" "$use_cache"
            ;;
        *)
            log_error "Unknown verification type: $verification_type"
            return 1
            ;;
    esac
}

# ÂÜÖÈÉ®ÂáΩÊï∞ÔºöÈ™åËØÅÊú¨Âú∞Ê®°ÂûãÂÆåÊï¥ÊÄß
_verify_local_model() {
    local model_spec="$1"
    local use_cache="$2"
    local check_blobs="$3"
    
    # Ëß£ÊûêÊ®°ÂûãËßÑÊ†º
    local model_name model_tag
    if [[ "$model_spec" =~ ^(.+):(.+)$ ]]; then
        model_name="${BASH_REMATCH[1]}"
        model_tag="${BASH_REMATCH[2]}"
    else
        log_error "Invalid model spec format: $model_spec"
        return 1
    fi
    
    # Á°ÆÂÆömanifestÊñá‰ª∂Ë∑ØÂæÑ
    local manifest_file
    if [[ "$model_name" == hf.co/* ]]; then
        manifest_file="$OLLAMA_MODELS_DIR/manifests/$model_name/$model_tag"
    elif [[ "$model_name" == *"/"* ]]; then
        local user_name="${model_name%/*}"
        local repo_name="${model_name#*/}"
        manifest_file="$OLLAMA_MODELS_DIR/manifests/registry.ollama.ai/$user_name/$repo_name/$model_tag"
    else
        manifest_file="$OLLAMA_MODELS_DIR/manifests/registry.ollama.ai/library/$model_name/$model_tag"
    fi
    
    # Ê£ÄÊü•manifestÊñá‰ª∂ÊòØÂê¶Â≠òÂú®
    [[ ! -f "$manifest_file" ]] && return 1
    
    # Â¶ÇÊûú‰∏çÈúÄË¶ÅÊ£ÄÊü•blobÔºåÂè™È™åËØÅmanifestÂ≠òÂú®Âç≥ÂèØ
    [[ "$check_blobs" == "false" ]] && return 0
    
    # Ëé∑ÂèñblobÊñá‰ª∂ÂàóË°®Âπ∂È™åËØÅ
    local blob_files
    if [[ "$use_cache" == "true" ]]; then
        blob_files=$(get_model_blobs_cached "$model_spec")
        [[ -z "$blob_files" ]] && return 1
        
        # Ê£ÄÊü•ÊØè‰∏™blobÊñá‰ª∂
        while IFS= read -r blob_relative_path; do
            [[ -n "$blob_relative_path" && ! -f "$OLLAMA_MODELS_DIR/$blob_relative_path" ]] && return 1
        done <<< "$blob_files"
    else
        blob_files=$(get_model_blob_paths "$manifest_file" "$OLLAMA_MODELS_DIR")
        [[ -z "$blob_files" ]] && return 1
        
        # Ê£ÄÊü•ÊØè‰∏™blobÊñá‰ª∂
        while IFS= read -r blob_file; do
            [[ -n "$blob_file" && ! -f "$blob_file" ]] && return 1
        done <<< "$blob_files"
    fi
    
    return 0
}

# ÂÜÖÈÉ®ÂáΩÊï∞ÔºöÈ™åËØÅÂ§á‰ªΩÁõÆÊ†áÔºàÁõÆÂΩïÂ§á‰ªΩÔºâ
_verify_backup_target() {
    local backup_target="$1"
    local model_spec="$2"
    local use_cache="$3"
    local check_blobs="$4"
    
    # Ê£ÄÊü•ÁõÆÂΩïÂ§á‰ªΩ
    if [[ -d "$backup_target" ]]; then
        # È™åËØÅÁõÆÂΩïÁªìÊûÑ
        if [[ -d "$backup_target/manifests" ]] && [[ -d "$backup_target/blobs" ]]; then
            # È™åËØÅMD5Ê†°È™å
            local md5_file="${backup_target}.md5"
            if [[ -f "$md5_file" ]]; then
                if verify_directory_md5 "$backup_target" "$md5_file"; then
                    [[ -n "${VERBOSE}" ]] && log_info "ÁõÆÂΩïÂ§á‰ªΩMD5Ê†°È™åÈÄöËøá: $backup_target"
                    return 0
                else
                    log_error "ÁõÆÂΩïÂ§á‰ªΩMD5Ê†°È™åÂ§±Ë¥•: $backup_target"
                    return 1
                fi
            else
                log_warning "Êú™ÊâæÂà∞MD5Ê†°È™åÊñá‰ª∂: $md5_file"
                return 0  # Ê≤°ÊúâMD5Êñá‰ª∂‰πüËÆ§‰∏∫ÊúâÊïàÔºå‰ΩÜ‰ºöËÆ∞ÂΩïË≠¶Âëä
            fi
        else
            log_error "Êó†ÊïàÁöÑÁõÆÂΩïÂ§á‰ªΩÁªìÊûÑ: $backup_target"
            return 1
        fi
    fi
    
    return 1
}


# ÂÜÖÈÉ®ÂáΩÊï∞ÔºöÈ™åËØÅHuggingFaceÊ®°Âûã
_verify_hf_model() {
    local source_dir="$1"
    local check_files="$2"
    
    # Ê£ÄÊü•Ê∫êÁõÆÂΩïÊòØÂê¶Â≠òÂú®
    [[ ! -d "$source_dir" ]] && return 1
    
    # Â¶ÇÊûú‰∏çÈúÄË¶ÅÊ£ÄÊü•Êñá‰ª∂ÔºåÂè™È™åËØÅÁõÆÂΩïÂ≠òÂú®
    [[ "$check_files" == "false" ]] && return 0
    
    # Ê£ÄÊü•ÂøÖË¶ÅÁöÑÊñá‰ª∂
    local has_model_files=false
    for ext in safetensors bin gguf; do
        if ls "$source_dir"/*."$ext" >/dev/null 2>&1; then
            has_model_files=true
            break
        fi
    done
    
    [[ "$has_model_files" == "false" ]] && return 1
    return 0
}

# ÂÜÖÈÉ®ÂáΩÊï∞ÔºöÈ™åËØÅÂ§á‰ªΩÊñá‰ª∂Ôºà‰∏öÂä°ÈÄªËæëÂÆåÊï¥ÊÄßÔºâ
_verify_backup_file() {
    local backup_file="$1"
    local use_detailed_check="$2"
    
    [[ ! -f "$backup_file" ]] && return 1
    
    # Âü∫Êú¨tarÊñá‰ª∂ÂÆåÊï¥ÊÄßÊ£ÄÊü•
    if ! docker run --rm --entrypoint="" -v "$(dirname "$backup_file"):/data" hf_downloader:latest sh -c "
        cd /data && tar -tf '$(basename "$backup_file")' >/dev/null 2>&1
    "; then
        return 1
    fi
    
    # Â¶ÇÊûúÈúÄË¶ÅËØ¶ÁªÜÊ£ÄÊü•ÔºåÊâßË°å‰∏öÂä°ÈÄªËæëÈ™åËØÅ
    [[ "$use_detailed_check" == "true" ]] && validate_model_business_integrity "$backup_file"
}

# Âà†Èô§‰∏çÂÆåÊï¥ÁöÑÂ§á‰ªΩÊñá‰ª∂
remove_incomplete_backup() {
    local backup_base="$1"
    local backup_suffix="${2:-}"
    
    log_verbose "Âà†Èô§‰∏çÂÆåÊï¥ÁöÑÂ§á‰ªΩ: ${backup_base}${backup_suffix}"
    
    # Âà†Èô§ÁõÆÂΩïÂ§á‰ªΩ
    local backup_dir="${backup_base}${backup_suffix}"
    if [[ -d "$backup_dir" ]]; then
        rm -rf "$backup_dir"
        log_verbose "Â∑≤Âà†Èô§Â§á‰ªΩÁõÆÂΩï: $backup_dir"
    fi
    
    # Âà†Èô§MD5Ê†°È™åÊñá‰ª∂
    local md5_file="${backup_dir}.md5"
    if [[ -f "$md5_file" ]]; then
        rm -f "$md5_file"
        log_verbose "Â∑≤Âà†Èô§MD5Ê†°È™åÊñá‰ª∂: $md5_file"
    fi
    
    # Âà†Èô§Â§á‰ªΩ‰ø°ÊÅØÊñá‰ª∂
    local info_file="${backup_base}${backup_suffix}_info.txt"
    if [[ -f "$info_file" ]]; then
        rm -f "$info_file"
        log_verbose "Â∑≤Âà†Èô§Â§á‰ªΩ‰ø°ÊÅØÊñá‰ª∂: $info_file"
    fi
}


# ÂÆâÂÖ®ÁöÑ‰∏¥Êó∂Êñá‰ª∂ÂàõÂª∫
create_temp_file() {
    local prefix="${1:-temp}"
    local temp_file
    temp_file=$(mktemp) || {
        log_error "Êó†Ê≥ïÂàõÂª∫‰∏¥Êó∂Êñá‰ª∂"
        return 1
    }
    echo "$temp_file"
}


# ÂàõÂª∫Ê®°ÂûãÂ§á‰ªΩÁõÆÂΩï
create_model_backup_dir() {
    local model_spec="$1"
    local base_backup_dir="$2"
    local model_safe_name=$(get_safe_model_name "$model_spec")
    local model_backup_dir="${base_backup_dir}/${model_safe_name}"
    
    # ÂàõÂª∫Â§á‰ªΩÁõÆÂΩï
    if ! mkdir -p "$model_backup_dir"; then
        log_error "Êó†Ê≥ïÂàõÂª∫Â§á‰ªΩÁõÆÂΩï: $model_backup_dir"
        return 1
    fi
    echo "$model_backup_dir"
}

# ÁîüÊàêÂ§á‰ªΩÂü∫Á°ÄË∑ØÂæÑ
get_backup_base_path() {
    local model_spec="$1"
    local backup_dir="$2"
    local suffix="${3:-}"
    local model_safe_name=$(get_safe_model_name "$model_spec")
    echo "${backup_dir}/${model_safe_name}${suffix}"
}

# ÂàõÂª∫tarÊñá‰ª∂ÁöÑÈÄöÁî®ÂáΩÊï∞ÔºàÁî®‰∫éHuggingFaceÊ®°ÂûãÔºâ
create_hf_tar_file() {
    local output_file="$1"
    local source_dir="$2"
    
    # ÂàõÂª∫ÊéíÈô§Êñá‰ª∂ÂàóË°®
    local temp_exclude="/tmp/tar_exclude_$$.txt"
    cat > "$temp_exclude" << 'EOF'
*.aria2
*.tmp
*.part
EOF
    
    # ‰ΩøÁî®DockerÂàõÂª∫tarÊñá‰ª∂
    local output_dir="$(dirname "$output_file")"
    local output_basename="$(basename "$output_file")"
    local source_parent="$(dirname "$source_dir")"
    local source_name="$(basename "$source_dir")"
    
    # Á°Æ‰øùhf_downloaderÈïúÂÉèÂ≠òÂú®
    ensure_hf_downloader_image || { rm -f "$temp_exclude"; return 1; }
    
    if docker run --rm --entrypoint="" \
        -v "$source_parent:/source:ro" \
        -v "$output_dir:/output" \
        -v "$temp_exclude:/exclude.txt:ro" \
        "$FULL_IMAGE_NAME" \
        tar -cf "/output/$output_basename" -C /source --exclude-from=/exclude.txt "$source_name" 2>/dev/null; then
        rm -f "$temp_exclude"
        return 0
    else
        log_error "ÂàõÂª∫tarÊñá‰ª∂Â§±Ë¥•: $output_file"
        rm -f "$temp_exclude"
        return 1
    fi
}

# Â§á‰ªΩ‰ø°ÊÅØÂíåÁÆ°ÁêÜÂáΩÊï∞

# ÂàõÂª∫Â§á‰ªΩ‰ø°ÊÅØÊñá‰ª∂
create_backup_info() {
    local model_spec="$1"
    local backup_base="$2"
    local backup_type="$3"  # "directory", "single" Êàñ "split"
    local volume_count="$4"
    local backup_extension="${5:-original}"
    
    local info_file="${backup_base}_info.txt"
    local current_time=$(date '+%Y-%m-%d %H:%M:%S %Z')
    local model_safe_name=$(get_safe_model_name "$model_spec")
    
    # ‰ΩøÁî®‰∏¥Êó∂Êñá‰ª∂ÂàõÂª∫Â§á‰ªΩ‰ø°ÊÅØ
    local temp_info=$(mktemp)
    cat > "$temp_info" << EOF
================================================================================
                           Ê®°ÂûãÂ§á‰ªΩ‰ø°ÊÅØ
================================================================================

Â§á‰ªΩÂü∫Êú¨‰ø°ÊÅØ:
  Ê®°ÂûãËßÑÊ†º: $model_spec
  Â§á‰ªΩÂêçÁß∞: ${model_safe_name}
  Â§á‰ªΩÁ±ªÂûã: $backup_type
  ÂàõÂª∫Êó∂Èó¥: $current_time

Â§á‰ªΩÊñá‰ª∂‰ø°ÊÅØ:
EOF

    # Ê†πÊçÆÂ§á‰ªΩÁ±ªÂûãÊ∑ªÂä†ÂÖ∑‰ΩìÁöÑÊñá‰ª∂‰ø°ÊÅØÂíåMD5
    if [[ "$backup_type" == "directory" ]]; then
        local backup_dir="${backup_base}_${backup_extension}"
        # ÂØπ‰∫éollamaÂ§á‰ªΩÔºåbackup_baseÂ∑≤ÁªèÊòØÂÆåÊï¥Ë∑ØÂæÑÔºå‰∏çÈúÄË¶ÅÊ∑ªÂä†ÂêéÁºÄ
        if [[ "$backup_extension" == "ollama" ]]; then
            backup_dir="$backup_base"
        fi
        local backup_size=$(get_file_size_human "$backup_dir" || echo "Êú™Áü•")
        local md5_file="${backup_dir}.md5"
        local md5_status="ÊúâÊïà"
        
        if [[ ! -f "$md5_file" ]]; then
            md5_status="Áº∫Â§±"
        fi
        
        cat >> "$temp_info" << EOF
  Â§á‰ªΩÊñπÂºè: ÁõÆÂΩïÂ§çÂà∂
  Â§á‰ªΩÁõÆÂΩï: $(basename "$backup_dir")
  Â§á‰ªΩÂ§ßÂ∞è: $backup_size
  MD5Ê†°È™åÊñá‰ª∂: $md5_status

Êñá‰ª∂ÂàóË°®:
EOF
        
        # Ê∑ªÂä†Êñá‰ª∂ÂàóË°®
        if [[ -d "$backup_dir" ]]; then
            find "$backup_dir" -type f -exec basename {} \; | sort >> "$temp_info"
        fi
        
        cat >> "$temp_info" << EOF

MD5Ê†°È™å‰ø°ÊÅØ:
EOF
        
        # Ê∑ªÂä†MD5Ê†°È™å‰ø°ÊÅØ
        if [[ -f "$md5_file" ]]; then
            cat "$md5_file" >> "$temp_info"
        else
            echo "  MD5Ê†°È™åÊñá‰ª∂ÂàõÂª∫Â§±Ë¥•Êàñ‰∏çÂ≠òÂú®" >> "$temp_info"
            echo "  Êñá‰ª∂Ë∑ØÂæÑ: $md5_file" >> "$temp_info"
            echo "  Âª∫ËÆÆ: ÈáçÊñ∞ËøêË°åÂ§á‰ªΩ‰ª•ÁîüÊàêMD5Ê†°È™åÊñá‰ª∂" >> "$temp_info"
        fi
        
        cat >> "$temp_info" << EOF

ÊÅ¢Â§çÂëΩ‰ª§:
  # ‰ΩøÁî®omo.shÊÅ¢Â§ç
  ./omo.sh --restore "$(basename "$backup_dir")"
  
  # ÊâãÂä®ÊÅ¢Â§çÔºàOllamaÊ®°ÂûãÔºâ
  cp -r "$(basename "$backup_dir")/manifests/"* "\$OLLAMA_MODELS_DIR/manifests/"
  cp "$(basename "$backup_dir")/blobs/"* "\$OLLAMA_MODELS_DIR/blobs/"
  
  # ÊâãÂä®ÊÅ¢Â§çÔºàHuggingFaceÊ®°ÂûãÔºâ
  cp -r "$(basename "$backup_dir")" "\$HF_DOWNLOAD_CACHE_DIR/"

EOF
    else
        log_error "‰∏çÊîØÊåÅÁöÑÂ§á‰ªΩÁ±ªÂûã: $backup_type"
        rm -f "$temp_info"
        return 1
    fi
    
    cat >> "$temp_info" << EOF
================================================================================
                               È™åËØÅ‰ø°ÊÅØ
================================================================================

Â§á‰ªΩÈ™åËØÅ:
1. Ê£ÄÊü•Êñá‰ª∂ÂÆåÊï¥ÊÄß:
   - ‰ΩøÁî®MD5Ê†°È™åÊñá‰ª∂È™åËØÅÊØè‰∏™Êñá‰ª∂ÁöÑÂÆåÊï¥ÊÄß
   - md5sum -c $(basename "${backup_dir}.md5")

2. Ê£ÄÊü•Â§á‰ªΩÁªìÊûÑ:
   - Á°Æ‰øùÂ§á‰ªΩÁõÆÂΩïÂåÖÂê´ÂÆåÊï¥ÁöÑÊñá‰ª∂ÁªìÊûÑ
   - ÂØπ‰∫éOllamaÊ®°Âûã: manifests/ Âíå blobs/ ÁõÆÂΩï
   - ÂØπ‰∫éHuggingFaceÊ®°Âûã: Ê®°ÂûãÊñá‰ª∂ÂíåÈÖçÁΩÆÊñá‰ª∂

Â§á‰ªΩÁâπÊÄß:
   - Áõ¥Êé•Â§çÂà∂: ÊûÅÂø´ÁöÑÂ§á‰ªΩÂíåÊÅ¢Â§çÈÄüÂ∫¶ÔºåÊó†ÈúÄÂéãÁº©/Ëß£ÂéãÁº©
   - MD5Ê†°È™å: Á°Æ‰øùÊñá‰ª∂ÂÆåÊï¥ÊÄßÂíå‰∏ÄËá¥ÊÄß
   - ÁÆÄÂåñÁÆ°ÁêÜ: Â§á‰ªΩÊñá‰ª∂ÂèØÁõ¥Êé•ËÆøÈóÆÂíåÊ£ÄÊü•

‰ΩøÁî®ËØ¥Êòé:
- Ê≠§Â§á‰ªΩÂåÖÂê´Ê®°ÂûãÁöÑÂÆåÊï¥Êñá‰ª∂ÁªìÊûÑ
- ÊÅ¢Â§çÂêéÂèØÁõ¥Êé•‰ΩøÁî®ÔºåÊó†ÈúÄÈ¢ùÂ§ñÂ§ÑÁêÜ
- ÊîØÊåÅÂ¢ûÈáèÂ§á‰ªΩÂíåÂ∑ÆÂºÇÊ£ÄÊü•

ÁîüÊàêÊó∂Èó¥: $current_time
================================================================================
EOF

    # Áõ¥Êé•ÂÜôÂÖ•‰ø°ÊÅØÊñá‰ª∂
    if mv "$temp_info" "$info_file"; then
        log_verbose_success "Â§á‰ªΩ‰ø°ÊÅØÊñá‰ª∂ÂàõÂª∫ÂÆåÊàê: $(basename "$info_file")"
    else
        log_error "Êó†Ê≥ïÂÜôÂÖ•Â§á‰ªΩ‰ø°ÊÅØÊñá‰ª∂: $info_file"
        rm -f "$temp_info"
        return 1
    fi
}

# ÂàõÂª∫HuggingFaceÂéüÂßãÊ®°ÂûãÂ§á‰ªΩÔºàÁõ¥Êé•Â§çÂà∂Ôºâ
backup_hf_original_model() {
    local model_name="$1"
    local source_dir="$2"
    
    log_info "Â§á‰ªΩÊ®°Âûã: $model_name"
    log_verbose "Ê∫êÁõÆÂΩï: $source_dir"
    
    # Ê£ÄÊü•Ê∫êÁõÆÂΩïÊòØÂê¶Â≠òÂú®
    if [[ ! -d "$source_dir" ]]; then
        log_error "Ê∫êÁõÆÂΩï‰∏çÂ≠òÂú®: $source_dir"
        return 1
    fi
    
    # Ê£ÄÊü•Êú¨Âú∞Ê®°ÂûãÂÆåÊï¥ÊÄß
    log_info "Ê£ÄÊü•Ê®°ÂûãÂÆåÊï¥ÊÄß..."
    if ! verify_integrity "hf_model" "$source_dir" "check_files:true"; then
        log_error "Êú¨Âú∞Ê®°Âûã‰∏çÂÆåÊï¥ÔºåÂèñÊ∂àÂ§á‰ªΩÊìç‰Ωú"
        return 1
    fi
    
    # ÂàõÂª∫Â§á‰ªΩÁõÆÂΩïÂíåÁîüÊàêË∑ØÂæÑ
    local model_backup_dir
    model_backup_dir=$(create_model_backup_dir "$model_name" "$ABS_HF_ORIGINAL_BACKUP_DIR") || return 1
    local model_safe_name=$(get_safe_model_name "$model_name")
    local backup_dir="$model_backup_dir/${model_safe_name}_original"
    
    # Ê£ÄÊü•ÊòØÂê¶Â∑≤Â≠òÂú®Â§á‰ªΩÁõÆÂΩï
    if [[ -d "$backup_dir" ]]; then
        log_info "Ê®°ÂûãÂ§á‰ªΩÂ∑≤Â≠òÂú®"
        return 0
    fi
    
    log_verbose "Ê®°ÂûãÂ§á‰ªΩÁõÆÂΩï: $backup_dir"
    
    # ËÆ°ÁÆóÊ∫êÁõÆÂΩïÂ§ßÂ∞è
    local source_size_human=$(get_file_size_human "$source_dir")
    log_verbose "Ê∫êÁõÆÂΩïÂ§ßÂ∞è: $source_size_human"
    
    # Â§çÂà∂Ê∫êÁõÆÂΩïÂà∞Â§á‰ªΩÁõÆÂΩïÔºåÊéíÈô§ .hfd ‰∏¥Êó∂ÁõÆÂΩï
    log_info "Ê≠£Âú®Â§çÂà∂Ê®°ÂûãÊñá‰ª∂..."
    mkdir -p "$backup_dir"
    if rsync -av --exclude='.hfd' "$source_dir/" "$backup_dir/" 2>/dev/null || {
        # Â¶ÇÊûúÊ≤°Êúâ rsyncÔºå‰ΩøÁî® cp Âä†ÊâãÂä®ÊéíÈô§
        log_verbose "rsync ‰∏çÂèØÁî®Ôºå‰ΩøÁî® cp Â§çÂà∂ÔºàÊéíÈô§ .hfd ÁõÆÂΩïÔºâ"
        # ‰ΩøÁî® find Â§çÂà∂ÔºåÊéíÈô§ .hfd ÁõÆÂΩï
        (cd "$source_dir" && find . -type d -name '.hfd' -prune -o -type f -print0 | cpio -0pdm "$backup_dir/") 2>/dev/null
    }; then
        # ËÆ°ÁÆóMD5Ê†°È™å
        log_verbose "ËÆ°ÁÆóMD5Ê†°È™åÂÄº..."
        local md5_file="${backup_dir}.md5"
        if calculate_directory_md5 "$backup_dir" "$md5_file"; then
            log_verbose "MD5Ê†°È™åÊñá‰ª∂Â∑≤ÂàõÂª∫: $md5_file"
        else
            log_warning "MD5Ê†°È™åÊñá‰ª∂ÂàõÂª∫Â§±Ë¥•"
        fi
        
        # ÂàõÂª∫Â§á‰ªΩ‰ø°ÊÅØÊñá‰ª∂
        create_backup_info "$model_name" "${model_backup_dir}/${model_safe_name}" "directory" 1 "original"
        
        log_success "HuggingFaceÂéüÂßãÊ®°ÂûãÂ§á‰ªΩÂÆåÊàê: $model_name"
        return 0
    else
        log_error "Â§çÂà∂Êñá‰ª∂Â§±Ë¥•"
        rm -rf "$backup_dir" 2>/dev/null
        return 1
    fi
}

# ÂàóÂá∫Â∑≤ÂÆâË£ÖÁöÑOllamaÊ®°ÂûãÂèäËØ¶ÁªÜ‰ø°ÊÅØ
list_installed_models() {
    log_info "Êâ´ÊèèÂ∑≤ÂÆâË£ÖÁöÑÊ®°Âûã..."
    
    # ÂàùÂßãÂåñÁºìÂ≠ò‰ª•ÊèêÈ´òÂÆåÊï¥ÊÄßÊ£ÄÊü•ÊÄßËÉΩ
    ensure_cache_initialized
    
    # Ê£ÄÊü•OllamaÊ®°ÂûãÁõÆÂΩïÊòØÂê¶Â≠òÂú®
    if [[ ! -d "$OLLAMA_MODELS_DIR" ]]; then
        log_error "OllamaÊ®°ÂûãÁõÆÂΩï‰∏çÂ≠òÂú®: $OLLAMA_MODELS_DIR"
        return 1
    fi
    
    local blobs_dir="$OLLAMA_MODELS_DIR/blobs"
    local manifests_base_dir="$OLLAMA_MODELS_DIR/manifests"
    
    # Ê£ÄÊü•manifestsÂü∫Á°ÄÁõÆÂΩïÊòØÂê¶Â≠òÂú®
    if [[ ! -d "$manifests_base_dir" ]]; then
        log_warning "Êú™ÂèëÁé∞Â∑≤ÂÆâË£ÖÁöÑÊ®°Âûã"
        return 0
    fi
    
    echo ""
    echo "=================================================================================="
    echo "                             Â∑≤ÂÆâË£ÖÁöÑOllamaÊ®°Âûã"
    echo "=================================================================================="
    echo ""
    
    local model_count=0
    local total_size=0
    local total_version_count=0
    
    # ÈÄíÂΩíÊü•ÊâæÊâÄÊúâ manifest Êñá‰ª∂
    local manifest_files=()
    while IFS= read -r -d '' manifest_file; do
        manifest_files+=("$manifest_file")
    done < <(find "$manifests_base_dir" -type f -print0 2>/dev/null)
    
    # ÊåâÊ®°ÂûãÁªÑÁªá manifest Êñá‰ª∂
    declare -A model_manifests
    
    for manifest_file in "${manifest_files[@]}"; do
        # ÊèêÂèñÁõ∏ÂØπ‰∫é manifests_base_dir ÁöÑË∑ØÂæÑ
        local relative_path="${manifest_file#$manifests_base_dir/}"
        
        # Ê†πÊçÆË∑ØÂæÑÁªìÊûÑÊèêÂèñÊ®°ÂûãÂêçÂíåÁâàÊú¨
        local model_name=""
        local version=""
        local full_model_path=""
        
        if [[ "$relative_path" =~ ^registry\.ollama\.ai/library/([^/]+)/(.+)$ ]]; then
            # ‰º†Áªü Ollama Ê®°Âûã: registry.ollama.ai/library/model_name/version
            model_name="${BASH_REMATCH[1]}"
            version="${BASH_REMATCH[2]}"
            full_model_path="registry.ollama.ai/library/$model_name"
        elif [[ "$relative_path" =~ ^hf\.co/([^/]+)/([^/]+)/(.+)$ ]]; then
            # HF-GGUF Ê®°Âûã: hf.co/user/repo/version
            local user="${BASH_REMATCH[1]}"
            local repo="${BASH_REMATCH[2]}"
            version="${BASH_REMATCH[3]}"
            model_name="hf.co/$user/$repo"
            full_model_path="hf.co/$user/$repo"
        else
            # ÂÖ∂‰ªñÊú™Áü•Ê†ºÂºèÔºåÂ∞ùËØïÈÄöÁî®Ëß£Êûê
            local path_parts
            IFS='/' read -ra path_parts <<< "$relative_path"
            if [[ ${#path_parts[@]} -ge 2 ]]; then
                version="${path_parts[-1]}"
                unset path_parts[-1]
                model_name=$(IFS='/'; echo "${path_parts[*]}")
                full_model_path="$model_name"
            else
                continue
            fi
        fi
        
        # Â∞Ü manifest Ê∑ªÂä†Âà∞ÂØπÂ∫îÊ®°ÂûãÁªÑ
        if [[ -n "$model_name" && -n "$version" ]]; then
            local key="$model_name"
            if [[ -z "${model_manifests[$key]:-}" ]]; then
                model_manifests[$key]="$manifest_file|$version|$full_model_path"
            else
                model_manifests[$key]="${model_manifests[$key]};;$manifest_file|$version|$full_model_path"
            fi
        fi
    done
    
    # ÊòæÁ§∫ÊØè‰∏™Ê®°ÂûãÁöÑ‰ø°ÊÅØ
    for model_name in "${!model_manifests[@]}"; do
        local model_data="${model_manifests[$model_name]}"
        
        # Ëß£ÊûêÁ¨¨‰∏Ä‰∏™Êù°ÁõÆ‰ª•Ëé∑ÂèñË∑ØÂæÑ‰ø°ÊÅØ
        local first_entry="${model_data%%;*}"
        local full_model_path="${first_entry##*|}"
        local model_dir="$manifests_base_dir/$full_model_path"
        
        echo "üì¶ Ê®°Âûã: $model_name"
        [[ "${VERBOSE}" == "true" ]] && echo "   ‚îú‚îÄ ‰ΩçÁΩÆ: $model_dir"
        
        local version_count=0
        
        # Â§ÑÁêÜÊâÄÊúâÁâàÊú¨
        IFS=';;' read -ra entries <<< "$model_data"
        for entry in "${entries[@]}"; do
            IFS='|' read -r manifest_file version _ <<< "$entry"
            
            if [[ ! -f "$manifest_file" ]]; then
                continue
            fi
            
            # Ê£ÄÊü•Ê®°ÂûãÂÆåÊï¥ÊÄßÔºà‰ΩøÁî®ÁºìÂ≠ò‰ºòÂåñÔºâ
            local integrity_status=""
            local check_model_spec="${model_name}:${version}"
            if verify_integrity "model" "$check_model_spec" "use_cache:true,check_blobs:true"; then
                integrity_status=" ‚úì(ÂÆåÊï¥)"
            else
                integrity_status=" ‚ö†Ô∏è(‰∏çÂÆåÊï¥)"
            fi
            
            echo "   ‚îú‚îÄ ÁâàÊú¨: $version$integrity_status"
            
            # ËØªÂèñmanifestÊñá‰ª∂Ëé∑Âèñblob‰ø°ÊÅØ
            if [[ "${VERBOSE}" == "true" ]] && [[ -f "$manifest_file" ]]; then
                local manifest_content
                if manifest_content=$(cat "$manifest_file" 2>/dev/null); then
                    # manifestÊòØJSONÊ†ºÂºèÔºåËß£ÊûêËé∑ÂèñÊâÄÊúâÂ±ÇÁöÑÂ§ßÂ∞è
                    local total_model_size=0
                    local blob_count=0
                    local model_type="Êú™Áü•"
                    
                    # Â∞ùËØï‰ªéJSON‰∏≠ÊèêÂèñÊ®°ÂûãÁ±ªÂûã
                    if echo "$manifest_content" | grep -q "application/vnd.ollama.image.model"; then
                        model_type="OllamaÊ®°Âûã"
                    fi
                    
                    # ÊèêÂèñconfigÂ§ßÂ∞è
                    local config_size
                    if config_size=$(echo "$manifest_content" | grep -o '"config":{[^}]*"size":[0-9]*' | grep -o '[0-9]*$' 2>/dev/null); then
                        total_model_size=$((total_model_size + config_size))
                        blob_count=$((blob_count + 1))
                    fi
                    
                    # ÊèêÂèñÊâÄÊúâlayersÁöÑÂ§ßÂ∞è
                    local layer_sizes
                    if layer_sizes=$(echo "$manifest_content" | grep -o '"size":[0-9]*' | grep -o '[0-9]*' 2>/dev/null); then
                        while IFS= read -r size; do
                            if [[ -n "$size" && "$size" -gt 0 ]]; then
                                total_model_size=$((total_model_size + size))
                                blob_count=$((blob_count + 1))
                            fi
                        done <<< "$layer_sizes"
                    fi
                    
                    # Ê†ºÂºèÂåñÂ§ßÂ∞èÊòæÁ§∫
                    local human_size=$(format_bytes "$total_model_size")
                    
                    echo "   ‚îú‚îÄ Â§ßÂ∞è: $human_size"
                    
                    total_size=$((total_size + total_model_size))
                fi
            fi
            
            version_count=$((version_count + 1))
        done
        
        echo "   ‚îî‚îÄ ÁâàÊú¨Êï∞Èáè: $version_count"
        echo ""
        model_count=$((model_count + 1))
        total_version_count=$((total_version_count + version_count))
    done
    
    # ÊòæÁ§∫ÁªüËÆ°‰ø°ÊÅØ
    echo "=================================================================================="
    echo "ÁªüËÆ°‰ø°ÊÅØ:"
    echo "  üìä ÊÄªÊ®°ÂûãÊï∞: $model_count"
    echo "  üî¢ ÊÄªÁâàÊú¨Êï∞: $total_version_count"
    
    # Ê†ºÂºèÂåñÊÄªÂ§ßÂ∞è
    if [[ "${VERBOSE}" == "true" ]]; then
        local total_human_size=$(format_bytes "$total_size")
        echo "  üíæ ÊÄªÂ§ßÂ∞è: $total_human_size"
    fi
    echo "  üìÅ ÁõÆÂΩï: $OLLAMA_MODELS_DIR"
    
    # ÊòæÁ§∫Á£ÅÁõò‰ΩøÁî®ÊÉÖÂÜµ
    local disk_usage
    if disk_usage=$(du -sh "$OLLAMA_MODELS_DIR" 2>/dev/null); then
        echo "  üóÑÔ∏è Á£ÅÁõòÂç†Áî®: $(echo "$disk_usage" | cut -f1)"
    fi
    
    echo "=================================================================================="
    echo ""
    
    return 0
}

# Â§á‰ªΩOllamaÊ®°ÂûãÔºàÁõ¥Êé•Â§çÂà∂Ôºâ
backup_ollama_model() {
    local model_spec="$1"
    local backup_dir="$2"
    
    # ÂàùÂßãÂåñÁºìÂ≠ò‰ª•ÊèêÈ´òÂÆåÊï¥ÊÄßÊ£ÄÊü•ÊÄßËÉΩ
    ensure_cache_initialized
    
    # Ëß£ÊûêÊ®°ÂûãÂêçÁß∞ÂíåÁâàÊú¨
    local model_name model_version
    if ! parse_model_spec "$model_spec" model_name model_version; then
        return 1
    fi
    
    log_verbose "Â§á‰ªΩÊ®°Âûã: $model_name:$model_version"
    local model_spec="${model_name}:${model_version}"
    if ! verify_integrity "model" "$model_spec" "use_cache:true,check_blobs:true"; then
        log_error "Êú¨Âú∞Ê®°Âûã‰∏çÂÆåÊï¥ÔºåÂèñÊ∂àÂ§á‰ªΩÊìç‰Ωú"
        return 1
    fi
    
    # ÂàõÂª∫Â§á‰ªΩÁõÆÂΩïÂíåÁîüÊàêË∑ØÂæÑ
    local model_backup_dir
    model_backup_dir=$(create_model_backup_dir "$model_spec" "$backup_dir") || return 1
    local model_safe_name=$(get_safe_model_name "$model_spec")
    local backup_model_dir="$model_backup_dir/$model_safe_name"
    
    
    # Ê£ÄÊü•ÊòØÂê¶Â∑≤Â≠òÂú®Â§á‰ªΩÁõÆÂΩï
    if [[ -d "$backup_model_dir" ]]; then
        log_success "Ê®°ÂûãÂ§á‰ªΩÂ∑≤Â≠òÂú®"
        return 0
    fi
    
    # Á°ÆÂÆömanifestÊñá‰ª∂Ë∑ØÂæÑ
    local manifest_file
    if [[ "$model_name" == hf.co/* ]]; then
        # HuggingFace GGUFÊ®°ÂûãÔºåÂ¶Ç hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF
        manifest_file="$OLLAMA_MODELS_DIR/manifests/$model_name/$model_version"
    elif [[ "$model_name" == *"/"* ]]; then
        # Áî®Êà∑ÂàÜ‰∫´ÁöÑÊ®°ÂûãÔºåÂ¶Ç lrs33/bce-embedding-base_v1
        local user_name="${model_name%/*}"
        local repo_name="${model_name#*/}"
        manifest_file="$OLLAMA_MODELS_DIR/manifests/registry.ollama.ai/$user_name/$repo_name/$model_version"
    else
        # ÂÆòÊñπÊ®°Âûã
        manifest_file="$OLLAMA_MODELS_DIR/manifests/registry.ollama.ai/library/$model_name/$model_version"
    fi
    
    # Ê£ÄÊü•manifestÊñá‰ª∂ÊòØÂê¶Â≠òÂú®
    if [[ ! -f "$manifest_file" ]]; then
        log_error "Ê®°Âûã‰∏çÂ≠òÂú®: $model_spec"
        return 1
    fi
    
    # Ëé∑ÂèñblobÊñá‰ª∂Ë∑ØÂæÑ
    local blob_files
    blob_files=$(get_model_blob_paths "$manifest_file" "$OLLAMA_MODELS_DIR")
    
    if [[ -z "$blob_files" ]]; then
        log_error "Êú™ÊâæÂà∞Ê®°ÂûãÁõ∏ÂÖ≥ÁöÑblobÊñá‰ª∂"
        return 1
    fi
    
    # ÂàõÂª∫Â§á‰ªΩÁõÆÂΩïÁªìÊûÑ
    mkdir -p "$backup_model_dir/manifests"
    mkdir -p "$backup_model_dir/blobs"
    
    log_verbose "ÂºÄÂßãÂ§çÂà∂Êñá‰ª∂..."
    
    # Â§çÂà∂manifestÊñá‰ª∂
    local manifest_rel_path="${manifest_file#$OLLAMA_MODELS_DIR/manifests/}"
    local manifest_backup_dir="$backup_model_dir/manifests/$(dirname "$manifest_rel_path")"
    mkdir -p "$manifest_backup_dir"
    if ! cp "$manifest_file" "$manifest_backup_dir/"; then
        log_error "Â§çÂà∂manifestÊñá‰ª∂Â§±Ë¥•: $manifest_file"
        rm -rf "$backup_model_dir"
        return 1
    fi
    
    # Â§çÂà∂blobÊñá‰ª∂
    while IFS= read -r blob_file; do
        if [[ -f "$blob_file" ]]; then
            local blob_name=$(basename "$blob_file")
            if ! cp "$blob_file" "$backup_model_dir/blobs/"; then
                log_error "Â§çÂà∂blobÊñá‰ª∂Â§±Ë¥•: $blob_file"
                rm -rf "$backup_model_dir"
                return 1
            fi
        fi
    done <<< "$blob_files"
    
    # ËÆ°ÁÆóMD5Ê†°È™å
    log_verbose "ËÆ°ÁÆóMD5Ê†°È™åÂÄº..."
    local md5_file="${backup_model_dir}.md5"
    if calculate_directory_md5 "$backup_model_dir" "$md5_file"; then
        log_verbose "MD5Ê†°È™åÊñá‰ª∂Â∑≤ÂàõÂª∫: $md5_file"
    else
        log_warning "MD5Ê†°È™åÊñá‰ª∂ÂàõÂª∫Â§±Ë¥•"
    fi
    
    # ÂàõÂª∫Â§á‰ªΩ‰ø°ÊÅØÊñá‰ª∂
    create_backup_info "$model_spec" "$backup_model_dir" "directory" 1 "ollama"
    
    log_verbose_success "Ê®°ÂûãÂ§á‰ªΩÂÆåÊàê: $model_spec"
    return 0
}



# Êô∫ËÉΩÂà†Èô§Ê®°ÂûãÔºàËá™Âä®ËØÜÂà´Ê®°ÂûãÁ±ªÂûãÔºâ
remove_model_smart() {
    local model_input="$1"
    local force_delete="${2:-false}"
    
    log_info "Âà†Èô§Ê®°Âûã: $model_input"
    
    # Ê£ÄÊü•ËæìÂÖ•Ê†ºÂºèÔºåÂà§Êñ≠ÊòØ‰ªÄ‰πàÁ±ªÂûãÁöÑÊ®°Âûã
    if [[ "$model_input" =~ ^([^:]+):(.+)$ ]]; then
        local model_name="${BASH_REMATCH[1]}"
        local model_tag_or_quant="${BASH_REMATCH[2]}"
        
        # ÂÖàÊ£ÄÊü•ÊòØÂê¶ÊòØOllamaÊ®°ÂûãÔºàÁõ¥Êé•Ê†ºÂºèÔºömodel:tagÔºâ
        if check_ollama_model "$model_name" "$model_tag_or_quant"; then
            if remove_ollama_model "$model_input" "$force_delete"; then
                return 0
            else
                return 1
            fi
        fi
        
        # Ê£ÄÊü•ÊòØÂê¶ÊòØGGUFÊ®°ÂûãÔºàÁîüÊàêÁöÑOllamaÊ®°ÂûãÂêçÔºâ
        local generated_name=$(generate_ollama_model_name "$model_name" "$model_tag_or_quant")
        
        # Áªü‰∏ÄÂà†Èô§Â§ÑÁêÜ
        if remove_ollama_model "$generated_name" "$force_delete"; then
            return 0
        else
            return 1
        fi
        
    else
        log_error "Ê®°ÂûãÊ†ºÂºèÈîôËØØÔºåÂ∫î‰∏∫ 'Ê®°ÂûãÂêç:ÁâàÊú¨' Êàñ 'Ê®°ÂûãÂêç:ÈáèÂåñÁ±ªÂûã'"
        log_error "‰æãÂ¶Ç: 'llama2:7b' Êàñ 'microsoft/DialoGPT-small:q4_0'"
        return 1
    fi
}

# Ê£ÄÊµãÂ§á‰ªΩÊñá‰ª∂Á±ªÂûã




# ÊÅ¢Â§çOllamaÊ®°ÂûãÔºàÁõÆÂΩïÂ§á‰ªΩÔºâ
restore_ollama_model() {
    local backup_dir="$1"
    local force_restore="$2"
    
    log_info "ÊÅ¢Â§çÊ®°Âûã: $(basename "$backup_dir")"
    
    # Ê£ÄÊü•Â§á‰ªΩÁõÆÂΩïÊòØÂê¶Â≠òÂú®
    if [[ ! -d "$backup_dir" ]]; then
        log_error "Â§á‰ªΩÊñá‰ª∂‰∏çÂ≠òÂú®: $backup_dir"
        return 1
    fi
    
    # Ê£ÄÊü•Â§á‰ªΩÁõÆÂΩïÁªìÊûÑ
    if [[ ! -d "$backup_dir/manifests" ]] || [[ ! -d "$backup_dir/blobs" ]]; then
        log_error "Â§á‰ªΩÊñá‰ª∂ÊçüÂùèÊàñÊ†ºÂºèÈîôËØØ"
        return 1
    fi
    
    # MD5Ê†°È™å
    local md5_file="${backup_dir}.md5"
    if [[ -f "$md5_file" ]]; then
        log_info "Ê†°È™åÂ§á‰ªΩÊñá‰ª∂..."
        if verify_directory_md5 "$backup_dir" "$md5_file"; then
            log_verbose_success "MD5Ê†°È™åÈÄöËøá"
        else
            log_error "Â§á‰ªΩÊñá‰ª∂Ê†°È™åÂ§±Ë¥•ÔºåÂèØËÉΩÂ∑≤ÊçüÂùè"
            if [[ "$force_restore" != "true" ]]; then
                return 1
            fi
            log_warning "Âº∫Âà∂ÊÅ¢Â§çÊ®°ÂºèÔºåÁªßÁª≠Êìç‰Ωú..."
        fi
    else
        log_warning "Ë∑≥ËøáÂÆåÊï¥ÊÄßÊ†°È™å"
    fi
    
    # Ê£ÄÊü•ÊòØÂê¶ÈúÄË¶ÅÂº∫Âà∂Ë¶ÜÁõñ
    if [[ "$force_restore" != "true" ]]; then
        log_info "Ê£ÄÊü•Ê®°ÂûãÂÜ≤Á™Å..."
        local conflicts_found=false
        
        # Ê£ÄÊü•manifestsÂÜ≤Á™Å
        if find "$backup_dir/manifests" -type f 2>/dev/null | while read -r manifest_file; do
            local rel_path="${manifest_file#$backup_dir/manifests/}"
            local target_file="$OLLAMA_MODELS_DIR/manifests/$rel_path"
            if [[ -f "$target_file" ]]; then
                echo "conflict"
                break
            fi
        done | grep -q "conflict"; then
            conflicts_found=true
        fi
        
        # Ê£ÄÊü•blobsÂÜ≤Á™Å
        if find "$backup_dir/blobs" -type f 2>/dev/null | while read -r blob_file; do
            local blob_name=$(basename "$blob_file")
            local target_file="$OLLAMA_MODELS_DIR/blobs/$blob_name"
            if [[ -f "$target_file" ]]; then
                echo "conflict"
                break
            fi
        done | grep -q "conflict"; then
            conflicts_found=true
        fi
        
        if [[ "$conflicts_found" == "true" ]]; then
            log_error "Ê£ÄÊµãÂà∞Êñá‰ª∂ÂÜ≤Á™ÅÔºå‰ΩøÁî® --force Âº∫Âà∂Ë¶ÜÁõñ"
            return 1
        fi
    fi
    
    # ‰ΩøÁî®DockerÁ°Æ‰øùÁõÆÊ†áÁõÆÂΩïÂ≠òÂú®Âπ∂ÊúâÊ≠£Á°ÆÊùÉÈôê
    ensure_hf_downloader_image || return 1
    
    # ‰ΩøÁî®DockerÂàõÂª∫OllamaÁõÆÂΩïÂπ∂ËÆæÁΩÆÊùÉÈôê
    if ! docker run --rm --entrypoint="" --user root \
        -v "$OLLAMA_MODELS_DIR:/ollama" \
        "$FULL_IMAGE_NAME" \
        sh -c "mkdir -p /ollama/manifests /ollama/blobs"; then
        log_error "Êó†Ê≥ïÂàõÂª∫OllamaÁõÆÂΩï"
        return 1
    fi
    
    # Â§çÂà∂manifests
    log_verbose "ÊÅ¢Â§çÊ®°Âûã‰ø°ÊÅØ..."
    if ! docker run --rm --entrypoint="" --user root \
        -v "$backup_dir:/backup" \
        -v "$OLLAMA_MODELS_DIR:/ollama" \
        "$FULL_IMAGE_NAME" \
        sh -c "cp -r /backup/manifests/* /ollama/manifests/"; then
        log_error "manifestÊñá‰ª∂Â§çÂà∂Â§±Ë¥•"
        return 1
    fi
    
    # Â§çÂà∂blobs
    log_verbose "ÊÅ¢Â§çÊ®°ÂûãÊï∞ÊçÆ..."
    if ! docker run --rm --entrypoint="" --user root \
        -v "$backup_dir:/backup" \
        -v "$OLLAMA_MODELS_DIR:/ollama" \
        "$FULL_IMAGE_NAME" \
        sh -c "cp /backup/blobs/* /ollama/blobs/"; then
        log_error "blobÊñá‰ª∂Â§çÂà∂Â§±Ë¥•"
        return 1
    fi
    
    log_verbose_success "Ê®°ÂûãÊÅ¢Â§çÂÆåÊàê"
    return 0
}

# Ëá™Âä®ËØÜÂà´Â§á‰ªΩÁ±ªÂûãÂπ∂ÊÅ¢Â§ç
# ÊâπÈáèÂ§á‰ªΩÊ®°ÂûãÔºàÊ†πÊçÆmodels.listÊñá‰ª∂Ôºâ
backup_models_from_list() {
    local models_file="$1"
    local backup_dir="$2"
    
    log_verbose "ÊâπÈáèÂ§á‰ªΩÊ®°Âûã..."
    log_verbose "Ê®°ÂûãÂàóË°®Êñá‰ª∂: $models_file"
    log_verbose "Â§á‰ªΩÁõÆÂΩï: $backup_dir"
    
    # Ëß£ÊûêÊ®°ÂûãÂàóË°®
    local models=()
    parse_models_list "$models_file" models
    
    if [[ ${#models[@]} -eq 0 ]]; then
        log_warning "Ê≤°ÊúâÊâæÂà∞‰ªª‰ΩïÊ®°ÂûãËøõË°åÂ§á‰ªΩ"
        return 1
    fi
    
    # ÂàõÂª∫Â§á‰ªΩÁõÆÂΩï
    mkdir -p "$backup_dir"
    
    local total_models=${#models[@]}
    local processed=0
    local success=0
    local failed=0
    
    log_verbose "ÂÖ±ÊâæÂà∞ $total_models ‰∏™Ê®°ÂûãËøõË°åÂ§á‰ªΩ"
    
    # È¢ÑÂÖàÂàùÂßãÂåñOllamaÁºìÂ≠òÔºåÈÅøÂÖçÊØè‰∏™Ê®°ÂûãÈÉΩÈáçÊñ∞ÂàùÂßãÂåñ
    local has_ollama_models=false
    for model in "${models[@]}"; do
        if [[ "$model" =~ ^ollama: ]] || [[ "$model" =~ ^hf-gguf: ]]; then
            has_ollama_models=true
            break
        fi
    done
    
    if [[ "$has_ollama_models" == "true" ]]; then
        log_verbose "Ê£ÄÊµãÂà∞OllamaÊ®°ÂûãÔºåÈ¢ÑÂÖàÂàùÂßãÂåñÊ®°ÂûãÁºìÂ≠ò..."
        if ! init_ollama_cache; then
            log_error "OllamaÁºìÂ≠òÂàùÂßãÂåñÂ§±Ë¥•ÔºåÂèØËÉΩÂΩ±ÂìçÂ§á‰ªΩÊÄßËÉΩ"
        fi
    fi
    
    for model in "${models[@]}"; do
        ((processed++))
        log_info "Â§á‰ªΩÊ®°Âûã [$processed/$total_models]: $model"
        
        # Ëß£ÊûêÊ®°ÂûãÊù°ÁõÆ
        if [[ "$model" =~ ^ollama:([^:]+):(.+)$ ]]; then
            local model_name="${BASH_REMATCH[1]}"
            local model_tag="${BASH_REMATCH[2]}"
            local model_spec="${model_name}:${model_tag}"
            
            
            # Ê£ÄÊü•Ê®°ÂûãÊòØÂê¶Â≠òÂú®
            if check_ollama_model "$model_name" "$model_tag"; then
                if backup_ollama_model "$model_spec" "$backup_dir"; then
                    ((success++))
                else
                    ((failed++))
                fi
            else
                ((failed++))
            fi
            
        elif [[ "$model" =~ ^hf-gguf:(.+)$ ]]; then
            local model_full_name="${BASH_REMATCH[1]}"
            
            # Ëß£ÊûêHuggingFace GGUFÊ®°ÂûãÂêçÁß∞
            if [[ "$model_full_name" =~ ^(.+):(.+)$ ]]; then
                local model_name="${BASH_REMATCH[1]}"
                local model_tag="${BASH_REMATCH[2]}"
            else
                local model_name="$model_full_name"
                local model_tag="latest"
            fi
            
            local model_spec="${model_name}:${model_tag}"
            
            # Ê£ÄÊü•HF GGUFÊ®°ÂûãÊòØÂê¶Â≠òÂú®
            if check_hf_gguf_model "$model_name" "$model_tag"; then
                if backup_ollama_model "$model_spec" "$backup_dir"; then
                    ((success++))
                else
                    ((failed++))
                fi
            else
                ((failed++))
            fi
            
        elif [[ "$model" =~ ^huggingface:([^:]+):(.+)$ ]]; then
            local model_name="${BASH_REMATCH[1]}"
            local quantize_type="${BASH_REMATCH[2]}"
            
            log_verbose "Â§á‰ªΩHuggingFaceÊ®°Âûã: $model_name (ÈáèÂåñ: $quantize_type)"
            
            # Ê£ÄÊü•HuggingFaceÊ®°ÂûãÊòØÂê¶Â≠òÂú®‰∫éOllama‰∏≠
            if check_huggingface_model_in_ollama "$model_name" "$quantize_type"; then
                local ollama_model_name=$(generate_ollama_model_name "$model_name" "$quantize_type")
                if backup_ollama_model "$ollama_model_name" "$backup_dir"; then
                    ((success++))
                    log_verbose_success "HuggingFaceÊ®°ÂûãÂ§á‰ªΩÊàêÂäü: $model_name"
                else
                    ((failed++))
                    log_error "HuggingFaceÊ®°ÂûãÂ§á‰ªΩÂ§±Ë¥•: $model_name"
                fi
            else
                log_warning "HuggingFaceÊ®°Âûã‰∏çÂ≠òÂú®ÔºåË∑≥ËøáÂ§á‰ªΩ: $model_name"
                ((failed++))
            fi
            
        else
            log_error "Êó†ÊïàÁöÑÊ®°ÂûãÊù°ÁõÆÊ†ºÂºè: $model"
            ((failed++))
        fi
        
        echo "" # Ê∑ªÂä†Á©∫Ë°åÂàÜÈöî
    done
    
    # ÊòæÁ§∫Â§á‰ªΩÊÄªÁªì
    log_verbose_success "ÊâπÈáèÂ§á‰ªΩÂÆåÊàê ($success/$total_models)"
    if [[ $failed -gt 0 ]]; then
        log_warning "Â§á‰ªΩÂ§±Ë¥•: $failed"
        return 1
    fi
    
    # ÊòæÁ§∫Â§á‰ªΩÁõÆÂΩï‰ø°ÊÅØ
    if [[ "${VERBOSE}" == "true" ]] && [[ -d "$backup_dir" ]]; then
        # Âè™ÁªüËÆ°È°∂Á∫ßÊ®°ÂûãÁõÆÂΩïÔºåÊéíÈô§Â≠êÁõÆÂΩï
        local backup_count=$(find "$backup_dir" -maxdepth 1 -type d ! -path "$backup_dir" | wc -l)
        local total_size=$(du -sh "$backup_dir" 2>/dev/null | cut -f1)
        log_info "Â§á‰ªΩÁõÆÂΩï‰∏ãÂÖ±Êúâ: $backup_count ‰∏™Ê®°ÂûãÔºåÊÄªÂ§ßÂ∞è: $total_size"
    fi
    
    # Ê∏ÖÁêÜÂÆåÊï¥ÊÄßÊ£ÄÊü•ÁºìÂ≠ò
    clear_integrity_cache
    
    if [[ $failed -eq 0 ]]; then
        log_verbose_success "ÂÖ®ÈÉ®Ê®°ÂûãÂ§á‰ªΩÂÆåÊàê"
        return 0
    else
        log_warning "ÈÉ®ÂàÜÊ®°ÂûãÂ§á‰ªΩÂ§±Ë¥•"
        return 1
    fi
}

# ÊâπÈáèÂà†Èô§Ê®°ÂûãÔºàÊ†πÊçÆmodels.listÊñá‰ª∂Ôºâ
remove_models_from_list() {
    local models_file="$1"
    local force_delete="${2:-false}"
    
    log_verbose "ÊâπÈáèÂà†Èô§Ê®°Âûã..."
    log_verbose "Ê®°ÂûãÂàóË°®Êñá‰ª∂: $models_file"
    log_info "Âº∫Âà∂Âà†Èô§Ê®°Âºè: $force_delete"
    
    # Ëß£ÊûêÊ®°ÂûãÂàóË°®
    local models=()
    parse_models_list "$models_file" models
    
    if [[ ${#models[@]} -eq 0 ]]; then
        log_warning "Ê≤°ÊúâÊâæÂà∞‰ªª‰ΩïÊ®°ÂûãËøõË°åÂà†Èô§"
        return 1
    fi
    
    local total_models=${#models[@]}
    local processed=0
    local success=0
    local failed=0
    
    log_verbose "ÂÖ±ÊâæÂà∞ $total_models ‰∏™Ê®°ÂûãËøõË°åÂà†Èô§"
    
    # Â¶ÇÊûú‰∏çÊòØÂº∫Âà∂Âà†Èô§ÔºåÊòæÁ§∫Ë¶ÅÂà†Èô§ÁöÑÊ®°ÂûãÂàóË°®Âπ∂ËØ∑Ê±ÇÁ°ÆËÆ§
    if [[ "$force_delete" != "true" ]]; then
        log_warning "Âç≥Â∞ÜÂà†Èô§‰ª•‰∏ãÊ®°ÂûãÔºö"
        for model in "${models[@]}"; do
            if [[ "$model" =~ ^ollama:([^:]+):(.+)$ ]]; then
                local model_name="${BASH_REMATCH[1]}"
                local model_tag="${BASH_REMATCH[2]}"
                echo "  - OllamaÊ®°Âûã: ${model_name}:${model_tag}"
            elif [[ "$model" =~ ^huggingface:([^:]+):(.+)$ ]]; then
                local model_name="${BASH_REMATCH[1]}"
                local quantize_type="${BASH_REMATCH[2]}"
                echo "  - GGUFÊ®°Âûã: $model_name ($quantize_type)"
            elif [[ "$model" =~ ^hf-gguf:(.+)$ ]]; then
                local model_full_name="${BASH_REMATCH[1]}"
                echo "  - HuggingFace GGUFÊ®°Âûã: $model_full_name"
            fi
        done
        echo ""
        echo -n "Á°ÆËÆ§Âà†Èô§ÊâÄÊúâËøô‰∫õÊ®°ÂûãÔºü[y/N]: "
        read -r confirm
        if [[ "$confirm" != "y" && "$confirm" != "Y" ]]; then
            log_info "ÂèñÊ∂àÊâπÈáèÂà†Èô§Êìç‰Ωú"
            return 2  # ÁâπÊÆäÈÄÄÂá∫Á†ÅË°®Á§∫Áî®Êà∑ÂèñÊ∂à
        fi
        echo ""
    fi
    
    for model in "${models[@]}"; do
        ((processed++))
        log_info "Âà†Èô§Ê®°Âûã [$processed/$total_models]: $model"
        
        # Ëß£ÊûêÊ®°ÂûãÊù°ÁõÆ
        if [[ "$model" =~ ^ollama:([^:]+):(.+)$ ]]; then
            local model_name="${BASH_REMATCH[1]}"
            local model_tag="${BASH_REMATCH[2]}"
            local model_spec="${model_name}:${model_tag}"
            
            log_verbose "Âà†Èô§OllamaÊ®°Âûã: $model_spec"
            
            if remove_ollama_model "$model_spec" "true"; then
                ((success++))
                log_verbose_success "OllamaÊ®°ÂûãÂà†Èô§ÊàêÂäü: $model_spec"
            else
                ((failed++))
                log_error "OllamaÊ®°ÂûãÂà†Èô§Â§±Ë¥•: $model_spec"
            fi
            
        elif [[ "$model" =~ ^huggingface:([^:]+):(.+)$ ]]; then
            local model_name="${BASH_REMATCH[1]}"
            local quantize_type="${BASH_REMATCH[2]}"
            
            log_verbose "Âà†Èô§HuggingFace GGUFÊ®°Âûã: $model_name ($quantize_type)"
            
            # ÁîüÊàêÂØπÂ∫îÁöÑOllamaÊ®°ÂûãÂêçÁß∞
            local ollama_model_name=$(generate_ollama_model_name "$model_name" "$quantize_type")
            if remove_ollama_model "$ollama_model_name" "true"; then
                ((success++))
                log_verbose_success "GGUFÊ®°ÂûãÂà†Èô§ÊàêÂäü: $model_name ($quantize_type)"
            else
                ((failed++))
                log_error "GGUFÊ®°ÂûãÂà†Èô§Â§±Ë¥•: $model_name ($quantize_type)"
            fi
            
        elif [[ "$model" =~ ^hf-gguf:(.+)$ ]]; then
            local model_full_name="${BASH_REMATCH[1]}"
            
            # Ëß£ÊûêHuggingFace GGUFÊ®°ÂûãÂêçÁß∞
            if [[ "$model_full_name" =~ ^(.+):(.+)$ ]]; then
                local model_name="${BASH_REMATCH[1]}"
                local model_tag="${BASH_REMATCH[2]}"
            else
                local model_name="$model_full_name"
                local model_tag="latest"
            fi
            
            local model_spec="${model_name}:${model_tag}"
            log_verbose "Âà†Èô§HuggingFace GGUFÊ®°Âûã: $model_spec"
            
            if remove_ollama_model "$model_spec" "true"; then
                ((success++))
                log_verbose_success "HuggingFace GGUFÊ®°ÂûãÂà†Èô§ÊàêÂäü: $model_spec"
            else
                ((failed++))
                log_error "HuggingFace GGUFÊ®°ÂûãÂà†Èô§Â§±Ë¥•: $model_spec"
            fi
            
        else
            log_error "Êó†ÊïàÁöÑÊ®°ÂûãÊù°ÁõÆÊ†ºÂºè: $model"
            ((failed++))
        fi
        
        echo "" # Ê∑ªÂä†Á©∫Ë°åÂàÜÈöî
    done
    
    # ÊòæÁ§∫Âà†Èô§ÊÄªÁªì  
    log_verbose_success "ÊâπÈáèÂà†Èô§ÂÆåÊàê ($success/$total_models)"
    if [[ $failed -gt 0 ]]; then
        log_warning "Âà†Èô§Â§±Ë¥•: $failed"
    fi
    
    if [[ $failed -eq 0 ]]; then
        log_verbose_success "ÂÖ®ÈÉ®Ê®°ÂûãÂà†Èô§ÂÆåÊàê"
        return 0
    else
        log_warning "ÈÉ®ÂàÜÊ®°ÂûãÂà†Èô§Â§±Ë¥•"
        return 1
    fi
}

# Ê£ÄÊü•Ollama‰∏≠ÊòØÂê¶Â≠òÂú®ÊåáÂÆöÊ®°Âûã
# Ê£ÄÊü•HuggingFaceÊ®°ÂûãÊòØÂê¶Â∑≤Â≠òÂú®‰∫éOllama‰∏≠ÔºàÊô∫ËÉΩÂåπÈÖçÔºâ
check_huggingface_model_in_ollama() {
    local model_name="$1"
    local quantize_type="$2"
    
    log_verbose "Ê£ÄÊü•HuggingFaceÊ®°Âûã: $model_name ($quantize_type)"
    
    # ÁîüÊàêÊúüÊúõÁöÑOllamaÊ®°ÂûãÂêçÁß∞ÔºàÂ∏¶hf-ÂâçÁºÄÔºâ
    local expected_ollama_name=$(generate_ollama_model_name "$model_name" "$quantize_type")
    
    # ‰ΩøÁî®ÁÆÄÂåñÁöÑÂÆπÂô®Ê£ÄÊü•
    if check_ollama_model_exists "$expected_ollama_name"; then
        log_verbose_success "ÊâæÂà∞ÂåπÈÖçÁöÑOllamaÊ®°Âûã: $expected_ollama_name"
        return 0
    fi
    
    log_verbose_warning "Êú™ÊâæÂà∞ÂåπÈÖçÁöÑOllamaÊ®°Âûã: $expected_ollama_name"
    return 1
}

# ‰ªéHuggingFaceÂéüÂßãÂ§á‰ªΩÊÅ¢Â§çÂπ∂ÈáçÊñ∞ËΩ¨Êç¢
restore_and_reconvert_hf_model() {
    local model_name="$1"
    local quantize_type="$2"
    local skip_md5_check="${3:-false}"  # Êñ∞Â¢ûÂèÇÊï∞ÔºåÈªòËÆ§‰∏∫false
    
    log_info "‰ªéÂéüÂßãÂ§á‰ªΩÊÅ¢Â§çÂπ∂ÈáçÊñ∞ËΩ¨Êç¢: $model_name ($quantize_type)"
    
    # ÁîüÊàêÊñá‰ª∂Á≥ªÁªüÂÆâÂÖ®ÁöÑÊ®°ÂûãÂêçÁß∞
    local model_safe_name=$(get_safe_model_name "$model_name" "filesystem")
    local model_backup_dir="${ABS_HF_ORIGINAL_BACKUP_DIR}/${model_safe_name}"
    local backup_dir="${model_backup_dir}/${model_safe_name}_original"
    
    # Ê£ÄÊü•Â§á‰ªΩÁõÆÂΩï
    if [[ ! -d "$backup_dir" ]]; then
        log_verbose_warning "Êú™ÊâæÂà∞Â§á‰ªΩÁõÆÂΩï: $backup_dir"
        return 1
    fi
    
    # MD5Ê†°È™åÔºàÂ¶ÇÊûúÊ≤°ÊúâË∑≥ËøáÁöÑËØùÔºâ
    if [[ "$skip_md5_check" != "true" ]]; then
        local md5_file="${backup_dir}.md5"
        if [[ -f "$md5_file" ]]; then
            log_info "Ê≠£Âú®È™åËØÅMD5Ê†°È™åÂÄº..."
            if verify_directory_md5 "$backup_dir" "$md5_file"; then
                log_verbose_success "MD5Ê†°È™åÈÄöËøá"
            else
                log_error "MD5Ê†°È™åÂ§±Ë¥•ÔºåÂ§á‰ªΩÂèØËÉΩÂ∑≤ÊçüÂùè"
                return 1
            fi
        else
            log_warning "Êú™ÊâæÂà∞MD5Ê†°È™åÊñá‰ª∂ÔºåË∑≥ËøáÊ†°È™å"
        fi
    fi
    
    # ÂàõÂª∫‰∏¥Êó∂ÁõÆÂΩïËøõË°åÊÅ¢Â§ç
    local restore_temp_dir=$(mktemp -d) || { log_error "Êó†Ê≥ïÂàõÂª∫‰∏¥Êó∂ÁõÆÂΩï"; return 1; }
    
    cleanup_restore_temp() { [[ -d "${restore_temp_dir:-}" ]] && rm -rf "$restore_temp_dir"; }
    add_cleanup_function "cleanup_restore_temp"
    
    # Áõ¥Êé•Â§çÂà∂Â§á‰ªΩÁõÆÂΩïÂà∞‰∏¥Êó∂ÁõÆÂΩï
    log_info "ÊÅ¢Â§çÊ®°ÂûãÊñá‰ª∂..."
    local restored_model_dir="$restore_temp_dir/restored_model"
    if ! cp -r "$backup_dir" "$restored_model_dir"; then
        log_error "Â§á‰ªΩÊÅ¢Â§çÂ§±Ë¥•"
        cleanup_restore_temp
        remove_cleanup_function "cleanup_restore_temp"
        return 1
    fi
    
    # Â∞ÜÊÅ¢Â§çÁöÑÊ®°ÂûãÂ§çÂà∂Âà∞ÁºìÂ≠òÁõÆÂΩï‰æõËΩ¨Êç¢ËÑöÊú¨‰ΩøÁî®
    local cache_model_dir="${ABS_HF_DOWNLOAD_CACHE_DIR}/${model_safe_name}"
    
    # Ê∏ÖÁêÜÊóßÁºìÂ≠òÂπ∂Â§çÂà∂ÊÅ¢Â§çÁöÑÊ®°Âûã
    [[ -d "$cache_model_dir" ]] && rm -rf "$cache_model_dir"
    if ! cp -r "$restored_model_dir" "$cache_model_dir"; then
        log_error "Ê®°ÂûãÂ§çÂà∂Â§±Ë¥•"
        cleanup_restore_temp
        remove_cleanup_function "cleanup_restore_temp"
        return 1
    fi
    
    # ÊûÑÂª∫Âπ∂ÊâßË°åËΩ¨Êç¢ÂëΩ‰ª§ÔºåÁõ¥Êé•‰ΩøÁî®restore_temp_dir‰Ωú‰∏∫ËæìÂá∫ÁõÆÂΩï
    local container_name="llm-reconvert-$$"
    local docker_cmd=()
    mapfile -t docker_cmd < <(build_full_docker_cmd "$container_name" "true" "false" \
        --volume "${restore_temp_dir}:/app/models" \
        --volume "${ABS_HF_DOWNLOAD_CACHE_DIR}:/app/download_cache")
    
    [[ ${#docker_cmd[@]} -eq 0 ]] && { log_error "DockerÂëΩ‰ª§ÊûÑÂª∫Â§±Ë¥•"; return 1; }
    
    docker_cmd+=("${FULL_IMAGE_NAME}" "${model_name}" "--quantize" "${quantize_type}" "--gguf-dir" "/app/models")
    [[ "${VERBOSE}" == "true" ]] && docker_cmd+=("--verbose")
    
    # ÊâßË°åËΩ¨Êç¢
    local conversion_result=0
    log_info "ÂºÄÂßãÈáçÊñ∞ËΩ¨Êç¢Ê®°Âûã..."
    
    if "${docker_cmd[@]}" >/dev/null 2>&1; then
        # ÂØºÂÖ•Âà∞OllamaÔºå‰ΩøÁî®restore_temp_dirÊü•ÊâæGGUFÊñá‰ª∂
        if import_gguf_to_ollama_from_temp "$model_name" "$quantize_type" "$restore_temp_dir"; then
            log_success "Ê®°ÂûãÊÅ¢Â§ç„ÄÅËΩ¨Êç¢Âπ∂ÂØºÂÖ•ÂÆåÊàê: $model_name"
            conversion_result=0
        else
            log_error "ËΩ¨Êç¢ÊàêÂäü‰ΩÜÂØºÂÖ•OllamaÂ§±Ë¥•"
            conversion_result=1
        fi
    else
        log_error "Ê®°ÂûãËΩ¨Êç¢Â§±Ë¥•: $model_name"
        conversion_result=1
    fi
    
    # Ê∏ÖÁêÜÁºìÂ≠ò
    [[ -d "$cache_model_dir" ]] && rm -rf "$cache_model_dir" 2>/dev/null
    cleanup_restore_temp
    remove_cleanup_function "cleanup_restore_temp"
    
    return $conversion_result
}

# Ê£ÄÊü•Ollama‰∏≠ÊòØÂê¶Â≠òÂú®ÊåáÂÆöÊ®°ÂûãÔºàÈÄöÁî®ÂáΩÊï∞Ôºâ


# ‰ªé‰∏¥Êó∂ÁõÆÂΩïÂØºÂÖ•GGUFÊ®°ÂûãÂà∞Ollama
import_gguf_to_ollama_from_temp() {
    local model_name="$1"
    local quantize_type="$2"
    local temp_dir="$3"
    
    log_verbose "ÂºÄÂßã‰ªé‰∏¥Êó∂ÁõÆÂΩïÂØºÂÖ•GGUFÊ®°ÂûãÂà∞Ollama: $model_name ($quantize_type)"
    
    # Êü•Êâæ‰∏¥Êó∂ÁõÆÂΩï‰∏≠ÁöÑGGUFÊñá‰ª∂
    local gguf_file=$(find "$temp_dir" -name "*.gguf" -type f | head -n1)
    if [[ ! -f "$gguf_file" ]]; then
        log_error "Âú®‰∏¥Êó∂ÁõÆÂΩï‰∏≠Êú™ÊâæÂà∞GGUFÊñá‰ª∂: $temp_dir"
        return 1
    fi
    
    log_info "ÊâæÂà∞GGUFÊñá‰ª∂: $gguf_file"
    
    # ÁîüÊàêOllamaÊ®°ÂûãÂêçÁß∞ÔºàÂ∏¶hf-ÂâçÁºÄÔºâ
    local ollama_model_name=$(generate_ollama_model_name "$model_name" "$quantize_type")
    log_verbose "OllamaÊ®°ÂûãÂêçÁß∞: $ollama_model_name"
    
    # Ê£ÄÊü•Ê®°ÂûãÊòØÂê¶Â∑≤Â≠òÂú®‰∫éOllama‰∏≠
    if check_ollama_model_exists "$ollama_model_name"; then
        log_success "Ê®°ÂûãÂ∑≤Â≠òÂú®‰∫éOllama‰∏≠ÔºåË∑≥ËøáÂØºÂÖ•: $ollama_model_name"
        return 0
    fi
    
    # ÂàõÂª∫‰∏¥Êó∂Modelfile
    local temp_modelfile
    temp_modelfile=$(mktemp) || {
        log_error "Êó†Ê≥ïÂàõÂª∫‰∏¥Êó∂Modelfile"
        return 1
    }
    cat > "$temp_modelfile" << EOF
FROM ${gguf_file}
TEMPLATE """{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
"""
PARAMETER stop "<|im_end|>"
PARAMETER temperature 0.7
PARAMETER top_p 0.9
EOF
    
    log_verbose "ÂàõÂª∫Modelfile: $temp_modelfile"
    
    # ‰ΩøÁî®‰∏¥Êó∂ÂÆπÂô®ÂØºÂÖ•GGUFÊ®°Âûã
    log_verbose "ÂêØÂä®‰∏¥Êó∂ÂÆπÂô®ÂØºÂÖ•GGUFÊ®°Âûã"
    local import_name="ollama-import-$$"
    
    # ÂÆö‰πâÊ∏ÖÁêÜÂáΩÊï∞
    cleanup_import_container() {
        if docker ps -a --format "{{.Names}}" | grep -q "^${import_name}$"; then
            docker rm -f "$import_name" > /dev/null 2>&1
        fi
        rm -f "$temp_modelfile"
    }
    
    
    # ËÆæÁΩÆ‰ø°Âè∑Â§ÑÁêÜ
    add_cleanup_function "cleanup_import_container"
    
    # Ëé∑ÂèñÁªùÂØπË∑ØÂæÑ
    local abs_ollama_dir
    # ÂØπ‰∫éOllamaÂÆπÂô®ÔºåÈúÄË¶ÅÊåÇËΩΩÁöÑÊòØ.ollamaÁõÆÂΩïÔºàÂç≥dataÁõÆÂΩïÔºâÔºåËÄå‰∏çÊòØdata/models
    abs_ollama_dir="$ABS_OLLAMA_DATA_DIR"
    
    # ÂêØÂä®‰∏¥Êó∂ÂÆπÂô®
    local import_cmd=("docker" "run" "-d" "--name" "$import_name")
    
    # Ê∑ªÂä†GPUÈÖçÁΩÆ
    import_cmd+=("--gpus" "all")
    
    # Ê∑ªÂä†Âç∑ÊåÇËΩΩ
    import_cmd+=("-v" "${abs_ollama_dir}:/root/.ollama")
    import_cmd+=("-p" "11434:11434")  # ‰ΩøÁî®Âõ∫ÂÆöÁ´ØÂè£Êò†Â∞Ñ
    import_cmd+=("$DOCKER_IMAGE_OLLAMA")
    
    if ! "${import_cmd[@]}"; then
        log_error "Êó†Ê≥ïÂêØÂä®‰∏¥Êó∂ÂØºÂÖ•ÂÆπÂô®"
        rm -f "$temp_modelfile"
        return 1
    fi
    
    # Á≠âÂæÖÊúçÂä°Â∞±Áª™
    local max_attempts=30
    local attempt=0
    
    while (( attempt < max_attempts )); do
        if docker exec "$import_name" ollama list > /dev/null 2>&1; then
            break
        fi
        sleep 2
        ((attempt++))
    done
    
    if (( attempt >= max_attempts )); then
        log_error "Á≠âÂæÖÂØºÂÖ•ÂÆπÂô®ÊúçÂä°Ë∂ÖÊó∂"
        docker rm -f "$import_name" > /dev/null 2>&1
        rm -f "$temp_modelfile"
        return 1
    fi
    
    # Â∞ÜGGUFÊñá‰ª∂ÂíåModelfileÂ§çÂà∂Âà∞ÂÆπÂô®‰∏≠
    local container_gguf_path="/tmp/$(basename "$gguf_file")"
    local container_modelfile="/tmp/Modelfile-$$"
    
    if ! docker cp "$gguf_file" "$import_name:$container_gguf_path"; then
        log_error "Êó†Ê≥ïÂ∞ÜGGUFÊñá‰ª∂Â§çÂà∂Âà∞ÂÆπÂô®"
        docker rm -f "$import_name" > /dev/null 2>&1
        rm -f "$temp_modelfile"
        return 1
    fi
    
    # Êõ¥Êñ∞Modelfile‰∏≠ÁöÑË∑ØÂæÑ‰∏∫ÂÆπÂô®ÂÜÖË∑ØÂæÑ
    sed -i "s|FROM .*|FROM $container_gguf_path|" "$temp_modelfile"
    
    if ! docker cp "$temp_modelfile" "$import_name:$container_modelfile"; then
        log_error "Êó†Ê≥ïÂ∞ÜModelfileÂ§çÂà∂Âà∞ÂÆπÂô®"
        docker rm -f "$import_name" > /dev/null 2>&1
        rm -f "$temp_modelfile"
        return 1
    fi
    
    # Âú®ÂÆπÂô®‰∏≠ÊâßË°åollama createÂëΩ‰ª§
    log_verbose "ÊâßË°åÂëΩ‰ª§: docker exec $import_name ollama create $ollama_model_name -f $container_modelfile"
    local result=1
    if docker exec "$import_name" ollama create "$ollama_model_name" -f "$container_modelfile"; then
        log_success "GGUFÊ®°ÂûãÂ∑≤ÂØºÂÖ•Ollama: $ollama_model_name"
        result=0
    else
        log_error "GGUFÊ®°ÂûãÂØºÂÖ•Â§±Ë¥•: $ollama_model_name"
    fi
    
    # Ê∏ÖÁêÜÂÆπÂô®Âíå‰∏¥Êó∂Êñá‰ª∂
    cleanup_import_container
    remove_cleanup_function "cleanup_import_container"
    
    return $result
}

# ‰∏ãËΩΩÂπ∂ËΩ¨Êç¢HuggingFaceÊ®°Âûã
download_huggingface_model() {
    local model_name="$1"
    local quantize_type="$2"
    
    log_info "ÂºÄÂßã‰∏ãËΩΩÂπ∂ËΩ¨Êç¢HuggingFaceÊ®°Âûã: $model_name (ÈáèÂåñ: $quantize_type)"
    
    # Ê£ÄÊµãÊúÄ‰ºòHuggingFaceÁ´ØÁÇπ
    detect_optimal_hf_endpoint
    
    # Â¶ÇÊûúÂéüÂßãÂ§á‰ªΩÊÅ¢Â§çÂ§±Ë¥•ÔºåËøõË°åÊ≠£Â∏∏ÁöÑ‰∏ãËΩΩÊµÅÁ®ã
    
    # ÂàõÂª∫‰∏¥Êó∂ÁõÆÂΩïÁî®‰∫éÂ≠òÂÇ®GGUFÊñá‰ª∂
    local temp_dir
    temp_dir=$(mktemp -d) || {
        log_error "Êó†Ê≥ïÂàõÂª∫‰∏¥Êó∂ÁõÆÂΩï"
        return 1
    }
    
    # ÂÆö‰πâÊ∏ÖÁêÜÂáΩÊï∞
    cleanup_temp_dir() {
        if [[ -d "${temp_dir:-}" ]]; then
            log_verbose "Ê∏ÖÁêÜ‰∏¥Êó∂ÁõÆÂΩï: $temp_dir"
            docker_rm_rf "$temp_dir"
        fi
    }
    
    # ÂÆö‰πâÂÆπÂô®Ê∏ÖÁêÜÂáΩÊï∞
    cleanup_converter_container() {
        local container_name="llm-converter-$$"
        if docker ps -a --format "{{.Names}}" | grep -q "^${container_name}$"; then
            log_warning "Ê£ÄÊµãÂà∞‰∏≠Êñ≠ÔºåÊ≠£Âú®ÂÅúÊ≠¢Âπ∂Ê∏ÖÁêÜËΩ¨Êç¢ÂÆπÂô®: $container_name"
            docker stop "$container_name" > /dev/null 2>&1
            docker rm -f "$container_name" > /dev/null 2>&1
        fi
        cleanup_temp_dir
    }
    
    # ËÆæÁΩÆ‰ø°Âè∑Â§ÑÁêÜÔºåÁ°Æ‰øùÂÆπÂô®Ë¢´Ê≠£Á°ÆÊ∏ÖÁêÜ
    add_cleanup_function "cleanup_converter_container"
    
    # ÊûÑÂª∫docker runÂëΩ‰ª§Ôºå‰ΩøÁî®ÊåáÂÆöÁöÑÂÆπÂô®Âêç
    local container_name="llm-converter-$$"
    mapfile -t docker_cmd < <(build_full_docker_cmd "$container_name" "true" "true" \
        --volume "$temp_dir:/app/models" \
        --volume "${ABS_HF_DOWNLOAD_CACHE_DIR}:/app/download_cache")
    
    
    
    # ÈïúÂÉèÂíåÂèÇÊï∞
    docker_cmd+=("${FULL_IMAGE_NAME}")
    docker_cmd+=("${model_name}")
    docker_cmd+=("--quantize" "${quantize_type}")
    docker_cmd+=("--gguf-dir" "/app/models")
    
    # Ê∑ªÂä†verboseÂèÇÊï∞ÊîØÊåÅ
    if [[ "${VERBOSE}" == "true" ]]; then
        docker_cmd+=("--verbose")
    fi
    
    # ÊâßË°åËΩ¨Êç¢ÂëΩ‰ª§Ôºå‰ΩøÁî®ÂÆûÊó∂ËæìÂá∫
    local conversion_result=0
    log_info "Ê≠£Âú®‰∏ãËΩΩÂíåËΩ¨Êç¢Ê®°Âûã..."
    echo "----------------------------------------"
    
    # ‰ΩøÁî® unbuffer ÊàñËÄÖÁõ¥Êé•ÁÆ°ÈÅìËæìÂá∫Êù•Á°Æ‰øùÂÆûÊó∂ÊòæÁ§∫
    if "${docker_cmd[@]}" 2>&1 | while IFS= read -r line; do
        echo "[HF-DOCKER] $line"
    done; then
        echo "----------------------------------------"
        log_success "HuggingFaceÊ®°Âûã‰∏ãËΩΩÂπ∂ËΩ¨Êç¢ÂÆåÊàê: $model_name"
        
        # Ëá™Âä®ÂØºÂÖ•Âà∞Ollama
        log_info "ÂºÄÂßãÂØºÂÖ•GGUFÊ®°ÂûãÂà∞Ollama..."
        if import_gguf_to_ollama_from_temp "$model_name" "$quantize_type" "$temp_dir"; then
            log_success "Ê®°ÂûãÂ∑≤ÊàêÂäüÂØºÂÖ•Âà∞Ollama: $ollama_model_name"
            
            # È™åËØÅÂØºÂÖ•ÂêéÁöÑÊ®°ÂûãÂÆåÊï¥ÊÄß
            local final_model_name="${ollama_model_name%:*}"
            local final_model_tag="${ollama_model_name#*:}"
            if verify_model_after_installation "$final_model_name" "$final_model_tag"; then
                log_verbose_success "Ê®°ÂûãÂÆåÊï¥ÊÄßÈ™åËØÅÈÄöËøá: $ollama_model_name"
            else
                log_error "Ê®°ÂûãÂÆåÊï¥ÊÄßÈ™åËØÅÂ§±Ë¥•ÔºåÊ®°ÂûãÂ∑≤Ë¢´Ê∏ÖÁêÜ: $ollama_model_name"
            fi
            
            # Êñ∞ÊµÅÁ®ãÔºöÂú®ÂØºÂÖ•ÊàêÂäüÂêéËøõË°åÂ§á‰ªΩÂíåÊ∏ÖÁêÜ
            # Ê≠•È™§1: ÂàõÂª∫ÂéüÂßãÊ®°ÂûãÂ§á‰ªΩ
            log_info "ÂàõÂª∫ÂéüÂßãÊ®°ÂûãÂ§á‰ªΩ..."
            local model_safe_name=$(get_safe_model_name "$model_name" "filesystem")
            local cache_dir="${ABS_HF_DOWNLOAD_CACHE_DIR}/${model_safe_name}"
            
            # Ê£ÄÊü•ÊòØÂê¶Â≠òÂú®ÁºìÂ≠òÁõÆÂΩï
            if [[ -d "$cache_dir" ]]; then
                if backup_hf_original_model "$model_name" "$cache_dir"; then
                    log_verbose_success "ÂéüÂßãÊ®°ÂûãÂ§á‰ªΩÂàõÂª∫ÊàêÂäü"
                    
                    # Ê≠•È™§2: Âà†Èô§Â∑≤Â§á‰ªΩÁöÑÂéüÂßãÊ®°ÂûãÁºìÂ≠ò
                    log_info "Âà†Èô§Â∑≤Â§á‰ªΩÁöÑÂéüÂßãÊ®°ÂûãÁºìÂ≠ò..."
                    if docker_rm_rf "$cache_dir"; then
                        log_verbose_success "ÂéüÂßãÊ®°ÂûãÁºìÂ≠òÂ∑≤Ê∏ÖÁêÜ: $cache_dir"
                    else
                        log_warning "Ê∏ÖÁêÜÂéüÂßãÊ®°ÂûãÁºìÂ≠òÂ§±Ë¥•Ôºå‰ΩÜ‰∏çÂΩ±Âìç‰∏ªË¶ÅÂäüËÉΩ"
                    fi
                else
                    log_warning "ÂéüÂßãÊ®°ÂûãÂ§á‰ªΩÂàõÂª∫Â§±Ë¥•Ôºå‰øùÁïôÁºìÂ≠òÁõÆÂΩï"
                fi
            else
                log_info "Êú™ÊâæÂà∞ÁºìÂ≠òÁõÆÂΩïÔºåË∑≥ËøáÂ§á‰ªΩÂíåÊ∏ÖÁêÜ"
            fi
        else
            log_warning "GGUF‰∏ãËΩΩËΩ¨Êç¢ÊàêÂäüÔºå‰ΩÜÂØºÂÖ•OllamaÂ§±Ë¥•"
            conversion_result=1
        fi
    else
        echo "----------------------------------------"
        log_error "HuggingFaceÊ®°Âûã‰∏ãËΩΩËΩ¨Êç¢Â§±Ë¥•: $model_name"
        conversion_result=1
    fi
    
    # ÊâãÂä®Ê∏ÖÁêÜÂπ∂ÁßªÈô§Ê∏ÖÁêÜÂáΩÊï∞
    cleanup_converter_container
    remove_cleanup_function "cleanup_converter_container"
    
    return $conversion_result
}

# Ê£ÄÊü•OllamaÊ®°ÂûãÂú®backupsÁõÆÂΩï‰∏≠ÊòØÂê¶ÊúâÂ§á‰ªΩ
check_ollama_backup_exists() {
    local model_name="$1"
    local model_tag="$2"
    
    # ‰ΩøÁî®‰∏éget_safe_model_nameÁõ∏ÂêåÁöÑÈÄªËæëÁîüÊàêÂÆâÂÖ®ÂêçÁß∞
    local model_spec="${model_name}:${model_tag}"
    local model_safe_name=$(get_safe_model_name "$model_spec")
    local backup_parent_dir="$BACKUP_OUTPUT_DIR/${model_safe_name}"
    local backup_model_dir="$backup_parent_dir/${model_safe_name}"
    
    # Ê£ÄÊü•Â§á‰ªΩÁõÆÂΩïÊòØÂê¶Â≠òÂú®
    if [[ -d "$backup_model_dir" ]]; then
        # Ê£ÄÊü•ÊòØÂê¶ÊúâÊúâÊïàÁöÑÁõÆÂΩïÂ§á‰ªΩÁªìÊûÑ
        if [[ -d "$backup_model_dir/manifests" ]] && [[ -d "$backup_model_dir/blobs" ]]; then
            echo "$backup_parent_dir"
            return 0
        fi
    fi
    
    return 1
}

# Ê£ÄÊü•HuggingFaceÊ®°ÂûãÂú®hf_originalsÁõÆÂΩï‰∏≠ÊòØÂê¶ÊúâÂ§á‰ªΩ
check_hf_original_backup_exists() {
    local model_name="$1"
    
    # ‰ΩøÁî®Áªü‰∏ÄÁöÑÊñá‰ª∂Á≥ªÁªüÂÆâÂÖ®ÂëΩÂêç
    local model_safe_name=$(get_safe_model_name "$model_name" "filesystem")
    local backup_dir="$ABS_HF_ORIGINAL_BACKUP_DIR/${model_safe_name}"
    local backup_source_dir="$backup_dir/${model_safe_name}_original"
    
    # Ê£ÄÊü•Â§á‰ªΩÁõÆÂΩïÊòØÂê¶Â≠òÂú®
    if [[ -d "$backup_dir" ]]; then
        # Ê£ÄÊü•ÊòØÂê¶ÊúâÂéüÂßãÂ§á‰ªΩÁõÆÂΩï
        if [[ -d "$backup_source_dir" ]]; then
            echo "$backup_dir"
            return 0
        fi
    fi
    
    return 1
}

# Â∞ùËØï‰ªéÂ§á‰ªΩÊÅ¢Â§çOllamaÊ®°Âûã
try_restore_ollama_from_backup() {
    local model_name="$1"
    local model_tag="$2"
    
    log_verbose "Ê£ÄÊü•OllamaÊ®°ÂûãÂ§á‰ªΩ: ${model_name}:${model_tag}"
    
    local backup_dir
    if backup_dir=$(check_ollama_backup_exists "$model_name" "$model_tag"); then
        log_verbose_success "ÊâæÂà∞OllamaÊ®°ÂûãÂ§á‰ªΩ: $backup_dir"
        
        # ‰ΩøÁî®‰∏éget_safe_model_nameÁõ∏ÂêåÁöÑÈÄªËæëÁîüÊàêÂÆâÂÖ®ÂêçÁß∞
        local model_spec="${model_name}:${model_tag}"
        local model_safe_name=$(get_safe_model_name "$model_spec")
        
        # Êü•ÊâæÂ§á‰ªΩÁõÆÂΩïÔºàÊñ∞ÁöÑÁõ¥Êé•Â§çÂà∂Ê†ºÂºèÔºâ
        local backup_model_dir="$backup_dir/$model_safe_name"
        if [[ -d "$backup_model_dir" ]]; then
            # ÊÅ¢Â§çÊ®°Âûã
            log_info "Ê≠£Âú®‰ªéÂ§á‰ªΩÊÅ¢Â§çÊ®°Âûã..."
            if restore_ollama_model "$backup_model_dir" "true"; then
                log_success "‰ªéÂ§á‰ªΩÊàêÂäüÊÅ¢Â§çÊ®°Âûã: ${model_name}:${model_tag}"
                return 0
            else
                log_warning "‰ªéÂ§á‰ªΩÊÅ¢Â§çÊ®°ÂûãÂ§±Ë¥•ÔºåÂ∞ÜÂ∞ùËØïÈáçÊñ∞‰∏ãËΩΩ"
                return 1
            fi
        else
            log_error "Êú™ÊâæÂà∞ÊúâÊïàÁöÑÂ§á‰ªΩÁõÆÂΩï: $backup_model_dir"
            return 1
        fi
    else
        log_verbose "Êú™ÊâæÂà∞OllamaÊ®°ÂûãÂ§á‰ªΩ"
        return 1
    fi
}

# Â∞ùËØï‰ªéHuggingFaceÂéüÂßãÂ§á‰ªΩÊÅ¢Â§çÊ®°Âûã
try_restore_hf_from_original() {
    local model_name="$1"
    
    log_verbose "Ê£ÄÊü•HuggingFaceÂéüÂßãÊ®°ÂûãÂ§á‰ªΩ: $model_name"
    
    local backup_dir
    if backup_dir=$(check_hf_original_backup_exists "$model_name"); then
        log_verbose_success "ÊâæÂà∞HuggingFaceÂéüÂßãÊ®°ÂûãÂ§á‰ªΩ: $backup_dir"
        
        # ‰ΩøÁî®Áªü‰∏ÄÁöÑÊñá‰ª∂Á≥ªÁªüÂÆâÂÖ®ÂëΩÂêç
        local model_safe_name=$(get_safe_model_name "$model_name" "filesystem")
        
        # Êü•ÊâæÂéüÂßãÂ§á‰ªΩÁõÆÂΩïÔºàÊñ∞ÁöÑÁõ¥Êé•Â§çÂà∂Ê†ºÂºèÔºâ
        local backup_source_dir="$backup_dir/${model_safe_name}_original"
        if [[ -d "$backup_source_dir" ]]; then
            # ÊÅ¢Â§çÂà∞ÁºìÂ≠òÁõÆÂΩï
            local cache_dir="$ABS_HF_DOWNLOAD_CACHE_DIR/$model_safe_name"
            log_info "Ê≠£Âú®ÊÅ¢Â§çHuggingFaceÂéüÂßãÊ®°ÂûãÂà∞ÁºìÂ≠òÁõÆÂΩï..."
            
            # MD5Ê†°È™å
            local md5_file="${backup_source_dir}.md5"
            if [[ -f "$md5_file" ]]; then
                log_info "Ê≠£Âú®È™åËØÅMD5Ê†°È™åÂÄº..."
                if verify_directory_md5 "$backup_source_dir" "$md5_file"; then
                    log_verbose_success "MD5Ê†°È™åÈÄöËøá"
                else
                    log_error "MD5Ê†°È™åÂ§±Ë¥•ÔºåÂ§á‰ªΩÂèØËÉΩÂ∑≤ÊçüÂùè"
                    return 1
                fi
            else
                log_warning "Êú™ÊâæÂà∞MD5Ê†°È™åÊñá‰ª∂ÔºåË∑≥ËøáÊ†°È™å"
            fi
            
            # ÂàõÂª∫ÁºìÂ≠òÁõÆÂΩï
            mkdir -p "$(dirname "$cache_dir")"
            
            # Áõ¥Êé•Â§çÂà∂Â§á‰ªΩÁõÆÂΩïÂà∞ÁºìÂ≠òÁõÆÂΩï
            if cp -r "$backup_source_dir" "$cache_dir"; then
                log_success "‰ªéÂéüÂßãÂ§á‰ªΩÊàêÂäüÊÅ¢Â§çÊ®°ÂûãÂà∞ÁºìÂ≠ò: $model_name"
                return 0
            else
                log_warning "‰ªéÂéüÂßãÂ§á‰ªΩÊÅ¢Â§çÂ§±Ë¥•"
                return 1
            fi
        else
            log_error "Êú™ÊâæÂà∞ÊúâÊïàÁöÑÂéüÂßãÂ§á‰ªΩÁõÆÂΩï: $backup_source_dir"
            return 1
        fi
    else
        log_verbose "Êú™ÊâæÂà∞HuggingFaceÂéüÂßãÊ®°ÂûãÂ§á‰ªΩ"
        return 1
    fi
}

# Â§ÑÁêÜÂçï‰∏™Ê®°Âûã
process_model() {
    local model_entry="$1"
    local force_download="$2"
    local check_only="$3"
    
    # Ëß£ÊûêÊ®°ÂûãÊù°ÁõÆ
    local -A model_info
    if ! parse_model_entry "$model_entry" model_info; then
        log_error "Êó†ÊïàÁöÑÊ®°ÂûãÊù°ÁõÆÊ†ºÂºè: $model_entry"
        return 1
    fi
    
    log_verbose "Â§ÑÁêÜÊ®°Âûã: ${model_info[display]}"
    
    # Ê£ÄÊü•Ê®°ÂûãÊòØÂê¶Â≠òÂú®
    if [[ "$force_download" != "true" ]] && check_model_exists model_info; then
        log_success "Ê®°ÂûãÂ∑≤Â≠òÂú®"
        return 0
    fi
    
    # Ê®°Âûã‰∏çÂ≠òÂú®ÊàñÂº∫Âà∂‰∏ãËΩΩ
    if [[ "$check_only" == "true" ]]; then
        log_warning "ÈúÄË¶Å‰∏ãËΩΩ: ${model_info[display]}"
        return 0
    fi
    
    # Â∞ùËØï‰ªéÂ§á‰ªΩÊÅ¢Â§ç
    if try_restore_model model_info; then
        log_success "‰ªéÂ§á‰ªΩÊÅ¢Â§çÊàêÂäü"
        # Ê∏ÖÈô§ÁºìÂ≠òÔºåÂº∫Âà∂ÈáçÊñ∞Ê£ÄÊü•
        OLLAMA_CACHE_INITIALIZED="false"
        OLLAMA_MODELS_CACHE=""
        return 0
    fi
    
    # ÊâßË°å‰∏ãËΩΩ
    if download_model model_info; then
        log_success "Ê®°Âûã‰∏ãËΩΩÂÆåÊàê"
        # Ê∏ÖÈô§ÁºìÂ≠òÔºåÂº∫Âà∂ÈáçÊñ∞Ê£ÄÊü•
        OLLAMA_CACHE_INITIALIZED="false"
        OLLAMA_MODELS_CACHE=""
        return 0
    else
        log_error "Ê®°ÂûãÂ§ÑÁêÜÂ§±Ë¥•: ${model_info[display]}"
        return 1
    fi
}

# ‰∏ªÂáΩÊï∞
main() {
    # Ëé∑Âèñ‰∏ªÊú∫Êó∂Âå∫
    HOST_TIMEZONE=$(get_host_timezone)
    
    # Ê£ÄÊü•ÂèÇÊï∞ - ÊîØÊåÅhelpÂú®‰ªª‰Ωï‰ΩçÁΩÆ
    for arg in "$@"; do
        if [[ "$arg" = "--help" || "$arg" = "-h" ]]; then
            show_help
            exit 0
        fi
    done
    
    # ÈªòËÆ§ÂÄº
    MODELS_FILE="$MODELS_LIST_FILE"
    CHECK_ONLY="true"
    FORCE_DOWNLOAD="false"
    REBUILD="false"
    HF_TOKEN=""
    BACKUP_MODEL=""
    BACKUP_ALL="false"
    LIST_MODELS="false"
    RESTORE_FILE=""
    GENERATE_COMPOSE="false"
    FORCE_RESTORE="false"
    REMOVE_MODEL=""
    REMOVE_ALL="false"
    
    # Ëß£ÊûêÂëΩ‰ª§Ë°åÂèÇÊï∞
    while [[ $# -gt 0 ]]; do
        case $1 in
            --models-file)
                MODELS_FILE="$2"
                shift 2
                ;;
            --ollama-dir)
                # Â§ÑÁêÜÁî®Êà∑ÊåáÂÆöÁöÑOllamaÁõÆÂΩï
                local user_ollama_dir="$2"
                user_ollama_dir="${user_ollama_dir%/}"  # ÁßªÈô§Êú´Â∞æÊñúÊù†
                
                # ËÆæÁΩÆÊï∞ÊçÆÁõÆÂΩïÂíåÊ®°ÂûãÁõÆÂΩï
                if [[ "$user_ollama_dir" == */models ]]; then
                    OLLAMA_MODELS_DIR="$user_ollama_dir"
                    OLLAMA_DATA_DIR="${user_ollama_dir%/models}"
                else
                    OLLAMA_DATA_DIR="$user_ollama_dir"
                    OLLAMA_MODELS_DIR="$user_ollama_dir/models"
                fi
                shift 2
                ;;
            --hf-backup-dir)
                HF_ORIGINAL_BACKUP_DIR="$2"
                shift 2
                ;;
            --backup)
                BACKUP_MODEL="$2"
                shift 2
                ;;
            --backup-all)
                BACKUP_ALL="true"
                shift
                ;;
            --list)
                LIST_MODELS="true"
                shift
                ;;
            --restore)
                RESTORE_FILE="$2"
                shift 2
                ;;
            --remove)
                REMOVE_MODEL="$2"
                shift 2
                ;;
            --remove-all)
                REMOVE_ALL="true"
                shift
                ;;
            --backup-dir)
                BACKUP_OUTPUT_DIR="$2"
                shift 2
                ;;
            --check-only)
                CHECK_ONLY="true"
                shift
                ;;
            --install)
                CHECK_ONLY="false"
                shift
                ;;
            --force-download)
                FORCE_DOWNLOAD="true"
                CHECK_ONLY="false"  # Âº∫Âà∂‰∏ãËΩΩÊó∂Â∫îËØ•ÂÆûÈôÖÊâßË°å‰∏ãËΩΩ
                shift
                ;;
            --force)
                FORCE_RESTORE="true"
                shift
                ;;
            --hf-token)
                HF_TOKEN="$2"
                shift 2
                ;;
            --verbose)
                VERBOSE="true"
                shift
                ;;
            --rebuild)
                REBUILD="true"
                shift
                ;;
            --generate-compose)
                GENERATE_COMPOSE="true"
                shift
                ;;
            *)
                log_error "Êú™Áü•ÂèÇÊï∞: $1"
                show_help
                exit 1
                ;;
        esac
    done
    
    # ÊòæÁ§∫ÂΩìÂâç‰ªªÂä°ÔºàÁÆÄÂåñÔºâ
    local current_task=""
    if [[ -n "$BACKUP_MODEL" ]]; then
        current_task="Â§á‰ªΩÊ®°Âûã: $BACKUP_MODEL"
    elif [[ "$BACKUP_ALL" == "true" ]]; then
        current_task="ÊâπÈáèÂ§á‰ªΩÊâÄÊúâÊ®°Âûã"
    elif [[ -n "$RESTORE_FILE" ]]; then
        current_task="ÊÅ¢Â§çÊ®°Âûã: $RESTORE_FILE"
    elif [[ -n "$REMOVE_MODEL" ]]; then
        current_task="Âà†Èô§Ê®°Âûã: $REMOVE_MODEL"
    elif [[ "$REMOVE_ALL" == "true" ]]; then
        current_task="ÊâπÈáèÂà†Èô§ÊâÄÊúâÊ®°Âûã"
    elif [[ "$LIST_MODELS" == "true" ]]; then
        current_task="ÂàóÂá∫Â∑≤ÂÆâË£ÖÁöÑÊ®°Âûã"
    elif [[ "$GENERATE_COMPOSE" == "true" ]]; then
        current_task="ÁîüÊàêDocker ComposeÈÖçÁΩÆ"
    elif [[ "$CHECK_ONLY" == "true" ]]; then
        current_task="Ê£ÄÊü•Ê®°ÂûãÁä∂ÊÄÅ"
    else
        current_task="ÂÆâË£Ö/‰∏ãËΩΩÊ®°Âûã"
    fi
    
    log_info "üöÄ ‰ªªÂä°: $current_task"
    log_verbose "Ê®°ÂûãÂàóË°®Êñá‰ª∂: $MODELS_FILE"
    log_verbose "OllamaÁõÆÂΩï: $OLLAMA_MODELS_DIR"
    [[ -n "$BACKUP_OUTPUT_DIR" ]] && log_verbose "Â§á‰ªΩÁõÆÂΩï: $BACKUP_OUTPUT_DIR"
    
    # ÂàùÂßãÂåñË∑ØÂæÑ
    init_paths
    
    # Á°Æ‰øùOllamaÁõÆÂΩïÂ≠òÂú®
    if [[ ! -d "$OLLAMA_MODELS_DIR" ]]; then
        log_verbose "ÂàõÂª∫OllamaÊ®°ÂûãÁõÆÂΩï..."
        if ! mkdir -p "$OLLAMA_MODELS_DIR" 2>/dev/null; then
            log_warning "Êó†Ê≥ïÂàõÂª∫OllamaÊ®°ÂûãÁõÆÂΩïÔºåÊüê‰∫õÂäüËÉΩÂèØËÉΩ‰∏çÂèØÁî®"
        fi
    fi
    
    # ÊâßË°åÁâπÂÆö‰ªªÂä°Âπ∂ÈÄÄÂá∫
    if [[ -n "$BACKUP_MODEL" ]]; then
        execute_task "Ê®°ÂûãÂ§á‰ªΩ" backup_single_model "$BACKUP_MODEL" "$BACKUP_OUTPUT_DIR"
    elif [[ "$BACKUP_ALL" == "true" ]]; then
        execute_task "ÊâπÈáèÂ§á‰ªΩ" backup_models_from_list "$MODELS_FILE" "$BACKUP_OUTPUT_DIR"
    elif [[ "$LIST_MODELS" == "true" ]]; then
        execute_task "Ê®°ÂûãÂàóË°®" list_installed_models
    elif [[ "$GENERATE_COMPOSE" == "true" ]]; then
        execute_task "DockerÈÖçÁΩÆÁîüÊàê" generate_docker_compose
    elif [[ -n "$RESTORE_FILE" ]]; then
        execute_task "Ê®°ÂûãÊÅ¢Â§ç" restore_model "$RESTORE_FILE" "$FORCE_RESTORE"
    elif [[ -n "$REMOVE_MODEL" ]]; then
        execute_task "Ê®°ÂûãÂà†Èô§" remove_model_smart "$REMOVE_MODEL" "$FORCE_RESTORE"
    elif [[ "$REMOVE_ALL" == "true" ]]; then
        execute_task "ÊâπÈáèÂà†Èô§" remove_models_from_list "$MODELS_FILE" "$FORCE_RESTORE"
    fi
    
    # Ê£ÄÊü•‰æùËµñ
    check_dependencies
    
    # Ëß£ÊûêÊ®°ÂûãÂàóË°®
    local models=()
    parse_models_list "$MODELS_FILE" models
    
    if [[ ${#models[@]} -eq 0 ]]; then
        log_warning "Ê≤°ÊúâÊâæÂà∞‰ªª‰ΩïÊ®°ÂûãÔºåÈÄÄÂá∫"
        exit 0
    fi
    
    # Ê£ÄÊü•ÊòØÂê¶ÈúÄË¶ÅDockerÈïúÂÉèÔºà‰ªÖÂú®ÊúâHuggingFaceÊ®°ÂûãÊó∂Ôºâ
    local has_hf_models=false
    for model in "${models[@]}"; do
        if [[ "$model" =~ ^huggingface:|^hf-gguf: ]]; then
            has_hf_models=true
            break
        fi
    done
    
    if [[ "$has_hf_models" == "true" ]]; then
        if [[ "$REBUILD" == "true" ]]; then
            build_docker_image
        else
            # Á°Æ‰øùDockerÈïúÂÉèÂ≠òÂú®
            ensure_hf_downloader_image
        fi
    fi
    
    # Â§ÑÁêÜÊØè‰∏™Ê®°Âûã
    local total_models=${#models[@]}
    local processed=0
    local failed=0
    
    for model in "${models[@]}"; do
        processed=$((processed + 1))
        log_verbose "Â§ÑÁêÜÊ®°Âûã [$processed/$total_models]: $model"
        
        # Â§ÑÁêÜÂçï‰∏™Ê®°ÂûãÈîôËØØÔºå‰∏ç‰∏≠Êñ≠Êï¥‰∏™ÊµÅÁ®ã
        if ! process_model "$model" "$FORCE_DOWNLOAD" "$CHECK_ONLY"; then
            failed=$((failed + 1))
        fi
    done
    
    # ÊòæÁ§∫ÊÄªÁªì
    log_info "=== Â§ÑÁêÜÂÆåÊàê ==="
    log_info "ÊÄªÊ®°ÂûãÊï∞: $total_models"
    log_info "Â∑≤Â§ÑÁêÜ: $processed"
    if [[ $failed -gt 0 ]]; then
        log_warning "Â§±Ë¥•: $failed"
    else
        log_success "ÂÖ®ÈÉ®ÊàêÂäüÂÆåÊàê"
    fi
    
    if [[ "$CHECK_ONLY" == "true" ]]; then
        log_info "Ê£ÄÊü•Ê®°ÂºèÂÆåÊàêÔºåÊú™ÊâßË°åÂÆûÈôÖ‰∏ãËΩΩ"
    fi
}

# ==================================================================================
#                           Docker ComposeÁîüÊàêÂäüËÉΩ
# ==================================================================================

# ÁîüÊàêdocker-compose.yamlÊñá‰ª∂
update_existing_compose() {
    local output_file="$1"
    local custom_models="$2"
    local default_model="$3"
    
    log_info "Êõ¥Êñ∞Áé∞Êúâdocker-compose.yamlÊñá‰ª∂‰∏≠ÁöÑCUSTOM_MODELSÈÖçÁΩÆ"
    
    # ÂàõÂª∫Â§á‰ªΩ
    local backup_file="${output_file}.backup.$(date +%Y%m%d_%H%M%S)"
    cp "$output_file" "$backup_file"
    log_info "Â∑≤Â§á‰ªΩÁé∞ÊúâÊñá‰ª∂: $backup_file"
    
    # ‰ΩøÁî®PythonËÑöÊú¨Êõ¥Êñ∞CUSTOM_MODELSÁéØÂ¢ÉÂèòÈáè
    if grep -q "CUSTOM_MODELS=" "$output_file"; then
        # ‰ΩøÁî®PythonÊù•Á≤æÁ°ÆÂ§ÑÁêÜYAMLÊñá‰ª∂‰∏≠ÁöÑÂ§öË°åCUSTOM_MODELS
        # ‰ΩøÁî®‰∏¥Êó∂Êñá‰ª∂Â≠òÂÇ®Â§öË°åÂÜÖÂÆπ
        local temp_models_file=$(mktemp)
        echo "$custom_models" > "$temp_models_file"
        
        # ‰ΩøÁî®Á∫ØshellÂÆûÁé∞ÊõøÊç¢ÂäüËÉΩ
        update_docker_compose_models() {
            local file_path="$1"
            local models_file="$2"
            local default_model="$3"
            
            # ËØªÂèñÊñ∞ÁöÑÊ®°ÂûãÈÖçÁΩÆ
            local new_models
            new_models=$(cat "$models_file")
            
            # ÂàõÂª∫‰∏¥Êó∂Êñá‰ª∂
            local temp_file=$(mktemp)
            
            # ‰ΩøÁî®sedÂíåÁÆÄÂçïÁöÑÁä∂ÊÄÅÊú∫Â§ÑÁêÜÂ§öË°åCUSTOM_MODELSÊõøÊç¢
            # È¶ñÂÖàÊ†áËÆ∞ÂºÄÂßãÂíåÁªìÊùü‰ΩçÁΩÆ
            start_line=$(grep -n '^[[:space:]]*-[[:space:]]*"CUSTOM_MODELS=' "$file_path" | cut -d: -f1)
            end_line=$(tail -n +$((start_line + 1)) "$file_path" | grep -n '"$' | head -1 | cut -d: -f1)
            end_line=$((start_line + end_line))
            
            if [[ -n "$start_line" && -n "$end_line" ]]; then
                # ÊèêÂèñÂâçÁºÄÔºàÁº©ËøõÂíå"CUSTOM_MODELS="Ôºâ
                prefix=$(sed -n "${start_line}p" "$file_path" | sed 's/\(^[[:space:]]*-[[:space:]]*"CUSTOM_MODELS=\).*/\1/')
                
                # ÊûÑÂª∫Êñ∞Êñá‰ª∂ÔºöÂ§¥ÈÉ® + Êñ∞Ë°å + Â∞æÈÉ®
                head -n $((start_line - 1)) "$file_path" > "$temp_file"
                echo "${prefix}${new_models}\"" >> "$temp_file"
                tail -n +$((end_line + 1)) "$file_path" >> "$temp_file"
            else
                # Â¶ÇÊûúÊâæ‰∏çÂà∞Â§öË°åÊ†ºÂºèÔºåÂõûÈÄÄÂà∞ÁÆÄÂçïÊõøÊç¢
                cp "$file_path" "$temp_file"
            fi
            
            # Â§ÑÁêÜDEFAULT_MODELÊõøÊç¢  
            sed -E "s|(^[[:space:]]*-[[:space:]]*DEFAULT_MODEL=)[^[:space:]#]*(.*)|\\1${default_model}  # Ëá™Âä®ËÆæÁΩÆ‰∏∫models.listÁ¨¨‰∏Ä‰∏™Ê®°Âûã|" "$temp_file" > "$file_path"
            
            # Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂
            rm -f "$temp_file"
            return 0
        }
        
        if update_docker_compose_models "$output_file" "$temp_models_file" "$default_model"; then
            echo "SUCCESS"
        else
            echo "ERROR: Failed to update docker-compose.yaml"
            exit 1
        fi
        
        # Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂
        rm -f "$temp_models_file"
        
        if [[ $? -eq 0 ]]; then
            log_success "ÊàêÂäüÊõ¥Êñ∞docker-compose.yaml‰∏≠ÁöÑCUSTOM_MODELSÈÖçÁΩÆ"
            log_info "Êõ¥Êñ∞ÂÜÖÂÆπ: $custom_models"
        else
            log_error "‰ΩøÁî®PythonÊõ¥Êñ∞Â§±Ë¥•ÔºåÂ∞ùËØï‰ΩøÁî®sedÊñπÊ≥ï"
            
            # Â§áÁî®ÊñπÊ≥ïÔºö‰ΩøÁî®sedËøõË°åÁÆÄÂçïÊõøÊç¢
            sed -i.tmp "s|CUSTOM_MODELS=[^\"]*|CUSTOM_MODELS=$custom_models|g" "$output_file"
            rm -f "${output_file}.tmp"
            
            log_success "‰ΩøÁî®sedÊàêÂäüÊõ¥Êñ∞CUSTOM_MODELSÈÖçÁΩÆ"
        fi
    else
        log_error "Êú™Âú®docker-compose.yaml‰∏≠ÊâæÂà∞CUSTOM_MODELSÈÖçÁΩÆ"
        return 1
    fi
    
    return 0
}

generate_docker_compose() {
    local output_file="${1:-./docker-compose.yaml}"
    local models_file="${MODELS_FILE:-./models.list}"
    
    # Ê£ÄÊü•Ê®°ÂûãÂàóË°®Êñá‰ª∂ÊòØÂê¶Â≠òÂú®
    if [[ ! -f "$models_file" ]]; then
        log_error "Ê®°ÂûãÂàóË°®Êñá‰ª∂‰∏çÂ≠òÂú®: $models_file"
        return 1
    fi
    
    # Ê£ÄÊü•ÊòØÂê¶Â∑≤Â≠òÂú®docker-compose.yamlÊñá‰ª∂
    if [[ -f "$output_file" ]]; then
        log_info "Ê£ÄÊµãÂà∞Áé∞Êúâdocker-compose.yamlÊñá‰ª∂ÔºåÂ∞ÜÊõ¥Êñ∞CUSTOM_MODELSÈÖçÁΩÆ"
        
        # ÁîüÊàêCUSTOM_MODELSÂÜÖÂÆπ
        local custom_models_content
        custom_models_content=$(generate_custom_models_list "$models_file")
        
        if [[ -z "$custom_models_content" ]]; then
            log_warning "Êú™ÊâæÂà∞ÊøÄÊ¥ªÁöÑÊ®°ÂûãÔºåÂ∞ÜÁîüÊàêÈªòËÆ§ÈÖçÁΩÆ"
            custom_models_content="-all"
        fi
        
        # Ê£ÄÊü•ÊòØÂê¶ÊúâÂèØÁî®ÁöÑÊ®°Âûã
        if [[ "$custom_models_content" == "-all" ]]; then
            log_error "ÈîôËØØ: models.list ‰∏≠Ê≤°ÊúâÊâæÂà∞ÂèØÁî®ÁöÑÊ®°ÂûãÈÖçÁΩÆ"
            log_error "ËØ∑Á°Æ‰øù models.list ‰∏≠Ëá≥Â∞ëÊúâ‰∏Ä‰∏™Êú™Ë¢´Ê≥®ÈáäÁöÑÊ®°ÂûãÈÖçÁΩÆ"
            return 1
        fi
        
        # Ëá™Âä®Ê£ÄÊµãÈªòËÆ§Ê®°Âûã
        local default_model
        default_model=$(detect_default_model "$models_file")
        
        [[ -n "${VERBOSE}" ]] && log_info "ÁîüÊàêÁöÑCUSTOM_MODELS: $custom_models_content"
        [[ -n "${VERBOSE}" ]] && log_info "Ê£ÄÊµãÂà∞ÁöÑÈªòËÆ§Ê®°Âûã: $default_model"
        
        # Êõ¥Êñ∞Áé∞ÊúâÊñá‰ª∂
        update_existing_compose "$output_file" "$custom_models_content" "$default_model"
    else
        log_info "Âü∫‰∫éÊ®°ÂûãÂàóË°®ÁîüÊàêdocker-compose.yaml: $models_file"
        
        # ÁîüÊàêCUSTOM_MODELSÂÜÖÂÆπ
        local custom_models_content
        custom_models_content=$(generate_custom_models_list "$models_file")
        
        if [[ -z "$custom_models_content" ]]; then
            log_warning "Êú™ÊâæÂà∞ÊøÄÊ¥ªÁöÑÊ®°ÂûãÔºåÂ∞ÜÁîüÊàêÈªòËÆ§ÈÖçÁΩÆ"
            custom_models_content="-all"
        fi
        
        # Ëá™Âä®Ê£ÄÊµãÈªòËÆ§Ê®°Âûã
        local default_model
        default_model=$(detect_default_model "$models_file")
        
        # Ê£ÄÊü•ÊòØÂê¶ÊúâÂèØÁî®ÁöÑÊ®°Âûã (CUSTOM_MODELSÂè™Êúâ-allËØ¥ÊòéÊ≤°ÊúâÊøÄÊ¥ªÁöÑÊ®°Âûã)
        if [[ "$custom_models_content" == "-all" ]]; then
            log_error "ÈîôËØØ: models.list ‰∏≠Ê≤°ÊúâÊâæÂà∞ÂèØÁî®ÁöÑÊ®°ÂûãÈÖçÁΩÆ"
            log_error "ËØ∑Á°Æ‰øù models.list ‰∏≠Ëá≥Â∞ëÊúâ‰∏Ä‰∏™Êú™Ë¢´Ê≥®ÈáäÁöÑÊ®°ÂûãÈÖçÁΩÆ"
            return 1
        fi
        
        [[ -n "${VERBOSE}" ]] && log_info "ÁîüÊàêÁöÑCUSTOM_MODELS: $custom_models_content"
        [[ -n "${VERBOSE}" ]] && log_info "Ê£ÄÊµãÂà∞ÁöÑÈªòËÆ§Ê®°Âûã: $default_model"
        
        # ÁîüÊàêdocker-compose.yamlÂÜÖÂÆπ
        generate_compose_content "$output_file" "$custom_models_content" "$default_model"
    fi
}

# ÁîüÊàêCUSTOM_MODELSÂàóË°®
generate_custom_models_list() {
    local models_file="$1"
    local custom_models_entries=()
    
    # Ê∑ªÂä† -all ‰Ωú‰∏∫Á¨¨‰∏Ä‰∏™Êù°ÁõÆÔºàÈöêËóèÊâÄÊúâÈªòËÆ§Ê®°ÂûãÔºâ
    custom_models_entries+=("-all")
    
    while IFS= read -r line || [[ -n "$line" ]]; do
        # Ë∑≥ËøáÊ≥®ÈáäË°åÂíåÁ©∫Ë°å
        [[ "$line" =~ ^[[:space:]]*# ]] && continue
        [[ -z "${line// }" ]] && continue
        
        # Ëß£ÊûêË°åÂÜÖÂÆπ
        read -r model_type model_spec quantize_type <<< "$line"
        
        case "$model_type" in
            "ollama")
                if [[ -n "$model_spec" ]]; then
                    local alias=$(generate_model_alias "$model_spec" "ollama")
                    local entry="+${model_spec}@OpenAI=${alias}"
                    custom_models_entries+=("$entry")
                fi
                ;;
            "hf-gguf")
                if [[ -n "$model_spec" ]]; then
                    local alias=$(generate_model_alias "$model_spec" "hf-gguf")
                    local entry="+${model_spec}@OpenAI=${alias}"
                    custom_models_entries+=("$entry")
                fi
                ;;
            "huggingface")
                if [[ -n "$model_spec" && -n "$quantize_type" ]]; then
                    local ollama_name=$(echo "$model_spec" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9._-]/_/g')
                    local alias=$(generate_model_alias "${ollama_name}:latest" "huggingface")
                    local entry="+${ollama_name}:latest@OpenAI=${alias}"
                    custom_models_entries+=("$entry")
                fi
                ;;
        esac
    done < "$models_file"
    
    # ËæìÂá∫CUSTOM_MODELSÊ†ºÂºè
    if [[ ${#custom_models_entries[@]} -gt 1 ]]; then
        printf '%s' "${custom_models_entries[0]}"
        for ((i=1; i<${#custom_models_entries[@]}; i++)); do
            printf ',\\\n        %s' "${custom_models_entries[i]}"
        done
    else
        echo "-all"
    fi
}

# ÁîüÊàêÁÆÄÂçïÁöÑÊ®°ÂûãÂà´Âêç
generate_model_alias() {
    local model_spec="$1"
    local model_type="$2"
    
    # Ê†πÊçÆÊ®°ÂûãÁ±ªÂûãÊèêÂèñÂÆûÈôÖÁöÑÊ®°ÂûãÂêçÁß∞
    local model_name=""
    local model_version=""
    
    case "$model_type" in
        "hf-gguf")
            # ÂØπ‰∫é hf-gguf Ê®°ÂûãÔºå‰ªéË∑ØÂæÑ‰∏≠ÊèêÂèñÊ®°ÂûãÂêçÁß∞
            # Ê†ºÂºèÂ¶Ç: hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF:latest
            if [[ "$model_spec" =~ hf\.co/[^/]+/([^/:]+) ]]; then
                model_name="${BASH_REMATCH[1]}"
                # ÁßªÈô§Â∏∏ËßÅÁöÑ GGUF ÂêéÁºÄ
                model_name=$(echo "$model_name" | sed 's/-GGUF$//' | sed 's/_GGUF$//')
            fi
            ;;
        "huggingface")
            # ÂØπ‰∫é huggingface Ê®°ÂûãÔºå‰ΩøÁî®‰º†ÈÄíÁöÑÂ∑≤Â§ÑÁêÜÂêçÁß∞
            model_name="$model_spec"
            model_name="${model_name%:*}"
            ;;
        *)
            # ÂØπ‰∫é ollama ÂíåÂÖ∂‰ªñÁ±ªÂûãÔºå‰ΩøÁî®Âü∫Á°ÄÂêçÁß∞
            model_name="${model_spec%:*}"
            ;;
    esac
    
    # ‰ªéÊ®°ÂûãËßÑÊ†º‰∏≠ÊèêÂèñÁâàÊú¨‰ø°ÊÅØ
    if [[ "$model_spec" =~ :(.+)$ ]]; then
        model_version="${BASH_REMATCH[1]}"
    fi
    
    # Â¶ÇÊûúÊ≤°ÊúâÊèêÂèñÂà∞Ê®°ÂûãÂêçÁß∞Ôºå‰ΩøÁî®Á±ªÂûã‰Ωú‰∏∫ÂêéÂ§á
    if [[ -z "$model_name" ]]; then
        model_name="$model_type"
    fi
    
    # Ê∏ÖÁêÜÊ®°ÂûãÂêçÁß∞ÂíåÁâàÊú¨‰∏≠ÁöÑÁâπÊÆäÂ≠óÁ¨¶
    local clean_name=$(echo "$model_name" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9]/-/g' | sed 's/--*/-/g' | sed 's/^-\|-$//g')
    
    if [[ -n "$model_version" && "$model_version" != "latest" ]]; then
        local clean_version=$(echo "$model_version" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9.]/-/g' | sed 's/--*/-/g' | sed 's/^-\|-$//g')
        echo "${clean_name}-${clean_version}"
    else
        echo "$clean_name"
    fi
}

# Ê£ÄÊµãÈªòËÆ§Ê®°Âûã
detect_default_model() {
    local models_file="$1"
    local first_active_model=""
    
    while IFS= read -r line || [[ -n "$line" ]]; do
        # Ë∑≥ËøáÊ≥®ÈáäË°åÂíåÁ©∫Ë°å
        [[ "$line" =~ ^[[:space:]]*# ]] && continue
        [[ -z "${line// }" ]] && continue
        
        # Ëß£ÊûêË°åÂÜÖÂÆπ
        read -r model_type model_spec quantize_type <<< "$line"
        
        # ÊâæÂà∞Á¨¨‰∏Ä‰∏™ÊøÄÊ¥ªÁöÑÊ®°ÂûãÂπ∂ÁîüÊàêÂÖ∂Âà´Âêç
        if [[ -n "$model_spec" && -z "$first_active_model" ]]; then
            case "$model_type" in
                "ollama")
                    first_active_model=$(generate_model_alias "$model_spec" "ollama")
                    break
                    ;;
                "hf-gguf")
                    first_active_model=$(generate_model_alias "$model_spec" "hf-gguf")
                    break
                    ;;
                "huggingface")
                    if [[ -n "$quantize_type" ]]; then
                        local ollama_name=$(echo "$model_spec" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9._-]/_/g')
                        first_active_model=$(generate_model_alias "${ollama_name}:latest" "huggingface")
                        break
                    fi
                    ;;
            esac
        fi
    done < "$models_file"
    
    # Â¶ÇÊûúÊ≤°ÊúâÊâæÂà∞ÊøÄÊ¥ªÁöÑÊ®°ÂûãÔºå‰ΩøÁî®ÈªòËÆ§ÂÄº
    echo "${first_active_model:-qwen3-14b}"
}

# ÁîüÊàêdocker-compose.yamlÊñá‰ª∂ÂÜÖÂÆπ
detect_gpus() {
    local gpu_indices=""
    
    if command -v nvidia-smi &>/dev/null; then
        gpu_indices=$(nvidia-smi --query-gpu=index --format=csv,noheader,nounits 2>/dev/null | tr '\n' ',' | sed 's/,$//')
        if [[ -n "$gpu_indices" ]]; then
            echo "$gpu_indices"
        else
            echo "0,1,2,3"
        fi
    else
        echo "0,1,2,3"
    fi
}

generate_compose_content() {
    local output_file="$1"
    local custom_models="$2"
    local default_model="$3"
    
    local cuda_devices
    cuda_devices=$(detect_gpus)
    
    # Ëé∑Âèñ‰∏ªÊú∫Êó∂Âå∫
    local host_timezone=$(get_host_timezone)
    [[ -z "$host_timezone" ]] && host_timezone="UTC"
    
    # Â¶ÇÊûúÊñá‰ª∂Â∑≤Â≠òÂú®ÔºåÂàõÂª∫Â§á‰ªΩ
    if [[ -f "$output_file" ]]; then
        local backup_file="${output_file}.backup.$(date +%Y%m%d_%H%M%S)"
        cp "$output_file" "$backup_file"
        log_info "Â∑≤Â§á‰ªΩÁé∞ÊúâÊñá‰ª∂: $backup_file"
    fi
    
    # ÁîüÊàêdocker-compose.yamlÂÜÖÂÆπ
    cat > "$output_file" << EOF
services:
  ollama:
    image: $DOCKER_IMAGE_OLLAMA
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ./ollama:/root/.ollama
    networks:
      - llms-tools-network
    environment:
      # Ollama‰ºòÂåñÈÖçÁΩÆ
      - CUDA_VISIBLE_DEVICES=$cuda_devices # Ëá™Âä®Ê£ÄÊµãÂπ∂‰ΩøÁî®ÊâÄÊúâÂèØÁî®GPU
      - OLLAMA_NEW_ENGINE=1 # Êñ∞ÁöÑÂºïÊìé, ollamarunner
      - OLLAMA_SCHED_SPREAD=1 # ÂêØÁî®Â§öGPUË¥üËΩΩÂùáË°°
      - OLLAMA_KEEP_ALIVE=5m # Ê®°ÂûãÂú®ÂÜÖÂ≠ò‰∏≠‰øùÊåÅÂä†ËΩΩÁöÑÊó∂Èïø, ÂàÜÈíü
      - OLLAMA_NUM_PARALLEL=3 # Âπ∂ÂèëËØ∑Ê±ÇÊï∞
      - OLLAMA_FLASH_ATTENTION=1 # flash attention, Áî®‰∫é‰ºòÂåñÊ≥®ÊÑèÂäõËÆ°ÁÆó, Èôç‰ΩéÊòæÂ≠ò‰ΩøÁî®
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    command: [ "serve"]
    restart: unless-stopped

  one-api:
    image: $DOCKER_IMAGE_ONE_API
    container_name: one-api
    volumes:
      - ./one-api:/data
    networks:
      - llms-tools-network
    ports:
      - "3001:3001"
    depends_on:
      - ollama
    environment:
      - TZ=${host_timezone}
      - SESSION_SECRET=xxxxxxxxxxxxxxxxxxxxxx  # ‰øÆÊîπ‰∏∫ÈöèÊú∫ÁîüÊàêÁöÑ‰ºöËØùÂØÜÈí•
    command: [ "--port", "3001" ]
    restart: unless-stopped

  prompt-optimizer:
    image: $DOCKER_IMAGE_PROMPT_OPTIMIZER
    container_name: prompt-optimizer
    ports:
      - "8501:80"
    environment:
      - VITE_CUSTOM_API_BASE_URL=http://YOUR_SERVER_IP:3001/v1  # ‰øÆÊîπ‰∏∫‰Ω†ÁöÑÊúçÂä°Âô®IPÂú∞ÂùÄ
      - VITE_CUSTOM_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx  # ‰øÆÊîπ‰∏∫‰Ω†ÁöÑAPIÂØÜÈí•
      - VITE_CUSTOM_API_MODEL=$default_model  # Ëá™Âä®ËÆæÁΩÆ‰∏∫models.listÁ¨¨‰∏Ä‰∏™Ê®°Âûã
      - ACCESS_USERNAME=admin  # ‰øÆÊîπ‰∏∫‰Ω†ÁöÑÁî®Êà∑Âêç
      - ACCESS_PASSWORD=xxxxxxxxxxxxxxxxxxxxxx  # ‰øÆÊîπ‰∏∫‰Ω†ÁöÑÂØÜÁ†Å
    networks:
      - llms-tools-network
    depends_on:
      - one-api
    restart: unless-stopped

  chatgpt-next-web:
    image: $DOCKER_IMAGE_CHATGPT_NEXT_WEB
    container_name: chatgpt-next-web
    ports:
      - "3000:3000"
    environment:
      - OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx  # ‰øÆÊîπ‰∏∫‰Ω†ÁöÑOpenAI APIÂØÜÈí•
      - BASE_URL=http://one-api:3001
      - PROXY_URL=
      - "CUSTOM_MODELS=$custom_models"
      - DEFAULT_MODEL=$default_model  # Ëá™Âä®ËÆæÁΩÆ‰∏∫models.listÁ¨¨‰∏Ä‰∏™Ê®°Âûã
      - CODE=xxxxxxxxxxxxxxxxxxxxxx  # ‰øÆÊîπ‰∏∫‰Ω†ÁöÑËÆøÈóÆÂØÜÁ†Å
    networks:
      - llms-tools-network
    depends_on:
      - one-api
    restart: unless-stopped

networks:
  llms-tools-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.28.0/24
EOF

    log_success "ÊàêÂäüÁîüÊàêdocker-compose.yamlÊñá‰ª∂: $output_file"
    log_info "ÂåÖÂê´Ê®°ÂûãÈÖçÁΩÆ: $custom_models"
    log_info "ÈªòËÆ§Ê®°Âûã: $default_model"
    log_info "Ê£ÄÊµãÂà∞GPUËÆæÂ§á: $cuda_devices"
    echo ""
    log_info "‚ö†Ô∏è  ÈáçË¶ÅÊèêÁ§∫: ÁîüÊàêÁöÑÈÖçÁΩÆÊñá‰ª∂‰∏≠ÂåÖÂê´Âç†‰ΩçÁ¨¶ÔºåËØ∑Ê†πÊçÆ‰ª•‰∏ãËØ¥Êòé‰øÆÊîπÔºö"
    log_info "== ÂøÖÈ°ª‰øÆÊîπÁöÑÈÖçÁΩÆ =="
    log_info "1. VITE_CUSTOM_API_BASE_URL: Â∞Ü YOUR_SERVER_IP ÊõøÊç¢‰∏∫ÂÆûÈôÖÊúçÂä°Âô®IPÂú∞ÂùÄ"
    log_info "2. VITE_CUSTOM_API_KEY: ÊõøÊç¢‰∏∫ one-api ‰∏≠ÁöÑÊúâÊïàAPIÂØÜÈí•"
    log_info "3. ACCESS_USERNAME/ACCESS_PASSWORD: ËÆæÁΩÆ prompt-optimizer ÁöÑÁôªÂΩïÂá≠ÊçÆ"
    log_info "4. OPENAI_API_KEY: ÊõøÊç¢‰∏∫ one-api ‰∏≠ÁöÑÊúâÊïàAPIÂØÜÈí•"
    log_info "5. SESSION_SECRET: ÊõøÊç¢‰∏∫ÈöèÊú∫ÁîüÊàêÁöÑ‰ºöËØùÂØÜÈí•ÔºàÂª∫ËÆÆ32‰ΩçÈöèÊú∫Â≠óÁ¨¶‰∏≤Ôºâ"
    log_info "6. CODE: ËÆæÁΩÆ ChatGPT-Next-Web ÁöÑËÆøÈóÆÂØÜÁ†Å"
    log_info "7. VITE_CUSTOM_API_MODEL/DEFAULT_MODEL: Â∑≤Ëá™Âä®ËÆæÁΩÆ‰∏∫ $default_modelÔºåÂèØÊ†πÊçÆÈúÄË¶Å‰øÆÊîπ"
    echo ""
    log_info "== ÂèØÈÄâ‰øÆÊîπÁöÑÈÖçÁΩÆ =="
    log_info "‚Ä¢ Á´ØÂè£Êò†Â∞Ñ: Â¶ÇÈúÄÈÅøÂÖçÁ´ØÂè£ÂÜ≤Á™ÅÔºåÂèØ‰øÆÊîπ ports ÈÉ®ÂàÜÁöÑ‰∏ªÊú∫Á´ØÂè£"
    log_info "  - Ollama: 11434 -> Ëá™ÂÆö‰πâÁ´ØÂè£"
    log_info "  - One-API: 3001 -> Ëá™ÂÆö‰πâÁ´ØÂè£" 
    log_info "  - Prompt-Optimizer: 8501 -> Ëá™ÂÆö‰πâÁ´ØÂè£"
    log_info "  - ChatGPT-Next-Web: 3000 -> Ëá™ÂÆö‰πâÁ´ØÂè£"
    log_info "‚Ä¢ DockerÈïúÂÉè: Â¶ÇÈúÄ‰ΩøÁî®ÁâπÂÆöÁâàÊú¨ÔºåÂèØ‰øÆÊîπ image ÈÉ®ÂàÜÁöÑÊ†áÁ≠æ"
    log_info "‚Ä¢ ÁΩëÁªúÈÖçÁΩÆ: ÂèØ‰øÆÊîπ subnet ‰ª•ÈÅøÂÖçIPÂú∞ÂùÄÂÜ≤Á™Å"
    echo ""
    log_info "ÈÖçÁΩÆÂÆåÊàêÂêéËøêË°å: docker-compose up -d Êù•ÂêØÂä®ÊúçÂä°"
    
    return 0
}

# Âè™ÊúâÂú®Áõ¥Êé•ËøêË°åËÑöÊú¨Êó∂ÊâçÊâßË°åmainÂáΩÊï∞
if [[ "${BASH_SOURCE[0]:-$0}" == "${0}" ]]; then
    main "$@"
fi