#!/bin/bash
# =============================================================================
# OMO (oh-my-ollama or Ollama Models Organizer)
# =============================================================================
#
# ğŸ¤– åŠŸèƒ½æ¦‚è§ˆï¼š
#   ğŸ“¥ æ¨¡å‹ä¸‹è½½ï¼š
#       â€¢ ä»Ollamaå®˜æ–¹ä»“åº“ä¸‹è½½æ¨¡å‹
#       â€¢ ä»HuggingFaceä»“åº“ä¸‹è½½æ¨¡å‹å¹¶è‡ªåŠ¨è½¬æ¢é‡åŒ–
#       â€¢ ç›´æ¥ä¸‹è½½HuggingFaceçš„GGUFæ ¼å¼æ¨¡å‹
#       â€¢ æ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œç¼“å­˜å¤ç”¨
#       â€¢ æ™ºèƒ½HuggingFaceé•œåƒç«¯ç‚¹æ£€æµ‹
#
#   ğŸ’¾ æ¨¡å‹å¤‡ä»½ï¼š
#       â€¢ å®Œæ•´å¤‡ä»½Ollamaæ¨¡å‹ï¼ˆmanifest + blobsï¼‰
#       â€¢ å¤‡ä»½HuggingFaceåŸå§‹æ¨¡å‹æ–‡ä»¶
#       â€¢ MD5æ ¡éªŒç¡®ä¿æ•°æ®å®Œæ•´æ€§
#       â€¢ ç”Ÿæˆè¯¦ç»†å¤‡ä»½ä¿¡æ¯æ–‡ä»¶
#
#   ğŸ”„ æ¨¡å‹æ¢å¤ï¼š
#       â€¢ ä»å¤‡ä»½æ¢å¤Ollamaæ¨¡å‹
#       â€¢ æ¢å¤HuggingFaceåŸå§‹æ¨¡å‹åˆ°ç¼“å­˜
#       â€¢ æ”¯æŒå¼ºåˆ¶è¦†ç›–æ¨¡å¼
#       â€¢ è‡ªåŠ¨éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
#
#   ğŸ“‹ æ¨¡å‹ç®¡ç†ï¼š
#       â€¢ åˆ—å‡ºå·²å®‰è£…æ¨¡å‹åŠè¯¦ç»†ä¿¡æ¯
#       â€¢ æ™ºèƒ½åˆ é™¤æ¨¡å‹ï¼ˆå•ä¸ª/æ‰¹é‡ï¼‰
#       â€¢ æ¨¡å‹å®Œæ•´æ€§æ£€æŸ¥å’ŒéªŒè¯
#       â€¢ ç£ç›˜ä½¿ç”¨æƒ…å†µç»Ÿè®¡
#
#   ğŸ³ å®¹å™¨åŒ–éƒ¨ç½²ï¼š
#       â€¢ ç”ŸæˆDocker Composeé…ç½®
#       â€¢ é›†æˆOllamaã€One-APIã€Prompt-Optimizerç­‰æœåŠ¡
#       â€¢ è‡ªåŠ¨GPUæ”¯æŒå’Œæ—¶åŒºé…ç½®
#       â€¢ æ™ºèƒ½ç«¯å£å’Œç½‘ç»œé…ç½®
#
#   âš™ï¸  é«˜çº§ç‰¹æ€§ï¼š
#       â€¢ æ”¯æŒè‡ªå®šä¹‰é‡åŒ–ç±»å‹ï¼ˆq4_0, q5_0, q8_0ç­‰ï¼‰
#       â€¢ åŠ¨æ€Dockeré•œåƒæ„å»º
#       â€¢ å¹¶è¡Œå¤„ç†å’Œç¼“å­˜ä¼˜åŒ–
#       â€¢ è¯¦ç»†æ—¥å¿—å’Œé”™è¯¯å¤„ç†
#
# ğŸ“ æ”¯æŒçš„æ¨¡å‹æ ¼å¼ï¼š
#   â€¢ ollama [model]:[tag]     - Ollamaå®˜æ–¹æ¨¡å‹
#   â€¢ huggingface [model] [quant] - HuggingFaceæ¨¡å‹(éœ€è½¬æ¢)
#   â€¢ hf-gguf [model]:[tag]    - HuggingFace GGUFæ¨¡å‹(ç›´æ¥å¯¼å…¥)
#
# ğŸ”§ ç¯å¢ƒè¦æ±‚ï¼š
#   â€¢ Docker (æ”¯æŒGPUå¯é€‰)
#   â€¢ Bash 4.0+
#   â€¢ curl, jq (è‡ªåŠ¨å®‰è£…åˆ°å®¹å™¨)
#
# ğŸ‘¨â€ğŸ’» ä½œè€…ï¼šChain Lai
# ğŸ“– è¯¦ç»†ä½¿ç”¨è¯´æ˜è¯·è¿è¡Œï¼š./omo.sh --help
# =============================================================================

set -euo pipefail  # å¯ç”¨ä¸¥æ ¼çš„é”™è¯¯å¤„ç†

# å‡½æ•°ä¼˜åŒ–æç¤ºè¯
# ä¼˜åŒ–æ­¥éª¤
#   1. åˆ†æå‡½æ•°ï¼šæ‰¾å‡ºå¤æ‚åµŒå¥—ã€é‡å¤ä»£ç ã€å†—ä½™é€»è¾‘
#   2. æå–è¾…åŠ©å‡½æ•°ï¼šå°†å¤æ‚é€»è¾‘æ‹†åˆ†ä¸ºç‹¬ç«‹å‡½æ•°
#   3. åˆ›å»ºç»Ÿä¸€æ¡†æ¶ï¼šç”¨é€šç”¨æ¨¡å¼æ›¿ä»£é‡å¤çš„æ¡ä»¶åˆ†æ”¯
#   4. ä¼˜åŒ–è¾“å‡ºï¼šç»Ÿä¸€è¾“å‡ºæ ¼å¼ï¼Œæ¶ˆé™¤é‡å¤ä¿¡æ¯
#   5. éªŒè¯æ•ˆæœï¼šè¯­æ³•æ£€æŸ¥ + åŠŸèƒ½æµ‹è¯•

#==============================================================================
# å…¨å±€é…ç½®å’Œå˜é‡å®šä¹‰
#==============================================================================
SCRIPT_DIR=""
readonly SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]:-$0}")" && pwd)"
readonly MODELS_LIST_FILE="${SCRIPT_DIR}/models.list"
# åŸºç¡€è·¯å¾„é…ç½®ï¼ˆå¯åœ¨mainå‡½æ•°ä¸­è¢«è¦†ç›–ï¼‰
OLLAMA_DATA_DIR="${SCRIPT_DIR}/ollama"
OLLAMA_MODELS_DIR="${OLLAMA_DATA_DIR}/models"
BACKUP_OUTPUT_DIR="${SCRIPT_DIR}/backups"
HF_DOWNLOAD_CACHE_DIR="${SCRIPT_DIR}/hf_download_cache"
HF_ORIGINAL_BACKUP_DIR="${SCRIPT_DIR}/hf_originals"

# é¢„è®¡ç®—çš„ç»å¯¹è·¯å¾„ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
ABS_OLLAMA_DATA_DIR=""
ABS_HF_DOWNLOAD_CACHE_DIR=""
ABS_HF_ORIGINAL_BACKUP_DIR=""

# HuggingFaceé•œåƒé…ç½®
HF_ENDPOINT=""  # åˆå§‹ä¸ºç©ºï¼Œä¼šåœ¨éœ€è¦æ—¶åŠ¨æ€æ£€æµ‹æœ€ä¼˜ç«¯ç‚¹


# Dockeré•œåƒé…ç½®
readonly DOCKER_IMAGE_LLAMA_CPP="ghcr.io/ggml-org/llama.cpp:full-cuda"
readonly DOCKER_IMAGE_OLLAMA="ollama/ollama:latest"
readonly DOCKER_IMAGE_ONE_API="justsong/one-api:latest"
readonly DOCKER_IMAGE_PROMPT_OPTIMIZER="linshen/prompt-optimizer:latest"
readonly DOCKER_IMAGE_CHATGPT_NEXT_WEB="yidadaa/chatgpt-next-web:latest"

# å¤‡ä»½é…ç½®

# è¿è¡Œæ—¶é…ç½®
VERBOSE="false"  # è¯¦ç»†æ¨¡å¼å¼€å…³

#==============================================================================
# å·¥å…·å‡½æ•°
#==============================================================================

# æ˜¾ç¤ºå®¹å™¨æ—¥å¿—çš„å·¥å…·å‡½æ•°
show_container_logs() {
    local container_name="$1"
    log_error "å®¹å™¨æ—¥å¿—:"
    docker logs "$container_name" 2>&1 | tail -10
}

# è·å–ä¸»æœºæ—¶åŒº
get_host_timezone() {
    # å°è¯•å¤šç§æ–¹æ³•è·å–ä¸»æœºæ—¶åŒº
    if command_exists timedatectl; then
        # ä¼˜å…ˆä½¿ç”¨ timedatectlï¼ˆsystemd ç³»ç»Ÿï¼‰
        timedatectl show --property=Timezone --value 2>/dev/null
    elif [[ -L /etc/localtime ]]; then
        # é€šè¿‡ç¬¦å·é“¾æ¥è·å–æ—¶åŒº
        readlink /etc/localtime | sed 's|.*/zoneinfo/||'
    elif [[ -f /etc/timezone ]]; then
        # ä» /etc/timezone æ–‡ä»¶è¯»å–
        cat /etc/timezone
    else
        # é»˜è®¤å›é€€åˆ° UTC
        echo "UTC"
    fi
}

#==============================================================================
# Dockeré›†æˆæ¨¡å—ï¼ˆHuggingFaceæ¨¡å‹è½¬æ¢ï¼‰
#==============================================================================
readonly IMAGE_NAME="hf_downloader"
readonly IMAGE_TAG="latest"
readonly FULL_IMAGE_NAME="${IMAGE_NAME}:${IMAGE_TAG}"

# Dockeré›†æˆå†…å®¹ï¼ˆåµŒå…¥å¼æ–‡ä»¶ï¼‰
# åˆ›å»ºä¸´æ—¶æ„å»ºç›®å½•å¹¶å†™å…¥å¿…è¦æ–‡ä»¶
create_docker_build_context() {
    local build_dir="$1"
    
    mkdir -p "$build_dir"
    
    # è·å–ä¸»æœºæ—¶åŒº
    local host_timezone=$(get_host_timezone)
    [[ -z "$host_timezone" ]] && host_timezone="UTC"
    
    # å†™å…¥Dockerfile
    cat > "$build_dir/Dockerfile" << EOF
FROM $DOCKER_IMAGE_LLAMA_CPP
WORKDIR /app
ENV DEBIAN_FRONTEND=noninteractive TZ=${host_timezone}
RUN apt-get update && apt-get install -y --no-install-recommends curl aria2 jq tzdata && \
    apt-get clean && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* && \
    ln -snf /usr/share/zoneinfo/${host_timezone} /etc/localtime && echo ${host_timezone} > /etc/timezone && \
    curl -fsSL https://hf-mirror.com/hfd/hfd.sh -o /app/hfd.sh && chmod +x /app/hfd.sh
COPY convert_model.sh /app/
RUN chmod +x /app/convert_model.sh
ENTRYPOINT ["/app/convert_model.sh"]
EOF
    
    # å†™å…¥convert_model.sh
    cat > "$build_dir/convert_model.sh" << 'EOF'
#!/bin/bash
set -euo pipefail
# ç®€åŒ–æ—¥å¿—å‡½æ•°ï¼ˆDockerç¯å¢ƒä¸“ç”¨ï¼‰
log_info() { printf "[INFO] %s\n" "$1" >&2; }
log_success() { printf "[SUCCESS] %s\n" "$1" >&2; }
log_error() { printf "[ERROR] %s\n" "$1" >&2; }
run_command() {
    local description="$1" verbose="$2"
    shift 2
    log_info "$description"
    if [[ "$verbose" == "true" ]]; then
        "$@"
    else
        if "$@" >/dev/null 2>&1; then
            log_success "${description}å®Œæˆ"
        else
            log_error "${description}å¤±è´¥"
            "$@" 2>&1 | tail -5
            exit 1
        fi
    fi
}
download_model() {
    local model_name="$1" download_dir="$2"
    # log_info "ä¸‹è½½æ¨¡å‹: $model_name"
    if [[ -n "${HF_ENDPOINT:-}" ]]; then
        export HF_ENDPOINT="${HF_ENDPOINT}"
    fi
    local cache_dir="/app/download_cache"
    if [[ -d "$cache_dir" ]]; then
        local model_safe_name=$(echo "$model_name" | sed 's/[\/:]/_/g')
        local cached_model_dir="${cache_dir}/${model_safe_name}"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„ä¸‹è½½ï¼ˆå­˜åœ¨.aria2æ–‡ä»¶ï¼‰
        if [[ -d "$cached_model_dir" ]] && [[ -n "$(find "$cached_model_dir" -name "*.aria2" 2>/dev/null)" ]]; then
            log_info "æ£€æµ‹åˆ°æœªå®Œæˆçš„ä¸‹è½½ï¼Œç»§ç»­ä¸‹è½½..."
            export ARIA2C_OPTS="--continue=true --max-tries=10 --retry-wait=3 --split=8 --max-connection-per-server=8 --auto-file-renaming=false"
            if /app/hfd.sh "$model_name" --local-dir "$cached_model_dir" --tool aria2c; then
                rm -f "$cached_model_dir"/*.aria2 2>/dev/null || true
                if [[ -n "$(ls -A "$cached_model_dir" 2>/dev/null)" ]]; then
                    cp -r "$cached_model_dir"/* "$download_dir"/ 2>/dev/null || true
                fi
            else
                log_error "æ¨¡å‹ä¸‹è½½å¤±è´¥"
                exit 1
            fi
        elif [[ -d "$cached_model_dir" ]] && [[ -n "$(ls -A "$cached_model_dir" 2>/dev/null)" ]]; then
            log_info "ä½¿ç”¨å·²ç¼“å­˜çš„å®Œæ•´æ¨¡å‹"
            if [[ -n "$(ls -A "$cached_model_dir" 2>/dev/null)" ]]; then
                cp -r "$cached_model_dir"/* "$download_dir"/ 2>/dev/null || true
            fi
            return 0
        else
            # å…¨æ–°ä¸‹è½½
            mkdir -p "$cached_model_dir"
            export ARIA2C_OPTS="--continue=true --max-tries=10 --retry-wait=3 --split=8 --max-connection-per-server=8 --auto-file-renaming=false"
            if /app/hfd.sh "$model_name" --local-dir "$cached_model_dir" --tool aria2c; then
                rm -f "$cached_model_dir"/*.aria2 2>/dev/null || true
                if [[ -n "$(ls -A "$cached_model_dir" 2>/dev/null)" ]]; then
                    cp -r "$cached_model_dir"/* "$download_dir"/ 2>/dev/null || true
                fi
            else
                log_error "æ¨¡å‹ä¸‹è½½å¤±è´¥"
                exit 1
            fi
        fi
    else
        if ! /app/hfd.sh "$model_name" --local-dir "$download_dir" --tool aria2c; then
            log_error "æ¨¡å‹ä¸‹è½½å¤±è´¥"
            exit 1
        fi
    fi
}
convert_to_gguf() {
    local model_dir="$1" output_file="$2" verbose="$3"
    run_command "è½¬æ¢ä¸ºGGUFæ ¼å¼" "$verbose" \
        python3 /app/convert_hf_to_gguf.py "$model_dir" --outfile "$output_file" --outtype f16
}
quantize_model() {
    local input_file="$1" output_file="$2" quantize_type="$3" verbose="$4"
    run_command "é‡åŒ–æ¨¡å‹ (${quantize_type})" "$verbose" \
        /app/llama-quantize "$input_file" "$output_file" "$quantize_type"
    rm -f "$input_file"
}
convert_main() {
    local model_name="" quantize_type="q4_0" gguf_dir="/app/models" verbose=false
    while [[ $# -gt 0 ]]; do
        case $1 in
            --quantize)
                [[ -z "${2:-}" ]] && { log_error "ç¼ºå°‘ --quantize å‚æ•°å€¼"; exit 1; }
                quantize_type="$2"; shift 2 ;;
            --gguf-dir)
                [[ -z "${2:-}" ]] && { log_error "ç¼ºå°‘ --gguf-dir å‚æ•°å€¼"; exit 1; }
                gguf_dir="$2"; shift 2 ;;
            --verbose) verbose=true; shift ;;
            -*) log_error "æœªçŸ¥å‚æ•°: $1"; exit 1 ;;
            *)
                if [[ -z "$model_name" ]]; then
                    model_name="$1"
                else
                    log_error "å¤šä½™çš„å‚æ•°: $1"; exit 1
                fi
                shift ;;
        esac
    done
    if [[ -z "$model_name" ]]; then
        log_error "ç¼ºå°‘æ¨¡å‹åç§°å‚æ•°"; exit 1
    fi
    log_info "å¤„ç†æ¨¡å‹: $model_name (${quantize_type})"
    mkdir -p "$gguf_dir"
    local temp_dir="/tmp/model_download_$$"
    mkdir -p "$temp_dir"
    local model_basename=$(echo "$model_name" | sed 's/\//-/g')
    local final_gguf_file="${gguf_dir}/${model_basename}-${quantize_type}.gguf"
    if [[ -f "$final_gguf_file" ]]; then
        log_info "è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡è½¬æ¢"; exit 0
    fi
    local temp_gguf_file="${temp_dir}/${model_basename}.gguf"
    download_model "$model_name" "$temp_dir"
    convert_to_gguf "$temp_dir" "$temp_gguf_file" "$verbose"
    quantize_model "$temp_gguf_file" "$final_gguf_file" "$quantize_type" "$verbose"
    log_success "è½¬æ¢å®Œæˆ: $final_gguf_file"
    if [[ -f "$final_gguf_file" ]]; then
        local file_size=$(du -h "$final_gguf_file" | cut -f1)
        log_info "æ–‡ä»¶å¤§å°: $file_size"
    fi
}
convert_main "$@"
EOF
    
    chmod +x "$build_dir/convert_model.sh"
    log_verbose_success "Dockeræ„å»ºä¸Šä¸‹æ–‡åˆ›å»ºå®Œæˆ"
}

# æ¸…ç†ä¸´æ—¶æ„å»ºç›®å½•
cleanup_docker_build_context() {
    local build_dir="$1"
    if [[ -d "$build_dir" ]]; then
        rm -rf "$build_dir"
    fi
}

# é¢œè‰²å®šä¹‰
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly CYAN='\033[0;36m'
readonly MAGENTA='\033[0;95m'
readonly NC='\033[0m' # No Color

#==============================================================================
# ä»»åŠ¡æ‰§è¡Œæ¨¡å—  
#==============================================================================
# ç»Ÿä¸€çš„ä»»åŠ¡æ‰§è¡Œå‡½æ•°
execute_task() {
    local task_name="$1"
    local task_function="$2"
    shift 2
    local task_args=("$@")
    
    log_info "æ‰§è¡Œ${task_name}..."
    if "${task_function}" "${task_args[@]}"; then
        log_success "${task_name}å®Œæˆ"
        exit 0
    else
        local exit_code=$?
        if [[ $exit_code -eq 2 ]]; then
            # ç”¨æˆ·å–æ¶ˆæ“ä½œï¼Œä¸æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
            exit 0
        else
            log_error "${task_name}å¤±è´¥"
            exit 1
        fi
    fi
}

# å¤‡ä»½å•ä¸ªæ¨¡å‹çš„åŒ…è£…å‡½æ•°
backup_single_model() {
    local backup_model="$1"
    local backup_dir="$2"
    
    # å¤„ç†ä¸åŒç±»å‹çš„æ¨¡å‹å‰ç¼€
    local model_to_backup="$backup_model"
    if [[ "$backup_model" =~ ^hf-gguf:(.+)$ ]]; then
        model_to_backup="${BASH_REMATCH[1]}"
    elif [[ "$backup_model" =~ ^ollama:(.+)$ ]]; then
        model_to_backup="${BASH_REMATCH[1]}"
    elif [[ "$backup_model" =~ ^huggingface:([^:]+):(.+)$ ]]; then
        # HuggingFaceæ¨¡å‹éœ€è¦è½¬æ¢ä¸ºOllamaæ ¼å¼
        local model_name="${BASH_REMATCH[1]}"
        local quantize_type="${BASH_REMATCH[2]}"
        model_to_backup=$(generate_ollama_model_name "$model_name" "$quantize_type")
    fi
    
    backup_ollama_model "$model_to_backup" "$backup_dir"
}

# æ¢å¤æ¨¡å‹çš„åŒ…è£…å‡½æ•°
restore_model() {
    local restore_file="$1"
    local force_restore="$2"
    
    # å¦‚æœæ¢å¤æ–‡ä»¶ä¸æ˜¯ç»å¯¹è·¯å¾„ï¼Œåˆ™åœ¨BACKUP_OUTPUT_DIRä¸­æŸ¥æ‰¾
    local restore_path="$restore_file"
    if [[ "$restore_file" != /* ]]; then
        restore_path="$BACKUP_OUTPUT_DIR/$restore_file"
    fi
    
    restore_ollama_model "$restore_path" "$force_restore"
}

# æ¨¡å‹å¤„ç†å™¨ - è§£ææ¨¡å‹æ¡ç›®å¹¶è¿”å›å¤„ç†å‡½æ•°
parse_model_entry() {
    local model_entry="$1"
    local -n result_ref="$2"
    
    # æ¸…ç©ºç»“æœæ•°ç»„
    result_ref=()
    
    if [[ "$model_entry" =~ ^ollama:([^:]+):(.+)$ ]]; then
        result_ref[type]="ollama"
        result_ref[name]="${BASH_REMATCH[1]}"
        result_ref[tag]="${BASH_REMATCH[2]}"
        result_ref[display]="${result_ref[name]}:${result_ref[tag]} (Ollama)"
        
    elif [[ "$model_entry" =~ ^huggingface:([^:]+):(.+)$ ]]; then
        result_ref[type]="huggingface"
        result_ref[name]="${BASH_REMATCH[1]}"
        result_ref[quantize]="${BASH_REMATCH[2]}"
        result_ref[display]="${result_ref[name]} (é‡åŒ–: ${result_ref[quantize]})"
        
    elif [[ "$model_entry" =~ ^hf-gguf:(.+)$ ]]; then
        result_ref[type]="hf-gguf"
        local model_full_name="${BASH_REMATCH[1]}"
        if [[ "$model_full_name" =~ ^(.+):(.+)$ ]]; then
            result_ref[name]="${BASH_REMATCH[1]}"
            result_ref[tag]="${BASH_REMATCH[2]}"
        else
            result_ref[name]="$model_full_name"
            result_ref[tag]="latest"
        fi
        result_ref[display]="${result_ref[name]}:${result_ref[tag]} (HF-GGUF)"
    else
        return 1
    fi
    
    return 0
}

# æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
check_model_exists() {
    local -n model_info_ref=$1
    
    case "${model_info_ref[type]}" in
        "ollama")
            check_ollama_model "${model_info_ref[name]}" "${model_info_ref[tag]}"
            ;;
        "huggingface")
            check_huggingface_model_in_ollama "${model_info_ref[name]}" "${model_info_ref[quantize]}"
            ;;
        "hf-gguf")
            check_hf_gguf_model "${model_info_ref[name]}" "${model_info_ref[tag]}"
            ;;
        *)
            return 1
            ;;
    esac
}

# ä¸‹è½½æ¨¡å‹
download_model() {
    local -n model_info_ref=$1
    
    case "${model_info_ref[type]}" in
        "ollama")
            download_ollama_model "${model_info_ref[name]}" "${model_info_ref[tag]}"
            ;;
        "huggingface")
            download_huggingface_model "${model_info_ref[name]}" "${model_info_ref[quantize]}"
            ;;
        "hf-gguf")
            download_hf_gguf_model "${model_info_ref[name]}" "${model_info_ref[tag]}"
            ;;
        *)
            return 1
            ;;
    esac
}

# å°è¯•ä»å¤‡ä»½æ¢å¤æ¨¡å‹
try_restore_model() {
    local -n model_info_ref=$1
    
    case "${model_info_ref[type]}" in
        "ollama")
            try_restore_ollama_from_backup "${model_info_ref[name]}" "${model_info_ref[tag]}"
            ;;
        "hf-gguf")
            try_restore_ollama_from_backup "${model_info_ref[name]}" "${model_info_ref[tag]}"
            ;;
        "huggingface")
            # HuggingFaceæ¨¡å‹çš„æ¢å¤é€»è¾‘è¾ƒå¤æ‚ï¼Œå…ˆå°è¯•Ollamaå¤‡ä»½ï¼Œå†å°è¯•åŸå§‹å¤‡ä»½
            local expected_ollama_name=$(generate_ollama_model_name "${model_info_ref[name]}" "${model_info_ref[quantize]}")
            local ollama_model_name="${expected_ollama_name%:*}"
            local ollama_model_tag="${expected_ollama_name#*:}"
            
            if try_restore_ollama_from_backup "$ollama_model_name" "$ollama_model_tag"; then
                return 0
            fi
            
            # å°è¯•ä»åŸå§‹å¤‡ä»½æ¢å¤
            if try_restore_hf_from_original "${model_info_ref[name]}"; then
                log_verbose "ä»åŸå§‹å¤‡ä»½æ¢å¤ï¼Œå¼€å§‹è½¬æ¢..."
                if restore_and_reconvert_hf_model "${model_info_ref[name]}" "${model_info_ref[quantize]}" "true"; then
                    return 0
                fi
            fi
            return 1
            ;;
        *)
            return 1
            ;;
    esac
}

#==============================================================================
# æ—¥å¿—ç³»ç»Ÿæ¨¡å—
#==============================================================================
# æ—¥å¿—è§„åˆ™:
# 1. ä¸»æµç¨‹å‡½æ•°: ä½¿ç”¨æ ‡å‡†æ—¥å¿—å‡½æ•°(log_info, log_success, log_warning, log_error)
#    - åœ¨æ™®é€šæ¨¡å¼å’Œverboseæ¨¡å¼ä¸‹éƒ½æ˜¾ç¤º, ç”¨äºç”¨æˆ·å…³å¿ƒçš„æ ¸å¿ƒæ“ä½œè¿›åº¦
# 2. å·¥å…·å‡½æ•°: æ­£å¸¸è¿½è¸ªæ—¥å¿—ä½¿ç”¨verboseç‰ˆæœ¬(log_verbose, log_verbose_success)
#    - ä»…åœ¨verboseæ¨¡å¼æ˜¾ç¤º, è­¦å‘Šå’Œé”™è¯¯(log_warning, log_error)åœ¨ä»»ä½•æ¨¡å¼éƒ½æ˜¾ç¤º
# 3. é¿å…æ—¥å¿—é‡å¤: å·¥å…·å‡½æ•°çš„è¿½è¸ªä¿¡æ¯åªåœ¨verboseæ¨¡å¼æ˜¾ç¤º, ä¸»æµç¨‹ä¿æŒç®€æ´
#==============================================================================

log_info() {
    printf "${BLUE}[INFO]${NC} %s\n" "$1"
}

log_success() {
    printf "${GREEN}[SUCCESS]${NC} %s\n" "$1"
}

log_warning() {
    printf "${YELLOW}[WARNING]${NC} %s\n" "$1"
}

log_error() {
    printf "${RED}[ERROR]${NC} %s\n" "$1"
}


# Verbose-only logging functions
log_verbose() {
    if [[ "${VERBOSE}" == "true" ]]; then
        printf "${BLUE}[INFO]${NC} %s\n" "$1"
    fi
    return 0
}

log_verbose_success() {
    [[ "${VERBOSE}" == "true" ]] && printf "${GREEN}[SUCCESS]${NC} %s\n" "$1"
    return 0
}

log_verbose_warning() {
    [[ "${VERBOSE}" == "true" ]] && printf "${YELLOW}[WARNING]${NC} %s\n" "$1"
    return 0
}


# HuggingFaceç«¯ç‚¹æ™ºèƒ½æ£€æµ‹å‡½æ•°
detect_optimal_hf_endpoint() {
    # å¦‚æœå·²ç»æ£€æµ‹è¿‡ï¼Œç›´æ¥è¿”å›
    if [[ "$HF_ENDPOINT_DETECTED" == "true" ]]; then
        return 0
    fi
    
    local cache_file="/tmp/.hf_endpoint_cache"
    local cache_timeout=3600  # ç¼“å­˜1å°æ—¶
    
    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
    if [[ -f "$cache_file" ]]; then
        local cache_time=$(stat -c %Y "$cache_file" 2>/dev/null || echo 0)
        local current_time=$(date +%s)
        if [[ $((current_time - cache_time)) -lt $cache_timeout ]]; then
            HF_ENDPOINT=$(cat "$cache_file")
            HF_ENDPOINT_DETECTED="true"
            log_verbose "ä½¿ç”¨ç¼“å­˜çš„HuggingFaceç«¯ç‚¹: $HF_ENDPOINT"
            return 0
        fi
    fi
    
    local hf_official="https://huggingface.co"
    local hf_mirror="https://hf-mirror.com"
    local timeout=3
    
    log_verbose "æ£€æµ‹æœ€ä¼˜HuggingFaceç«¯ç‚¹..."
    
    # æµ‹è¯•å•ä¸ªç«¯ç‚¹çš„å‡½æ•°
    test_endpoint() {
        local endpoint="$1"
        local host=$(echo "$endpoint" | sed 's|https\?://||' | cut -d'/' -f1)
        
        # ä½¿ç”¨pingæµ‹è¯•è¿é€šæ€§å’Œå»¶è¿Ÿ
        local ping_result=$(ping -c 1 -W $timeout "$host" 2>/dev/null | grep 'time=' | sed 's/.*time=\([0-9.]*\).*/\1/')
        
        if [[ -n "$ping_result" ]]; then
            # å°†å»¶è¿Ÿè½¬æ¢ä¸ºæ¯«ç§’æ•´æ•°
            local latency=$(echo "$ping_result" | cut -d'.' -f1)
            [[ -z "$latency" || ! "$latency" =~ ^[0-9]+$ ]] && latency=0
            echo "$endpoint|$latency"  # ä½¿ç”¨ | åˆ†éš”ç¬¦é¿å…ä¸URLä¸­çš„:å†²çª
        else
            echo "$endpoint|999999"  # è¡¨ç¤ºæ— æ³•è®¿é—®
        fi
    }
    
    # å¹¶è¡Œæµ‹è¯•
    local official_result="" mirror_result=""
    {
        official_result=$(test_endpoint "$hf_official")
    } &
    local pid1=$!
    
    {
        mirror_result=$(test_endpoint "$hf_mirror")
    } &
    local pid2=$!
    
    # ç­‰å¾…æµ‹è¯•å®Œæˆ
    wait $pid1 $pid2
    
    # è§£æç»“æœ
    local official_latency="${official_result#*|}"
    local mirror_latency="${mirror_result#*|}"
    
    # æ”¶é›†å¯ç”¨ç«¯ç‚¹
    local available_endpoints=()
    [[ "$official_latency" != "999999" ]] && available_endpoints+=("$hf_official|$official_latency")
    [[ "$mirror_latency" != "999999" ]] && available_endpoints+=("$hf_mirror|$mirror_latency")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨ç«¯ç‚¹
    if [[ ${#available_endpoints[@]} -eq 0 ]]; then
        log_error "æ— æ³•è®¿é—®ä»»ä½•HuggingFaceç«¯ç‚¹ï¼Œè„šæœ¬ä¸­æ­¢"
        exit 1
    fi
    
    # é€‰æ‹©å»¶è¿Ÿæœ€ä½çš„ç«¯ç‚¹
    local best_endpoint=""
    local best_latency=999999
    for endpoint_info in "${available_endpoints[@]}"; do
        local endpoint="${endpoint_info%|*}"
        local latency="${endpoint_info#*|}"
        if [[ -n "$latency" && "$latency" =~ ^[0-9]+$ && $latency -lt $best_latency ]]; then
            best_latency=$latency
            best_endpoint=$endpoint
        fi
    done
    
    local selected_endpoint="$best_endpoint"
    log_verbose "é€‰æ‹©æœ€ä¼˜ç«¯ç‚¹: $selected_endpoint (${best_latency}ms)"
    
    # æ›´æ–°å…¨å±€å˜é‡å¹¶ç¼“å­˜ç»“æœ
    HF_ENDPOINT="$selected_endpoint"
    HF_ENDPOINT_DETECTED="true"
    echo "$selected_endpoint" > "$cache_file"
    
    return 0
}

# æ ¼å¼åŒ–å­—èŠ‚å¤§å°ä¸ºäººç±»å¯è¯»æ ¼å¼
format_bytes() {
    local bytes="$1"
    
    # ä½¿ç”¨å•æ¬¡awkè°ƒç”¨å‡å°‘å¼€é”€ï¼Œé¢„å®šä¹‰å¸¸é‡æé«˜å¯è¯»æ€§
    awk -v b="$bytes" '
    BEGIN {
        if (b >= 1073741824) printf "%.1fGB", b / 1073741824
        else if (b >= 1048576) printf "%.1fMB", b / 1048576  
        else printf "%.1fKB", b / 1024
    }'
}

# éªŒè¯æ¨¡å‹æ ¼å¼æ˜¯å¦æ­£ç¡®
validate_model_format() {
    local model_spec="$1"
    if [[ "$model_spec" != *":"* ]]; then
        log_error "æ¨¡å‹æ ¼å¼é”™è¯¯ï¼Œåº”ä¸º 'æ¨¡å‹å:ç‰ˆæœ¬'ï¼Œä¾‹å¦‚ 'llama2:7b'"
        return 1
    fi
    return 0
}

# ç­‰å¾…Ollamaå®¹å™¨å°±ç»ª
wait_for_ollama_ready() {
    local container_name="$1"
    local max_attempts=120  # å¢åŠ åˆ°120ç§’
    local attempt=0
    
    log_verbose "ç­‰å¾…OllamaæœåŠ¡å¯åŠ¨..."
    
    while (( attempt < max_attempts )); do
        # é¦–å…ˆæ£€æŸ¥å®¹å™¨æ˜¯å¦è¿˜åœ¨è¿è¡Œ
        if ! docker ps -q --filter "name=^${container_name}$" | grep -q .; then
            log_error "å®¹å™¨ $container_name å·²åœæ­¢è¿è¡Œ"
            show_container_logs "$container_name"
            return 1
        fi
        
        # æ£€æŸ¥ollamaæœåŠ¡æ˜¯å¦å°±ç»ª
        if docker exec "$container_name" ollama list &>/dev/null; then
            log_verbose_success "OllamaæœåŠ¡å·²å°±ç»ª"
            return 0
        fi
        
        # æ¯10ç§’æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
        if (( attempt % 10 == 0 && attempt > 0 )); then
            log_verbose "ç­‰å¾…ä¸­... ($attempt/$max_attempts ç§’)"
        fi
        
        sleep 1
        ((attempt++))
    done
    
    log_error "ç­‰å¾…OllamaæœåŠ¡å°±ç»ªè¶…æ—¶ ($max_attempts ç§’)"
    show_container_logs "$container_name"
    return 1
}

# æ„å»ºå®Œæ•´çš„Dockerè¿è¡Œå‘½ä»¤
build_full_docker_cmd() {
    local container_name="$1"
    local use_gpu="${2:-true}"
    local include_hf_token="${3:-false}"
    local extra_env=()
    local extra_volumes=()
    
    # å¤„ç†é¢å¤–çš„ç¯å¢ƒå˜é‡å’ŒæŒ‚è½½å·å‚æ•°
    shift 3
    while [[ $# -gt 0 ]]; do
        case "$1" in
            --env)
                extra_env+=("$2")
                shift 2
                ;;
            --volume)
                extra_volumes+=("$2")
                shift 2
                ;;
            *)
                break
                ;;
        esac
    done
    
    local docker_cmd=("docker" "run" "--name" "$container_name" "--rm" "-t")
    
    # GPUæ”¯æŒ
    if [[ "$use_gpu" == "true" ]]; then
        docker_cmd+=("--gpus" "all")
    fi
    
    # HF Tokenæ”¯æŒ
    if [[ "$include_hf_token" == "true" && -n "${HF_TOKEN:-}" ]]; then
        docker_cmd+=("-e" "HF_TOKEN=${HF_TOKEN}")
    fi
    
    # åŸºç¡€ç¯å¢ƒå˜é‡
    docker_cmd+=("-e" "HF_ENDPOINT=${HF_ENDPOINT:-https://hf-mirror.com}")
    docker_cmd+=("-e" "PYTHONUNBUFFERED=1")
    docker_cmd+=("-e" "TERM=xterm-256color")
    docker_cmd+=("-v" "/etc/localtime:/etc/localtime:ro")
    docker_cmd+=("-e" "TZ=${HOST_TIMEZONE:-UTC}")
    
    # æ·»åŠ é¢å¤–çš„ç¯å¢ƒå˜é‡
    for env_var in "${extra_env[@]}"; do
        docker_cmd+=("-e" "$env_var")
    done
    
    # æ·»åŠ é¢å¤–çš„æŒ‚è½½å·
    for volume in "${extra_volumes[@]}"; do
        docker_cmd+=("-v" "$volume")
    done
    
    printf '%s\n' "${docker_cmd[@]}"
}

# é€šç”¨cleanupå‡½æ•°ç”Ÿæˆå™¨
create_cleanup_function() {
    local cleanup_name="$1"
    shift
    local cleanup_items_str="$*"
    
    # åŠ¨æ€åˆ›å»ºcleanupå‡½æ•°
    eval "${cleanup_name}() {
        local item
        local cleanup_items=($cleanup_items_str)
        for item in \"\${cleanup_items[@]}\"; do
            if [[ -f \"\$item\" ]]; then
                rm -f \"\$item\"
            elif [[ -d \"\$item\" ]]; then
                rm -rf \"\$item\"
            elif [[ \"\$item\" =~ ^[a-zA-Z0-9_-]+\$ ]]; then
                # å‡è®¾æ˜¯å®¹å™¨å
                docker rm -f \"\$item\" &>/dev/null || true
            fi
        done
    }"
}

# è®¾ç½®cleanup trapçš„é€šç”¨å‡½æ•°
setup_cleanup_trap() {
    local cleanup_function="$1"
    local signals="${2:-EXIT INT TERM}"
    trap '$cleanup_function' "$signals"
}

# Ollamaæ¨¡å‹åˆ—è¡¨ç¼“å­˜
declare -g OLLAMA_MODELS_CACHE=""
declare -g OLLAMA_CACHE_INITIALIZED="false"

# ä¸´æ—¶Ollamaå®¹å™¨ç®¡ç†
declare -g TEMP_OLLAMA_CONTAINER=""
declare -g EXISTING_OLLAMA_CONTAINER=""

# å…¨å±€æ¸…ç†å‡½æ•°ç®¡ç†
declare -g GLOBAL_CLEANUP_FUNCTIONS=()
declare -g GLOBAL_CLEANUP_INITIALIZED="false"

# HuggingFaceç«¯ç‚¹æ£€æµ‹çŠ¶æ€ç®¡ç†
declare -g HF_ENDPOINT_DETECTED="false"

# å…¨å±€æ¸…ç†å‡½æ•°ç®¡ç†
add_cleanup_function() {
    local func_name="$1"
    if [[ -z "$func_name" ]]; then
        log_error "æ¸…ç†å‡½æ•°åç§°ä¸èƒ½ä¸ºç©º"
        return 1
    fi
    
    # æ£€æŸ¥å‡½æ•°æ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…é‡å¤æ·»åŠ 
    local func
    for func in "${GLOBAL_CLEANUP_FUNCTIONS[@]}"; do
        if [[ "$func" == "$func_name" ]]; then
            return 0  # å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
        fi
    done
    
    GLOBAL_CLEANUP_FUNCTIONS+=("$func_name")
    
    # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡æ·»åŠ ï¼Œè®¾ç½®å…¨å±€ trap
    if [[ "$GLOBAL_CLEANUP_INITIALIZED" == "false" ]]; then
        trap 'execute_global_cleanup' EXIT INT TERM
        GLOBAL_CLEANUP_INITIALIZED="true"
        log_verbose "åˆå§‹åŒ–å…¨å±€æ¸…ç†æœºåˆ¶"
    fi
}

# æ‰§è¡Œæ‰€æœ‰æ¸…ç†å‡½æ•°
execute_global_cleanup() {
    local exit_code=$?
    local func
    
    # å¦‚æœæ˜¯ä¸­æ–­ä¿¡å·ï¼Œæ˜¾ç¤ºä¸­æ–­æ¶ˆæ¯
    if [[ $exit_code -eq 130 ]]; then  # Ctrl+C
        log_warning "æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å· (Ctrl+C)"
    elif [[ $exit_code -eq 143 ]]; then  # SIGTERM
        log_warning "æ£€æµ‹åˆ°ç»ˆæ­¢ä¿¡å· (SIGTERM)"
    fi
    
    for func in "${GLOBAL_CLEANUP_FUNCTIONS[@]}"; do
        if declare -f "$func" >/dev/null 2>&1; then
            log_verbose "æ‰§è¡Œæ¸…ç†å‡½æ•°: $func"
            "$func"
        fi
    done
    
    # å¦‚æœæ˜¯ä¸­æ–­ï¼Œé€€å‡º
    if [[ $exit_code -eq 130 || $exit_code -eq 143 ]]; then
        exit $exit_code
    fi
}

# ç§»é™¤æ¸…ç†å‡½æ•°
remove_cleanup_function() {
    local func_name="$1"
    local new_array=()
    local func
    
    for func in "${GLOBAL_CLEANUP_FUNCTIONS[@]}"; do
        if [[ "$func" != "$func_name" ]]; then
            new_array+=("$func")
        fi
    done
    
    GLOBAL_CLEANUP_FUNCTIONS=("${new_array[@]}")
}

# åˆå§‹åŒ–Ollamaæ¨¡å‹åˆ—è¡¨ç¼“å­˜
init_ollama_cache() {
    if [[ "$OLLAMA_CACHE_INITIALIZED" == "true" ]]; then
        return 0
    fi
    
    log_verbose "åˆå§‹åŒ–Ollamaæ¨¡å‹åˆ—è¡¨ç¼“å­˜..."
    
    # ä½¿ç”¨ç»Ÿä¸€çš„å®¹å™¨é€»è¾‘è·å–æ¨¡å‹åˆ—è¡¨
    log_verbose "è·å–Ollamaæ¨¡å‹åˆ—è¡¨..."
    
    # è·å–æ¨¡å‹åˆ—è¡¨å¹¶ç¼“å­˜
    OLLAMA_MODELS_CACHE=$(execute_ollama_command_with_output "list" | awk 'NR>1 {print $1}' | sort)
    if [[ -n "$OLLAMA_MODELS_CACHE" ]]; then
        OLLAMA_CACHE_INITIALIZED="true"
        log_verbose_success "Ollamaæ¨¡å‹åˆ—è¡¨ç¼“å­˜åˆå§‹åŒ–å®Œæˆ"
    else
        log_verbose "Ollamaæ¨¡å‹åˆ—è¡¨ä¸ºç©º"
        OLLAMA_MODELS_CACHE=""
        OLLAMA_CACHE_INITIALIZED="true"
    fi
    
    return 0
}

# æ£€æŸ¥Ollamaæ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
check_ollama_model_exists() {
    local model_name="$1"
    
    # ç¡®ä¿ç¼“å­˜å·²åˆå§‹åŒ–
    if ! init_ollama_cache; then
        log_error "æ— æ³•åˆå§‹åŒ–Ollamaæ¨¡å‹ç¼“å­˜"
        return 1
    fi
    
    # åœ¨ç¼“å­˜ä¸­æŸ¥æ‰¾æ¨¡å‹
    if echo "$OLLAMA_MODELS_CACHE" | grep -q "^${model_name}$"; then
        return 0
    else
        return 1
    fi
}


# éªŒè¯æ¨¡å‹ä¸šåŠ¡é€»è¾‘å®Œæ•´æ€§
validate_model_business_integrity() {
    local backup_file="$1"
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•æå–å¤‡ä»½æ–‡ä»¶
    local temp_dir=$(mktemp -d) || { log_error "æ— æ³•åˆ›å»ºä¸´æ—¶ç›®å½•"; return 1; }
    
    # æ¸…ç†å‡½æ•°
    cleanup_temp_business() {
        [[ -d "${temp_dir:-}" ]] && docker_rm_rf "$temp_dir"
    }
    add_cleanup_function "cleanup_temp_business"
    
    # æå–å¤‡ä»½æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
    if ! docker run --rm --entrypoint="" -v "$(dirname "$backup_file"):/data" -v "$temp_dir:/temp" hf_downloader:latest sh -c "
        cd /data && tar -xf '$(basename "$backup_file")' -C /temp 2>/dev/null
    "; then
        log_error "æ— æ³•æå–å¤‡ä»½æ–‡ä»¶è¿›è¡Œä¸šåŠ¡é€»è¾‘æ£€æŸ¥"
        cleanup_temp_business
        return 1
    fi
    
    # æŸ¥æ‰¾manifestæ–‡ä»¶
    local manifest_files=()
    while IFS= read -r -d '' manifest; do
        manifest_files+=("$manifest")
    done < <(find "$temp_dir" -path "*/manifests/*" -type f -print0 2>/dev/null)
    
    if [[ ${#manifest_files[@]} -eq 0 ]]; then
        log_error "å¤‡ä»½ä¸­æœªæ‰¾åˆ°manifestæ–‡ä»¶"
        cleanup_temp_business
        return 1
    fi
    
    # æ£€æŸ¥æ¯ä¸ªmanifestå¼•ç”¨çš„blobæ–‡ä»¶
    local missing_blobs=0
    local total_blobs=0
    
    for manifest_file in "${manifest_files[@]}"; do
        if [[ -f "$manifest_file" ]]; then
            # è§£æmanifestæ–‡ä»¶ä¸­çš„blobå¼•ç”¨
            local blob_digests
            blob_digests=$(grep -o '"digest":"sha256:[a-f0-9]\{64\}"' "$manifest_file" 2>/dev/null | sed 's/"digest":"sha256:\([a-f0-9]\{64\}\)"/\1/g')
            
            for digest in $blob_digests; do
                ((total_blobs++))
                local blob_path="$temp_dir/blobs/sha256-$digest"
                if [[ ! -f "$blob_path" ]]; then
                    log_error "ç¼ºå°‘blobæ–‡ä»¶: sha256-$digest"
                    ((missing_blobs++))
                fi
            done
        fi
    done
    
    cleanup_temp_business
    remove_cleanup_function "cleanup_temp_business"
    
    if [[ $missing_blobs -gt 0 ]]; then
        log_error "å‘ç° $missing_blobs/$total_blobs ä¸ªblobæ–‡ä»¶ç¼ºå¤±"
        return 1
    fi
    
    log_verbose_success "æ¨¡å‹ä¸šåŠ¡é€»è¾‘å®Œæ•´æ€§éªŒè¯é€šè¿‡ ($total_blobs ä¸ªblobæ–‡ä»¶)"
    return 0
}


# æ¸…ç†ä¸å®Œæ•´çš„æ¨¡å‹
cleanup_incomplete_model() {
    local model_name="$1"
    local model_tag="$2"
    local full_model_name="${model_name}:${model_tag}"
    
    log_verbose_warning "æ£€æµ‹åˆ°ä¸å®Œæ•´çš„æ¨¡å‹ï¼Œæ­£åœ¨æ¸…ç†: $full_model_name"
    
    # ç¡®å®šmanifestæ–‡ä»¶è·¯å¾„
    local manifest_file
    if [[ "$model_name" == hf.co/* ]]; then
        # HuggingFace GGUFæ¨¡å‹
        manifest_file="$OLLAMA_MODELS_DIR/manifests/$model_name/$model_tag"
    elif [[ "$model_name" == *"/"* ]]; then
        # ç”¨æˆ·åˆ†äº«çš„æ¨¡å‹
        local user_name="${model_name%/*}"
        local repo_name="${model_name#*/}"
        manifest_file="$OLLAMA_MODELS_DIR/manifests/registry.ollama.ai/$user_name/$repo_name/$model_tag"
    else
        # å®˜æ–¹æ¨¡å‹
        manifest_file="$OLLAMA_MODELS_DIR/manifests/registry.ollama.ai/library/$model_name/$model_tag"
    fi
    
    # åˆ é™¤manifestæ–‡ä»¶
    if [[ -f "$manifest_file" ]]; then
        if docker_rm_rf "$manifest_file"; then
            log_verbose "å·²åˆ é™¤ä¸å®Œæ•´çš„manifestæ–‡ä»¶: $manifest_file"
        else
            log_warning "æ— æ³•åˆ é™¤manifestæ–‡ä»¶: $manifest_file"
        fi
    fi
    
    # æ¸…é™¤ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°æ£€æŸ¥
    OLLAMA_CACHE_INITIALIZED="false"
    OLLAMA_MODELS_CACHE=""
    
    log_verbose_success "ä¸å®Œæ•´æ¨¡å‹æ¸…ç†å®Œæˆ: $full_model_name"
}

# éªŒè¯æ¨¡å‹å®‰è£…åçš„å®Œæ•´æ€§
verify_model_after_installation() {
    local model_name="$1"
    local model_tag="$2"
    local full_model_name="${model_name}:${model_tag}"
    
    log_verbose "éªŒè¯æ¨¡å‹å®‰è£…å®Œæ•´æ€§: $full_model_name"
    
    # åˆå§‹åŒ–ç¼“å­˜ä»¥æé«˜å®Œæ•´æ€§æ£€æŸ¥æ€§èƒ½
    ensure_cache_initialized
    
    # ç­‰å¾…ä¸€ä¸‹è®©æ–‡ä»¶ç³»ç»ŸåŒæ­¥
    sleep 2
    
    # æ£€æŸ¥æ¨¡å‹å®Œæ•´æ€§ï¼ˆä½¿ç”¨ç¼“å­˜ä¼˜åŒ–ï¼‰
    local model_spec="${model_name}:${model_tag}"
    if verify_integrity "model" "$model_spec" "use_cache:true,check_blobs:true"; then
        log_verbose_success "æ¨¡å‹å®‰è£…å®Œæ•´æ€§éªŒè¯é€šè¿‡: $full_model_name"
        return 0
    else
        log_error "æ¨¡å‹å®‰è£…ä¸å®Œæ•´ï¼Œæ­£åœ¨æ¸…ç†: $full_model_name"
        cleanup_incomplete_model "$model_name" "$model_tag"
        return 1
    fi
}

# ç®€åŒ–çš„æ¨¡å‹æ£€æŸ¥å‡½æ•°
check_ollama_model() {
    local model_name="$1"
    local model_tag="$2"
    local full_model_name="${model_name}:${model_tag}"
    
    # é¦–å…ˆå°è¯•é€šè¿‡Ollamaå®¹å™¨æ£€æŸ¥ï¼ˆæœ€å‡†ç¡®ï¼‰
    if check_ollama_model_exists "$full_model_name"; then
        log_verbose_success "Ollamaæ¨¡å‹å·²å­˜åœ¨: $full_model_name"
        return 0
    fi
    
    # å¦‚æœOllamaå®¹å™¨æ£€æŸ¥å¤±è´¥ï¼Œè¿›è¡Œå®Œæ•´æ€§æ£€æŸ¥ï¼ˆä½¿ç”¨ç¼“å­˜ä¼˜åŒ–ï¼‰
    local model_spec="${model_name}:${model_tag}"
    if verify_integrity "model" "$model_spec" "use_cache:true,check_blobs:true"; then
        log_verbose_success "Ollamaæ¨¡å‹å·²å­˜åœ¨ï¼ˆæ–‡ä»¶ç³»ç»ŸéªŒè¯ï¼‰: $full_model_name"
        return 0
    else
        log_verbose_warning "Ollamaæ¨¡å‹ä¸å­˜åœ¨æˆ–ä¸å®Œæ•´: $full_model_name"
        return 1
    fi
}

# è§£ææ¨¡å‹è§„æ ¼ï¼ˆmodel:versionæ ¼å¼ï¼‰
parse_model_spec() {
    local model_spec="$1"
    local -n name_var="$2"
    local -n version_var="$3"
    
    if ! validate_model_format "$model_spec"; then
        return 1
    fi
    
    name_var="${model_spec%:*}"
    version_var="${model_spec#*:}"
    return 0
}

# åˆå§‹åŒ–ç»å¯¹è·¯å¾„
init_paths() {
    # è·å–ç»å¯¹è·¯å¾„ï¼Œå¦‚æœç›®å½•ä¸å­˜åœ¨åˆ™å…ˆåˆ›å»ºçˆ¶ç›®å½•
    mkdir -p "${OLLAMA_DATA_DIR}" "${HF_DOWNLOAD_CACHE_DIR}" "${HF_ORIGINAL_BACKUP_DIR}" || {
        log_error "æ— æ³•åˆ›å»ºå¿…è¦ç›®å½•"
        return 1
    }
    
    ABS_OLLAMA_DATA_DIR="$(realpath "${OLLAMA_DATA_DIR}")"
    ABS_HF_DOWNLOAD_CACHE_DIR="$(realpath "${HF_DOWNLOAD_CACHE_DIR}")"
    ABS_HF_ORIGINAL_BACKUP_DIR="$(realpath "${HF_ORIGINAL_BACKUP_DIR}")"
    
}

# Docker backup helper functions

# Dockerè¾…åŠ©å‡½æ•° - é‡å‘½ååˆ†å·æ–‡ä»¶ï¼ˆä».000,.001,.002æ ¼å¼åˆ°.001,.002,.003æ ¼å¼ï¼‰

# Docker helper function - list tar content directly

# Dockeræ–‡ä»¶ç³»ç»Ÿæ“ä½œè¾…åŠ©å‡½æ•°
docker_rm_rf() {
    local target_path="$1"
    local parent_dir
    local target_name
    
    # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢åˆ é™¤ç©ºè·¯å¾„æˆ–æ ¹ç›®å½•
    if [[ -z "$target_path" || "$target_path" == "/" ]]; then
        log_error "å®‰å…¨åˆ é™¤: è·¯å¾„ä¸ºç©ºæˆ–æ ¹ç›®å½•ï¼Œæ‹’ç»åˆ é™¤"
        return 1
    fi
    
    # è·å–çˆ¶ç›®å½•å’Œç›®æ ‡åç§°
    parent_dir="$(dirname "$target_path")"
    target_name="$(basename "$target_path")"
    
    # log_info "ä½¿ç”¨Dockeråˆ é™¤: $target_path"
    
    # ä½¿ç”¨Dockerå®¹å™¨ä»¥rootæƒé™åˆ é™¤æ–‡ä»¶/ç›®å½•ï¼Œè¦†ç›–ENTRYPOINT
    docker run --rm --entrypoint="" \
        -v "$parent_dir:/work" \
        "$FULL_IMAGE_NAME" \
        rm -rf "/work/$target_name" 2>/dev/null
}

docker_mkdir_p() {
    local target_path="$1"
    local parent_dir
    local target_name
    
    # å¦‚æœç›®å½•å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
    [[ -d "$target_path" ]] && return 0
    
    # è·å–çˆ¶ç›®å½•å’Œç›®æ ‡åç§°
    parent_dir="$(dirname "$target_path")"
    target_name="$(basename "$target_path")"
    
    
    # ä½¿ç”¨Dockerå®¹å™¨ä»¥rootæƒé™åˆ›å»ºç›®å½•ï¼Œè¦†ç›–ENTRYPOINT
    if docker run --rm --entrypoint="" --user root \
        -v "$parent_dir:/work" \
        "$FULL_IMAGE_NAME" \
        sh -c "mkdir -p /work/$target_name" 2>/dev/null; then
        return 0
    else 
        log_error "Dockeråˆ›å»ºç›®å½•å¤±è´¥: $target_path" >&2
        return 1
    fi
}


# ç¡®ä¿hf_downloaderé•œåƒå­˜åœ¨
ensure_hf_downloader_image() {
    if ! docker image inspect "$FULL_IMAGE_NAME" &>/dev/null; then
        log_verbose "æ„å»º $FULL_IMAGE_NAME é•œåƒ..."
        if ! build_docker_image; then
            log_error "$FULL_IMAGE_NAME é•œåƒæ„å»ºå¤±è´¥"
            return 1
        fi
        log_verbose_success "$FULL_IMAGE_NAME é•œåƒæ„å»ºå®Œæˆ"
    fi
    return 0
}


# ç¡®ä¿ollama/ollamaé•œåƒå­˜åœ¨
ensure_ollama_image() {
    if ! docker image inspect "$DOCKER_IMAGE_OLLAMA" &>/dev/null; then
        log_verbose "æ‹‰å– $DOCKER_IMAGE_OLLAMA é•œåƒ..."
        if ! docker pull "$DOCKER_IMAGE_OLLAMA"; then
            log_error "$DOCKER_IMAGE_OLLAMA é•œåƒæ‹‰å–å¤±è´¥"
            return 1
        fi
        log_verbose_success "$DOCKER_IMAGE_OLLAMA é•œåƒæ‹‰å–å®Œæˆ"
    fi
    return 0
}

# æŸ¥æ‰¾è¿è¡Œä¸­çš„Ollamaå®¹å™¨
find_running_ollama_container() {
    # æ£€æŸ¥æ˜¯å¦æœ‰è¿è¡Œä¸­çš„ Ollama å®¹å™¨
    local running_containers
    running_containers=$(docker ps --format "{{.Names}}" --filter "ancestor=ollama/ollama")
    
    if [[ -n "$running_containers" ]]; then
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªè¿è¡Œä¸­çš„å®¹å™¨
        EXISTING_OLLAMA_CONTAINER=$(echo "$running_containers" | head -n1)
        log_verbose "æ‰¾åˆ°è¿è¡Œä¸­çš„Ollamaå®¹å™¨: $EXISTING_OLLAMA_CONTAINER"
        return 0
    fi
    
    # æ£€æŸ¥æœ¬åœ°11434ç«¯å£æ˜¯å¦æœ‰æœåŠ¡å“åº”ï¼ˆå¯èƒ½æ˜¯å¤–éƒ¨å®¹å™¨ï¼‰
    if command -v curl >/dev/null 2>&1; then
        if curl -s --connect-timeout 2 http://localhost:11434/api/version >/dev/null 2>&1; then
            # æ‰¾åˆ°ä½¿ç”¨11434ç«¯å£çš„å®¹å™¨
            local port_container
            port_container=$(docker ps --format "{{.Names}}" --filter "publish=11434")
            if [[ -n "$port_container" ]]; then
                EXISTING_OLLAMA_CONTAINER=$(echo "$port_container" | head -n1)
                log_verbose "æ‰¾åˆ°ä½¿ç”¨11434ç«¯å£çš„Ollamaå®¹å™¨: $EXISTING_OLLAMA_CONTAINER"
                return 0
            fi
        fi
    fi
    
    EXISTING_OLLAMA_CONTAINER=""
    return 1
}

# å¯åŠ¨ä¸´æ—¶Ollamaå®¹å™¨
start_temp_ollama_container() {
    if [[ -n "$TEMP_OLLAMA_CONTAINER" ]]; then
        # æ£€æŸ¥ä¸´æ—¶å®¹å™¨æ˜¯å¦è¿˜åœ¨è¿è¡Œ
        if docker ps -q --filter "name=^${TEMP_OLLAMA_CONTAINER}$" | grep -q .; then
            log_verbose "ä¸´æ—¶Ollamaå®¹å™¨ä»åœ¨è¿è¡Œ: $TEMP_OLLAMA_CONTAINER"
            return 0
        else
            log_verbose "ä¸´æ—¶Ollamaå®¹å™¨å·²åœæ­¢ï¼Œé‡æ–°å¯åŠ¨"
            TEMP_OLLAMA_CONTAINER=""
        fi
    fi
    
    # ç¡®ä¿ Ollama é•œåƒå­˜åœ¨
    ensure_ollama_image || return 1
    
    TEMP_OLLAMA_CONTAINER="ollama-temp-$$"
    
    log_verbose "å¯åŠ¨ä¸´æ—¶Ollamaå®¹å™¨: $TEMP_OLLAMA_CONTAINER"
    
    # æ„å»ºå®¹å™¨å¯åŠ¨å‘½ä»¤
    local cmd=("docker" "run" "-d" "--name" "$TEMP_OLLAMA_CONTAINER")
    cmd+=("-e" "HF_ENDPOINT=${HF_ENDPOINT}")
    cmd+=("--gpus" "all")
    cmd+=("-v" "${ABS_OLLAMA_DATA_DIR}:/root/.ollama")
    cmd+=("-p" "11435:11434")  # ä½¿ç”¨ä¸åŒç«¯å£é¿å…å†²çª
    cmd+=("$DOCKER_IMAGE_OLLAMA")
    
    # å¯åŠ¨å®¹å™¨
    local start_output
    if start_output=$("${cmd[@]}" 2>&1); then
        log_verbose "ä¸´æ—¶å®¹å™¨å¯åŠ¨æˆåŠŸï¼ŒID: ${start_output:0:12}"
        
        # ç­‰å¾…æœåŠ¡å°±ç»ª
        if wait_for_ollama_ready "$TEMP_OLLAMA_CONTAINER"; then
            log_verbose_success "ä¸´æ—¶Ollamaå®¹å™¨å°±ç»ª: $TEMP_OLLAMA_CONTAINER"
            # è®¾ç½®æ¸…ç†é™·é˜±
            setup_temp_container_cleanup
            return 0
        else
            log_error "ä¸´æ—¶Ollamaå®¹å™¨å¯åŠ¨å¤±è´¥"
            docker rm -f "$TEMP_OLLAMA_CONTAINER" &>/dev/null
            TEMP_OLLAMA_CONTAINER=""
            return 1
        fi
    else
        log_error "æ— æ³•å¯åŠ¨ä¸´æ—¶Ollamaå®¹å™¨"
        log_error "Dockerå¯åŠ¨é”™è¯¯: $start_output"
        TEMP_OLLAMA_CONTAINER=""
        return 1
    fi
}

# æ¸…ç†ä¸´æ—¶Ollamaå®¹å™¨
cleanup_temp_ollama_container() {
    if [[ -n "$TEMP_OLLAMA_CONTAINER" ]]; then
        log_verbose "æ¸…ç†ä¸´æ—¶Ollamaå®¹å™¨: $TEMP_OLLAMA_CONTAINER"
        docker rm -f "$TEMP_OLLAMA_CONTAINER" &>/dev/null
        TEMP_OLLAMA_CONTAINER=""
    fi
}

# è®¾ç½®ä¸´æ—¶å®¹å™¨æ¸…ç†é™·é˜±
setup_temp_container_cleanup() {
    add_cleanup_function "cleanup_temp_ollama_container"
}

# ç»Ÿä¸€çš„Ollamaå‘½ä»¤æ‰§è¡Œå‡½æ•°
execute_ollama_command() {
    local action="$1"
    shift
    local args=("$@")
    
    log_verbose "æ‰§è¡ŒOllamaå‘½ä»¤: $action ${args[*]}"
    
    # é¦–å…ˆæŸ¥æ‰¾è¿è¡Œä¸­çš„Ollamaå®¹å™¨
    if find_running_ollama_container; then
        log_verbose "ä½¿ç”¨ç°æœ‰Ollamaå®¹å™¨: $EXISTING_OLLAMA_CONTAINER"
        log_verbose "æ‰§è¡Œå‘½ä»¤: docker exec $EXISTING_OLLAMA_CONTAINER ollama $action ${args[*]}"
        if docker exec "$EXISTING_OLLAMA_CONTAINER" ollama "$action" "${args[@]}"; then
            return 0
        else
            log_error "åœ¨ç°æœ‰å®¹å™¨ä¸­æ‰§è¡ŒOllamaå‘½ä»¤å¤±è´¥: $action ${args[*]}"
            return 1
        fi
    else
        # æ²¡æœ‰æ‰¾åˆ°è¿è¡Œä¸­çš„å®¹å™¨ï¼Œå¯åŠ¨ä¸´æ—¶å®¹å™¨
        log_verbose "æœªæ‰¾åˆ°è¿è¡Œä¸­çš„Ollamaå®¹å™¨ï¼Œå¯åŠ¨ä¸´æ—¶å®¹å™¨"
        if start_temp_ollama_container; then
            log_verbose "åœ¨ä¸´æ—¶å®¹å™¨ä¸­æ‰§è¡Œå‘½ä»¤: docker exec $TEMP_OLLAMA_CONTAINER ollama $action ${args[*]}"
            if docker exec "$TEMP_OLLAMA_CONTAINER" ollama "$action" "${args[@]}"; then
                return 0
            else
                log_error "åœ¨ä¸´æ—¶å®¹å™¨ä¸­æ‰§è¡ŒOllamaå‘½ä»¤å¤±è´¥: $action ${args[*]}"
                return 1
            fi
        else
            log_error "æ— æ³•å¯åŠ¨ä¸´æ—¶Ollamaå®¹å™¨"
            return 1
        fi
    fi
}

# æ‰§è¡ŒOllamaå‘½ä»¤å¹¶è·å–è¾“å‡º
execute_ollama_command_with_output() {
    local action="$1"
    shift
    local args=("$@")
    
    # é¦–å…ˆæŸ¥æ‰¾è¿è¡Œä¸­çš„Ollamaå®¹å™¨
    if find_running_ollama_container; then
        docker exec "$EXISTING_OLLAMA_CONTAINER" ollama "$action" "${args[@]}" 2>/dev/null
    else
        # æ²¡æœ‰æ‰¾åˆ°è¿è¡Œä¸­çš„å®¹å™¨ï¼Œå¯åŠ¨ä¸´æ—¶å®¹å™¨
        if start_temp_ollama_container; then
            docker exec "$TEMP_OLLAMA_CONTAINER" ollama "$action" "${args[@]}" 2>/dev/null
        else
            return 1
        fi
    fi
}


# æ˜¾ç¤ºä½¿ç”¨å¸®åŠ©
show_help() {
    cat << 'EOF'
ğŸ¤– OMO - Oh My Ollama / Ollama Models Organizer

ä½¿ç”¨æ–¹æ³•:
  ./omo.sh [OPTIONS]

é€‰é¡¹:
  --models-file FILE    æŒ‡å®šæ¨¡å‹åˆ—è¡¨æ–‡ä»¶ (é»˜è®¤: ./models.list)
  --ollama-dir DIR      æŒ‡å®šOllamaæ•°æ®ç›®å½• (é»˜è®¤: ./ollama)
  --hf-backup-dir DIR   æŒ‡å®šHuggingFaceåŸå§‹æ¨¡å‹å¤‡ä»½ç›®å½• (é»˜è®¤: ./hf_originals)
  --install             å®‰è£…/ä¸‹è½½æ¨¡å‹ (è¦†ç›–é»˜è®¤çš„ä»…æ£€æŸ¥è¡Œä¸º)
  --check-only          ä»…æ£€æŸ¥æ¨¡å‹çŠ¶æ€ï¼Œä¸ä¸‹è½½ (é»˜è®¤è¡Œä¸º)
  --force-download      å¼ºåˆ¶é‡æ–°ä¸‹è½½æ‰€æœ‰æ¨¡å‹ (è‡ªåŠ¨å¯ç”¨å®‰è£…æ¨¡å¼)
  --verbose             æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
  --hf-token TOKEN      HuggingFaceè®¿é—®ä»¤ç‰Œ
  --rebuild             å¼ºåˆ¶é‡æ–°æ„å»ºDockeré•œåƒ
  --list                åˆ—å‡ºå·²å®‰è£…çš„Ollamaæ¨¡å‹åŠè¯¦ç»†ä¿¡æ¯
  --backup MODEL        å¤‡ä»½æŒ‡å®šæ¨¡å‹ (æ ¼å¼: æ¨¡å‹å:ç‰ˆæœ¬)
  --backup-all          å¤‡ä»½æ‰€æœ‰æ¨¡å‹
  --restore FILE        æ¢å¤æŒ‡å®šå¤‡ä»½æ–‡ä»¶
  --remove MODEL        åˆ é™¤æŒ‡å®šæ¨¡å‹
  --remove-all          åˆ é™¤æ‰€æœ‰æ¨¡å‹
  --backup-dir DIR      å¤‡ä»½ç›®å½• (é»˜è®¤: ./backups)
  --force               å¼ºåˆ¶æ“ä½œï¼ˆè·³è¿‡ç¡®è®¤ï¼‰
  --generate-compose    ç”Ÿæˆdocker-compose.yamlæ–‡ä»¶ï¼ˆåŸºäºmodels.listï¼‰
  --help                æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯

æ¨¡å‹åˆ—è¡¨æ–‡ä»¶æ ¼å¼:
  ollama deepseek-r1:1.5b
  huggingface microsoft/DialoGPT-medium q4_0
  hf-gguf hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF:latest

ä¸‹è½½ç¼“å­˜:
  HuggingFaceæ¨¡å‹ä¸‹è½½æ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œç¼“å­˜å¤ç”¨
  ç¼“å­˜ç›®å½•: ./hf_download_cache (è‡ªåŠ¨åˆ›å»º)
  æ¯ä¸ªæ¨¡å‹æœ‰ç‹¬ç«‹çš„ç¼“å­˜å­ç›®å½•
  ä¸­æ–­åé‡æ–°è¿è¡Œè„šæœ¬å°†æ¢å¤ä¸‹è½½ï¼Œå®Œæˆåè‡ªåŠ¨ç¼“å­˜

åŸå§‹å¤‡ä»½:
  HuggingFaceæ¨¡å‹è½¬æ¢å®Œæˆåè‡ªåŠ¨å¤‡ä»½åŸå§‹æ–‡ä»¶
  å¤‡ä»½ç›®å½•: ./hf_originals (è‡ªåŠ¨åˆ›å»º)  
  å¤‡ä»½æ ¼å¼: 
EOF
    cat << 'EOF'
  å¤‡ä»½æ ¼å¼: ç›®å½•å¤åˆ¶ (æ¨¡å‹å_original/)
  è‡ªåŠ¨ç”Ÿæˆ: MD5æ ¡éªŒæ–‡ä»¶ (æ¨¡å‹å_original.md5)
  å¤‡ä»½ä¿¡æ¯æ–‡ä»¶: æ¨¡å‹å_original_info.txt (åŒ…å«æ–‡ä»¶åˆ—è¡¨å’ŒMD5æ ¡éªŒ)
  ç”¨é€”: HuggingFace APIè°ƒç”¨ã€é‡æ–°é‡åŒ–ã€æ¨¡å‹æ¢å¤ç­‰

Ollamaæ¨¡å‹å¤‡ä»½:
  æ”¯æŒå®Œæ•´çš„Ollamaæ¨¡å‹å¤‡ä»½å’Œæ¢å¤
  å¤‡ä»½ç›®å½•: ./backups (é»˜è®¤ï¼Œå¯é€šè¿‡--backup-diræŒ‡å®š)
  å¤‡ä»½æ ¼å¼: ç›®å½•å¤åˆ¶ (æ¨¡å‹å/)
  åŒ…å«å†…å®¹: manifestæ–‡ä»¶å’Œæ‰€æœ‰blobæ•°æ®
  è‡ªåŠ¨ç”Ÿæˆ: MD5æ ¡éªŒæ–‡ä»¶å’Œè¯¦ç»†ä¿¡æ¯æ–‡ä»¶
  
å¤‡ä»½ç‰¹æ€§:
  - ç›´æ¥å¤åˆ¶å¤‡ä»½ï¼Œæ— å‹ç¼©å¤„ç†ï¼Œå¤‡ä»½å’Œæ¢å¤é€Ÿåº¦æå¿«
  - MD5æ ¡éªŒç¡®ä¿æ–‡ä»¶å®Œæ•´æ€§
  - æ¯ä¸ªæ¨¡å‹ç‹¬ç«‹æ–‡ä»¶å¤¹ï¼Œä¾¿äºç®¡ç†

ç¤ºä¾‹:
  # æ£€æŸ¥æ¨¡å‹çŠ¶æ€ (é»˜è®¤è¡Œä¸º)
  ./omo.sh
  
  # å®‰è£…/ä¸‹è½½ç¼ºå¤±çš„æ¨¡å‹
  ./omo.sh --install
  
  # ä»…æ£€æŸ¥çŠ¶æ€ (åŒé»˜è®¤è¡Œä¸º)
  ./omo.sh --check-only
  
  # åˆ—å‡ºå·²å®‰è£…çš„æ¨¡å‹
  ./omo.sh --list
  
  # å¤‡ä»½æ¨¡å‹
  ./omo.sh --backup tinyllama:latest
  
  # åˆ é™¤æ¨¡å‹
  ./omo.sh --remove llama2:7b --force

EOF
}

# æ£€æŸ¥ä¾èµ–
# æ£€æŸ¥GPUæ”¯æŒ
check_gpu_support() {
    # æ£€æŸ¥æ˜¯å¦æ”¯æŒNVIDIA GPU
    if command_exists nvidia-smi && nvidia-smi &>/dev/null; then
        return 0  # æ”¯æŒGPU
    fi
    return 1  # ä¸æ”¯æŒGPU
}

check_dependencies() {
    local missing_deps=()
    
    # æ£€æŸ¥ docker
    if ! command_exists docker; then
        missing_deps+=("docker")
        log_error "Docker æœªå®‰è£…æˆ–ä¸åœ¨ PATH ä¸­"
    else
        # æ£€æŸ¥ Docker å®ˆæŠ¤è¿›ç¨‹æ˜¯å¦è¿è¡Œ
        if ! docker info &> /dev/null; then
            log_error "Docker å·²å®‰è£…ä½†å®ˆæŠ¤è¿›ç¨‹æœªè¿è¡Œï¼Œè¯·å¯åŠ¨ Docker æœåŠ¡"
            return 1
        fi
    fi
    
    # æ£€æŸ¥ tar
    if ! command_exists tar; then
        missing_deps+=("tar")
        log_error "tar æœªå®‰è£…ï¼Œç”¨äºæ¨¡å‹æ–‡ä»¶æ‰“åŒ…/è§£åŒ…"
    fi
    
    # å¦‚æœæœ‰ç¼ºå¤±çš„ä¾èµ–ï¼Œç»™å‡ºæç¤ºå¹¶é€€å‡º
    if [ ${#missing_deps[@]} -gt 0 ]; then
        log_error "ç¼ºå°‘å¿…éœ€çš„ç³»ç»Ÿä¾èµ–: ${missing_deps[*]}"
        log_error "è¯·å®‰è£…ç¼ºå¤±çš„ä¾èµ–åé‡æ–°è¿è¡Œè„šæœ¬"
        return 1
    fi
    
    # æ£€æŸ¥GPUæ”¯æŒï¼ˆå¿…éœ€é¡¹ï¼‰
    if ! check_gpu_support; then
        log_error "æœªæ£€æµ‹åˆ°NVIDIA GPUæ”¯æŒï¼Œæ­¤è„šæœ¬éœ€è¦GPUç¯å¢ƒ"
        log_error "è¯·ç¡®ä¿ï¼š1) å®‰è£…äº†NVIDIAé©±åŠ¨  2) å®‰è£…äº†nvidia-smiå·¥å…·"
        return 1
    fi
    
    log_verbose "æ£€æµ‹åˆ°GPUæ”¯æŒï¼Œå°†å¯ç”¨GPUåŠ é€Ÿ"
    
    # æ‰€æœ‰ä¾èµ–æ£€æŸ¥é€šè¿‡ï¼Œé™é»˜è¿”å›
    return 0
}

# æ„å»ºDockeré•œåƒ - é›†æˆç‰ˆæœ¬
build_docker_image() {
    log_verbose "æ„å»ºDockeré•œåƒ: $FULL_IMAGE_NAME"
    
    # åˆ›å»ºä¸´æ—¶æ„å»ºç›®å½•
    local temp_build_dir="/tmp/docker_build_$$"
    
    # ä¿å­˜å½“å‰çš„trapè®¾ç½®
    local original_trap
    original_trap=$(trap -p EXIT | sed "s/trap -- '//" | sed "s/' EXIT//")
    
    # è®¾ç½®å¤åˆtrap - ä¿®å¤shellcheck SC2089/SC2090
    if [[ -n "$original_trap" ]]; then
        trap "cleanup_docker_build_context '$temp_build_dir'; $original_trap" EXIT
    else
        trap "cleanup_docker_build_context '$temp_build_dir'" EXIT
    fi
    
    # åˆ›å»ºæ„å»ºä¸Šä¸‹æ–‡
    create_docker_build_context "$temp_build_dir"
    
    # æ„å»ºæ”¯æŒCUDAçš„é•œåƒ
    local build_args=()
    log_verbose "æ„å»ºæ”¯æŒCUDAçš„é•œåƒï¼Œè¯·è€å¿ƒç­‰å¾…..."
    build_args+=("--build-arg" "USE_CUDA=true")
    
    # æ‰§è¡Œæ„å»ºå‘½ä»¤
    local docker_build_cmd=("docker" "build" "${build_args[@]}" "-t" "$FULL_IMAGE_NAME" "$temp_build_dir")
    
    if "${docker_build_cmd[@]}"; then
        log_verbose_success "Dockeré•œåƒæ„å»ºå®Œæˆ: $FULL_IMAGE_NAME"
        cleanup_docker_build_context "$temp_build_dir"
        # æ¢å¤åŸå§‹trap
        if [[ -n "$original_trap" ]]; then
            trap '$original_trap' EXIT
        else
            trap - EXIT
        fi
        return 0
    else
        log_error "Dockeré•œåƒæ„å»ºå¤±è´¥"
        cleanup_docker_build_context "$temp_build_dir"
        # æ¢å¤åŸå§‹trap
        if [[ -n "$original_trap" ]]; then
            trap '$original_trap' EXIT
        else
            trap - EXIT
        fi
        exit 1
    fi
}


# è§£ææ¨¡å‹åˆ—è¡¨æ–‡ä»¶
parse_models_list() {
    local models_file="$1"
    local -n models_array=${2:-models}
    
    if [[ ! -f "$models_file" ]]; then
        log_error "æ¨¡å‹åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: $models_file"
        return 1
    fi
    
    log_verbose "è§£ææ¨¡å‹åˆ—è¡¨æ–‡ä»¶: $models_file"
    
    while IFS= read -r line || [[ -n "$line" ]]; do
        # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Šè¡Œ
        [[ -z "$line" || "$line" =~ ^[[:space:]]*# ]] && continue
        
        # ä½¿ç”¨ç©ºæ ¼åˆ†éš”è§£ææ¨¡å‹ä¿¡æ¯: æ¨¡å‹ç±»å‹ æ¨¡å‹åç§° [é‡åŒ–ç±»å‹]
        read -r model_type model_name quantization <<< "$line"
        
        if [[ -n "$model_type" && -n "$model_name" ]]; then
            if [[ "$model_type" == "ollama" || "$model_type" == "huggingface" || "$model_type" == "hf-gguf" ]]; then
                # å¦‚æœæœ‰é‡åŒ–ç±»å‹ï¼Œæ·»åŠ åˆ°æ¨¡å‹ä¿¡æ¯ä¸­
                if [[ -n "$quantization" ]]; then
                    models_array+=("$model_type:$model_name:$quantization")
                    log_verbose "æ·»åŠ æ¨¡å‹: $model_type -> $model_name:$quantization"
                else
                    models_array+=("$model_type:$model_name")
                    log_verbose "æ·»åŠ æ¨¡å‹: $model_type -> $model_name"
                fi
            else
                log_warning "æœªçŸ¥æ¨¡å‹ç±»å‹: $model_type (è¡Œ: $line)"
            fi
        else
            log_warning "å¿½ç•¥æ— æ•ˆè¡Œ: $line"
        fi
    done < "$models_file"
    
    # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°æœ‰æ•ˆæ¨¡å‹
    if [[ ${#models_array[@]} -eq 0 ]]; then
        log_warning "================================ WARNING ================================"
        log_warning "No valid models found in models.list file!"
        log_warning "All model entries are either commented out or invalid."
        log_warning ""
        log_warning "Please edit the models.list file:"
        log_warning "1. Uncomment (remove # at the beginning) the models you need"
        log_warning "2. Add your own model configurations"
        log_warning "3. Check model search URLs in the comments for available models"
        log_warning ""
        log_warning "Examples:"
        log_warning "  ollama deepseek-r1:1.5b"
        log_warning "  huggingface Qwen/Qwen3-0.6B q4_0"
        log_warning "  hf-gguf hf.co/MaziyarPanahi/gemma-3-1b-it-GGUF"
        log_warning "====================================================================="
        echo
    else
        log_verbose "å…±è§£æåˆ° ${#models_array[@]} ä¸ªæ¨¡å‹"
    fi
}

# æ£€æŸ¥HuggingFace GGUFæ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼ˆé€šè¿‡Ollamaæ£€æŸ¥ï¼‰
check_hf_gguf_model() {
    local model_name="$1"
    local model_tag="$2"
    local full_model_name="${model_name}:${model_tag}"
    
    
    # ä½¿ç”¨å®¹å™¨æ£€æŸ¥
    if check_ollama_model_exists "$full_model_name"; then
        log_verbose_success "HuggingFace GGUFæ¨¡å‹å·²å­˜åœ¨: $full_model_name"
        return 0
    fi
    
    log_verbose_warning "HuggingFace GGUFæ¨¡å‹ä¸å­˜åœ¨: $full_model_name"
    return 1
}

# ç”ŸæˆOllamaæ¨¡å‹åç§°
generate_ollama_model_name() {
    local model_name="$1"
    local quantize_type="$2"
    
    # æ¸…ç†é‡åŒ–ç±»å‹
    local clean_quant=$(echo "${quantize_type}" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9]//g')
    
    # ä½¿ç”¨ç»Ÿä¸€çš„å‘½åå‡½æ•°è¿›è¡ŒOllamaæ¨¡å‹åç§°è½¬æ¢
    local full_name_clean=$(get_safe_model_name "$model_name" "ollama")
    
    # ä¸ºä»HuggingFaceä¸‹è½½çš„æ¨¡å‹æ·»åŠ è¯†åˆ«å‰ç¼€å’Œé‡åŒ–åç¼€
    echo "hf-${full_name_clean}:${clean_quant}"
}

# ä¸‹è½½Ollamaæ¨¡å‹
download_ollama_model() {
    local model_name="$1"
    local model_tag="$2"
    
    log_info "ä¸‹è½½æ¨¡å‹: ${model_name}:${model_tag}"
    
    if execute_ollama_command "pull" "${model_name}:${model_tag}"; then
        log_verbose_success "Ollamaæ¨¡å‹ä¸‹è½½å®Œæˆ: ${model_name}:${model_tag}"
        
        # éªŒè¯ä¸‹è½½åçš„æ¨¡å‹å®Œæ•´æ€§
        if verify_model_after_installation "$model_name" "$model_tag"; then
            log_verbose_success "æ¨¡å‹å®Œæ•´æ€§éªŒè¯é€šè¿‡: ${model_name}:${model_tag}"
            return 0
        else
            log_verbose_warning "æ¨¡å‹å®Œæ•´æ€§éªŒè¯å¤±è´¥ï¼Œæ¨¡å‹å·²è¢«æ¸…ç†: ${model_name}:${model_tag}"
            return 1
        fi
    else
        log_error "Ollamaæ¨¡å‹ä¸‹è½½å¤±è´¥: ${model_name}:${model_tag}"
        return 1
    fi
}

# ä¸‹è½½HuggingFace GGUFæ¨¡å‹ï¼ˆé€šè¿‡Ollamaç›´æ¥ä¸‹è½½ï¼‰
download_hf_gguf_model() {
    local model_name="$1"
    local model_tag="$2"
    local full_model_name="${model_name}:${model_tag}"
    
    log_verbose "å¼€å§‹ä¸‹è½½HuggingFace GGUFæ¨¡å‹: $full_model_name"
    
    if execute_ollama_command "pull" "$full_model_name"; then
        log_verbose_success "HuggingFace GGUFæ¨¡å‹ä¸‹è½½å®Œæˆ: $full_model_name"
        
        # éªŒè¯ä¸‹è½½åçš„æ¨¡å‹å®Œæ•´æ€§
        if verify_model_after_installation "$model_name" "$model_tag"; then
            log_verbose_success "æ¨¡å‹å®Œæ•´æ€§éªŒè¯é€šè¿‡: $full_model_name"
            return 0
        else
            log_error "æ¨¡å‹å®Œæ•´æ€§éªŒè¯å¤±è´¥ï¼Œæ¨¡å‹å·²è¢«æ¸…ç†: $full_model_name"
            return 1
        fi
    else
        log_error "HuggingFace GGUFæ¨¡å‹ä¸‹è½½å¤±è´¥: $full_model_name"
        return 1
    fi
}

# åˆ é™¤Ollamaæ¨¡å‹
remove_ollama_model() {
    local model_spec="$1"
    local force_delete="${2:-false}"
    
    # è§£ææ¨¡å‹åç§°å’Œç‰ˆæœ¬
    if ! validate_model_format "$model_spec"; then
        return 1
    fi
    
    log_verbose "å‡†å¤‡åˆ é™¤Ollamaæ¨¡å‹: $model_spec"
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    local model_name model_version
    if ! parse_model_spec "$model_spec" model_name model_version; then
        return 1
    fi
    if ! check_ollama_model "$model_name" "$model_version"; then
        log_warning "æ¨¡å‹ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤: $model_spec"
        return 0
    fi
    
    # å¦‚æœä¸æ˜¯å¼ºåˆ¶åˆ é™¤ï¼Œè¯¢é—®ç”¨æˆ·ç¡®è®¤
    if [[ "$force_delete" != "true" ]]; then
        log_warning "å³å°†åˆ é™¤æ¨¡å‹: $model_spec"
        echo -n "ç¡®è®¤åˆ é™¤ï¼Ÿ[y/N]: "
        read -r confirm
        if [[ "$confirm" != "y" && "$confirm" != "Y" ]]; then
            log_verbose "å–æ¶ˆåˆ é™¤æ“ä½œ"
            return 0
        fi
    fi
    
    if execute_ollama_command "rm" "$model_spec"; then
        log_verbose_success "Ollamaæ¨¡å‹åˆ é™¤å®Œæˆ: $model_spec"
        return 0
    else
        log_error "Ollamaæ¨¡å‹åˆ é™¤å¤±è´¥: $model_spec"
        return 1
    fi
}

# è·å–æ¨¡å‹ç›¸å…³çš„blobæ–‡ä»¶è·¯å¾„
get_model_blob_paths() {
    local manifest_file="$1"
    local models_dir="$2"
    local blob_paths=()
    
    if [[ ! -f "$manifest_file" ]]; then
        log_error "æ¨¡å‹manifestæ–‡ä»¶ä¸å­˜åœ¨: $manifest_file"
        return 1
    fi
    
    # ä½¿ç”¨hf_downloaderé•œåƒä¸­çš„jqè§£æJSONæ–‡ä»¶
    local layers
    layers=$(docker run --rm --entrypoint="" -v "$(dirname "$manifest_file"):/data" hf_downloader jq -r '.layers[].digest, .config.digest' "/data/$(basename "$manifest_file")" 2>/dev/null | sort -u)
    
    # æ„å»ºblobæ–‡ä»¶è·¯å¾„
    while IFS= read -r digest; do
        if [[ -n "$digest" ]]; then
            # å°† sha256:xxx æ ¼å¼è½¬æ¢ä¸º sha256-xxx
            local blob_name="${digest//:/-}"
            local blob_file="$models_dir/blobs/$blob_name"
            blob_paths+=("$blob_file")
        fi
    done <<< "$layers"
    
    # è¾“å‡ºè·¯å¾„
    printf '%s\n' "${blob_paths[@]}"
}

# ===== å¤‡ä»½å·¥å…·å‡½æ•° =====

# é€šç”¨å‘½ä»¤æ£€æŸ¥å‡½æ•°
command_exists() {
    command -v "$1" >/dev/null 2>&1
}

# æ¨¡å‹åç§°å®‰å…¨åŒ–å¤„ç†
# ç»Ÿä¸€çš„æ¨¡å‹åç§°è½¬æ¢å‡½æ•°
# å‚æ•°1: æ¨¡å‹åç§°
# å‚æ•°2: è½¬æ¢ç±»å‹ (backup|ollama|filesystem)
get_safe_model_name() {
    local model_spec="$1"
    local conversion_type="${2:-backup}"
    
    case "$conversion_type" in
        "backup")
            # ç”¨äºå¤‡ä»½ç›®å½•å‘½åï¼š/ å’Œ : â†’ _
            echo "$model_spec" | sed 's/[\/:]/_/g'
            ;;
        "ollama")
            # ç”¨äºOllamaæ¨¡å‹å‘½åï¼šå¤æ‚è½¬æ¢è§„åˆ™ï¼ˆä¸€æ¬¡æ€§å¤„ç†ï¼‰
            local full_name_clean
            full_name_clean=$(echo "$model_spec" | tr '[:upper:]' '[:lower:]' | sed -e 's/\//_/g' -e 's/[^a-z0-9_-]/_/g' -e 's/__*/_/g' -e 's/--*/-/g' -e 's/^[-_]\+\|[-_]\+$//g')
            # é•¿åº¦é™åˆ¶
            if [[ ${#full_name_clean} -gt 50 ]]; then
                local prefix="${full_name_clean:0:30}"
                local suffix="${full_name_clean: -15}"
                full_name_clean="${prefix}_${suffix}"
            fi
            echo "$full_name_clean"
            ;;
        "filesystem")
            # ç”¨äºæ–‡ä»¶ç³»ç»Ÿå®‰å…¨å‘½åï¼š/ â†’ _ï¼Œå…¶ä»–éæ³•å­—ç¬¦ â†’ -
            echo "$model_spec" | sed -e 's/\//_/g' -e 's/[^a-zA-Z0-9._-]/-/g'
            ;;
        *)
            # é»˜è®¤ä½¿ç”¨backupè§„åˆ™
            echo "$model_spec" | sed 's/[\/:]/_/g'
            ;;
    esac
}

# æ–‡ä»¶å¤§å°å·¥å…·å‡½æ•°
get_file_size() {
    local file_path="$1"
    local format="${2:-mb}"  # mb, human
    
    case "$format" in
        "mb")
            du -sm "$file_path" 2>/dev/null | cut -f1
            ;;
        "human")
            du -sh "$file_path" 2>/dev/null | cut -f1 
            ;;
        *)
            log_error "Unknown format: $format. Use 'mb' or 'human'"
            return 1
            ;;
    esac
}

# å‘åå…¼å®¹çš„åŒ…è£…å‡½æ•°
get_file_size_mb() {
    get_file_size "$1" "mb"
}

get_file_size_human() {
    get_file_size "$1" "human"
}

# è®¡ç®—ç›®å½•çš„MD5æ ¡éªŒå€¼
calculate_directory_md5() {
    local dir_path="$1"
    local md5_file="$2"
    
    if [[ ! -d "$dir_path" ]]; then
        log_error "ç›®å½•ä¸å­˜åœ¨: $dir_path"
        return 1
    fi
    
    log_verbose "æ­£åœ¨è®¡ç®—ç›®å½•MD5æ ¡éªŒå€¼: $dir_path"
    
    # ä½¿ç”¨findå’Œmd5sumè®¡ç®—æ‰€æœ‰æ–‡ä»¶çš„MD5å€¼ï¼Œä½¿ç”¨ç›¸å¯¹è·¯å¾„
    # æŒ‰æ–‡ä»¶è·¯å¾„æ’åºä»¥ç¡®ä¿ç»“æœä¸€è‡´æ€§
    if (cd "$dir_path" && find . -type f -print0 | sort -z | xargs -0 md5sum) > "$md5_file" 2>/dev/null; then
        log_verbose "MD5æ ¡éªŒæ–‡ä»¶å·²ç”Ÿæˆ: $md5_file"
        return 0
    else
        log_error "MD5æ ¡éªŒè®¡ç®—å¤±è´¥"
        return 1
    fi
}

# éªŒè¯ç›®å½•çš„MD5æ ¡éªŒå€¼
verify_directory_md5() {
    local dir_path="$1"
    local md5_file="$2"
    
    if [[ ! -d "$dir_path" ]]; then
        log_error "ç›®å½•ä¸å­˜åœ¨: $dir_path"
        return 1
    fi
    
    if [[ ! -f "$md5_file" ]]; then
        log_error "MD5æ ¡éªŒæ–‡ä»¶ä¸å­˜åœ¨: $md5_file"
        return 1
    fi
    
    log_verbose "æ­£åœ¨éªŒè¯ç›®å½•MD5æ ¡éªŒå€¼: $dir_path"
    
    # ä¸´æ—¶è®¡ç®—å½“å‰ç›®å½•çš„MD5å€¼
    local temp_md5=$(mktemp)
    if ! calculate_directory_md5 "$dir_path" "$temp_md5"; then
        rm -f "$temp_md5"
        return 1
    fi
    
    # æ¯”è¾ƒMD5æ–‡ä»¶
    if diff "$md5_file" "$temp_md5" >/dev/null 2>&1; then
        log_verbose "MD5æ ¡éªŒé€šè¿‡"
        rm -f "$temp_md5"
        return 0
    else
        log_error "MD5æ ¡éªŒå¤±è´¥"
        rm -f "$temp_md5"
        return 1
    fi
}

# å…¨å±€ç¼“å­˜å˜é‡
declare -A BACKUP_CONTENT_CACHE
declare -A MODEL_BLOB_CACHE

# æ£€æŸ¥å¤‡ä»½å®Œæ•´æ€§ï¼ˆæ£€æŸ¥å¤‡ä»½ä¸­æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…éœ€çš„blobæ–‡ä»¶ï¼‰

# è·å–æ¨¡å‹blobåˆ—è¡¨ï¼ˆå¸¦ç¼“å­˜ï¼‰
get_model_blobs_cached() {
    local model_spec="$1"
    
    # æ£€æŸ¥ç¼“å­˜
    if [[ -n "${MODEL_BLOB_CACHE[$model_spec]:-}" ]]; then
        echo "${MODEL_BLOB_CACHE[$model_spec]}"
        return 0
    fi
    
    # è§£ææ¨¡å‹åç§°å’Œç‰ˆæœ¬
    local model_name model_version
    if ! parse_model_spec "$model_spec" model_name model_version; then
        return 1
    fi
    
    # ç¡®å®šmanifestæ–‡ä»¶è·¯å¾„
    local manifest_file
    if [[ "$model_name" == hf.co/* ]]; then
        manifest_file="$OLLAMA_MODELS_DIR/manifests/$model_name/$model_version"
    elif [[ "$model_name" == *"/"* ]]; then
        local user_name="${model_name%/*}"
        local repo_name="${model_name#*/}"
        manifest_file="$OLLAMA_MODELS_DIR/manifests/registry.ollama.ai/$user_name/$repo_name/$model_version"
    else
        manifest_file="$OLLAMA_MODELS_DIR/manifests/registry.ollama.ai/library/$model_name/$model_version"
    fi
    
    # è·å–blobæ–‡ä»¶åˆ—è¡¨
    if [[ -f "$manifest_file" ]]; then
        local blobs=$(get_model_blob_paths "$manifest_file" "$OLLAMA_MODELS_DIR" | sed "s|^$OLLAMA_MODELS_DIR/||")
        if [[ -n "$blobs" ]]; then
            # ç¼“å­˜ç»“æœ
            MODEL_BLOB_CACHE[$model_spec]="$blobs"
            echo "$blobs"
            return 0
        fi
    fi
    
    return 1
}

# å¿«é€Ÿæ£€æŸ¥å•æ–‡ä»¶å¤‡ä»½å®Œæ•´æ€§


# æ¸…ç†å®Œæ•´æ€§æ£€æŸ¥ç¼“å­˜
clear_integrity_cache() {
    [[ -n "${VERBOSE}" ]] && log_verbose "æ¸…ç†å®Œæ•´æ€§æ£€æŸ¥ç¼“å­˜"
    unset BACKUP_CONTENT_CACHE
    unset MODEL_BLOB_CACHE
    declare -g -A BACKUP_CONTENT_CACHE
    declare -g -A MODEL_BLOB_CACHE
}

# ç¡®ä¿å®Œæ•´æ€§æ£€æŸ¥ç¼“å­˜å·²åˆå§‹åŒ–
ensure_cache_initialized() {
    # å¦‚æœç¼“å­˜æ•°ç»„ä¸å­˜åœ¨ï¼Œåˆå§‹åŒ–å®ƒä»¬
    if [[ ! -v BACKUP_CONTENT_CACHE ]] || [[ ! -v MODEL_BLOB_CACHE ]]; then
        declare -g -A BACKUP_CONTENT_CACHE
        declare -g -A MODEL_BLOB_CACHE
        [[ -n "${VERBOSE}" ]] && log_verbose "å®Œæ•´æ€§æ£€æŸ¥ç¼“å­˜å·²åˆå§‹åŒ–"
    fi
}

# ==================================================================================
#                           ç»Ÿä¸€å®Œæ•´æ€§éªŒè¯æ¶æ„
# ==================================================================================

# é€šç”¨å®Œæ•´æ€§éªŒè¯å‡½æ•° - ç»Ÿä¸€æ‰€æœ‰éªŒè¯é€»è¾‘çš„å…¥å£ç‚¹
verify_integrity() {
    local verification_type="$1"  # model, backup, hf_model
    local target="$2"             # ç›®æ ‡æ–‡ä»¶/è·¯å¾„/æ¨¡å‹è§„æ ¼
    local options="${3:-}"        # é™„åŠ é€‰é¡¹ (use_cache:true, check_blobs:true, etc.)
    
    # è§£æé€‰é¡¹
    local use_cache="true"
    local check_blobs="true"
    local model_spec=""
    
    # è§£æé€‰é¡¹å­—ç¬¦ä¸²
    if [[ -n "$options" ]]; then
        while IFS=',' read -ra ADDR; do
            for i in "${ADDR[@]}"; do
                case "$i" in
                    use_cache:*)
                        use_cache="${i#*:}"
                        ;;
                    check_blobs:*)
                        check_blobs="${i#*:}"
                        ;;
                    model_spec:*)
                        model_spec="${i#*:}"
                        ;;
                esac
            done
        done <<< "$options"
    fi
    
    # ç¡®ä¿ç¼“å­˜å·²åˆå§‹åŒ–
    [[ "$use_cache" == "true" ]] && ensure_cache_initialized
    
    # æ ¹æ®éªŒè¯ç±»å‹è°ƒç”¨ç›¸åº”çš„éªŒè¯é€»è¾‘
    case "$verification_type" in
        "model")
            _verify_local_model "$target" "$use_cache" "$check_blobs"
            ;;
        "backup")
            _verify_backup_target "$target" "$model_spec" "$use_cache" "$check_blobs"
            ;;
        "hf_model")
            _verify_hf_model "$target" "$check_blobs"
            ;;
        "backup_file")
            _verify_backup_file "$target" "$use_cache"
            ;;
        *)
            log_error "Unknown verification type: $verification_type"
            return 1
            ;;
    esac
}

# å†…éƒ¨å‡½æ•°ï¼šéªŒè¯æœ¬åœ°æ¨¡å‹å®Œæ•´æ€§
_verify_local_model() {
    local model_spec="$1"
    local use_cache="$2"
    local check_blobs="$3"
    
    # è§£ææ¨¡å‹è§„æ ¼
    local model_name model_tag
    if [[ "$model_spec" =~ ^(.+):(.+)$ ]]; then
        model_name="${BASH_REMATCH[1]}"
        model_tag="${BASH_REMATCH[2]}"
    else
        log_error "Invalid model spec format: $model_spec"
        return 1
    fi
    
    # ç¡®å®šmanifestæ–‡ä»¶è·¯å¾„
    local manifest_file
    if [[ "$model_name" == hf.co/* ]]; then
        manifest_file="$OLLAMA_MODELS_DIR/manifests/$model_name/$model_tag"
    elif [[ "$model_name" == *"/"* ]]; then
        local user_name="${model_name%/*}"
        local repo_name="${model_name#*/}"
        manifest_file="$OLLAMA_MODELS_DIR/manifests/registry.ollama.ai/$user_name/$repo_name/$model_tag"
    else
        manifest_file="$OLLAMA_MODELS_DIR/manifests/registry.ollama.ai/library/$model_name/$model_tag"
    fi
    
    # æ£€æŸ¥manifestæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    [[ ! -f "$manifest_file" ]] && return 1
    
    # å¦‚æœä¸éœ€è¦æ£€æŸ¥blobï¼ŒåªéªŒè¯manifestå­˜åœ¨å³å¯
    [[ "$check_blobs" == "false" ]] && return 0
    
    # è·å–blobæ–‡ä»¶åˆ—è¡¨å¹¶éªŒè¯
    local blob_files
    if [[ "$use_cache" == "true" ]]; then
        blob_files=$(get_model_blobs_cached "$model_spec")
        [[ -z "$blob_files" ]] && return 1
        
        # æ£€æŸ¥æ¯ä¸ªblobæ–‡ä»¶
        while IFS= read -r blob_relative_path; do
            [[ -n "$blob_relative_path" && ! -f "$OLLAMA_MODELS_DIR/$blob_relative_path" ]] && return 1
        done <<< "$blob_files"
    else
        blob_files=$(get_model_blob_paths "$manifest_file" "$OLLAMA_MODELS_DIR")
        [[ -z "$blob_files" ]] && return 1
        
        # æ£€æŸ¥æ¯ä¸ªblobæ–‡ä»¶
        while IFS= read -r blob_file; do
            [[ -n "$blob_file" && ! -f "$blob_file" ]] && return 1
        done <<< "$blob_files"
    fi
    
    return 0
}

# å†…éƒ¨å‡½æ•°ï¼šéªŒè¯å¤‡ä»½ç›®æ ‡ï¼ˆç›®å½•å¤‡ä»½ï¼‰
_verify_backup_target() {
    local backup_target="$1"
    local model_spec="$2"
    local use_cache="$3"
    local check_blobs="$4"
    
    # æ£€æŸ¥ç›®å½•å¤‡ä»½
    if [[ -d "$backup_target" ]]; then
        # éªŒè¯ç›®å½•ç»“æ„
        if [[ -d "$backup_target/manifests" ]] && [[ -d "$backup_target/blobs" ]]; then
            # éªŒè¯MD5æ ¡éªŒ
            local md5_file="${backup_target}.md5"
            if [[ -f "$md5_file" ]]; then
                if verify_directory_md5 "$backup_target" "$md5_file"; then
                    [[ -n "${VERBOSE}" ]] && log_info "ç›®å½•å¤‡ä»½MD5æ ¡éªŒé€šè¿‡: $backup_target"
                    return 0
                else
                    log_error "ç›®å½•å¤‡ä»½MD5æ ¡éªŒå¤±è´¥: $backup_target"
                    return 1
                fi
            else
                log_warning "æœªæ‰¾åˆ°MD5æ ¡éªŒæ–‡ä»¶: $md5_file"
                return 0  # æ²¡æœ‰MD5æ–‡ä»¶ä¹Ÿè®¤ä¸ºæœ‰æ•ˆï¼Œä½†ä¼šè®°å½•è­¦å‘Š
            fi
        else
            log_error "æ— æ•ˆçš„ç›®å½•å¤‡ä»½ç»“æ„: $backup_target"
            return 1
        fi
    fi
    
    return 1
}


# å†…éƒ¨å‡½æ•°ï¼šéªŒè¯HuggingFaceæ¨¡å‹
_verify_hf_model() {
    local source_dir="$1"
    local check_files="$2"
    
    # æ£€æŸ¥æºç›®å½•æ˜¯å¦å­˜åœ¨
    [[ ! -d "$source_dir" ]] && return 1
    
    # å¦‚æœä¸éœ€è¦æ£€æŸ¥æ–‡ä»¶ï¼ŒåªéªŒè¯ç›®å½•å­˜åœ¨
    [[ "$check_files" == "false" ]] && return 0
    
    # æ£€æŸ¥å¿…è¦çš„æ–‡ä»¶
    local has_model_files=false
    for ext in safetensors bin gguf; do
        if ls "$source_dir"/*."$ext" >/dev/null 2>&1; then
            has_model_files=true
            break
        fi
    done
    
    [[ "$has_model_files" == "false" ]] && return 1
    return 0
}

# å†…éƒ¨å‡½æ•°ï¼šéªŒè¯å¤‡ä»½æ–‡ä»¶ï¼ˆä¸šåŠ¡é€»è¾‘å®Œæ•´æ€§ï¼‰
_verify_backup_file() {
    local backup_file="$1"
    local use_detailed_check="$2"
    
    [[ ! -f "$backup_file" ]] && return 1
    
    # åŸºæœ¬taræ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥
    if ! docker run --rm --entrypoint="" -v "$(dirname "$backup_file"):/data" hf_downloader:latest sh -c "
        cd /data && tar -tf '$(basename "$backup_file")' >/dev/null 2>&1
    "; then
        return 1
    fi
    
    # å¦‚æœéœ€è¦è¯¦ç»†æ£€æŸ¥ï¼Œæ‰§è¡Œä¸šåŠ¡é€»è¾‘éªŒè¯
    [[ "$use_detailed_check" == "true" ]] && validate_model_business_integrity "$backup_file"
}

# åˆ é™¤ä¸å®Œæ•´çš„å¤‡ä»½æ–‡ä»¶
remove_incomplete_backup() {
    local backup_base="$1"
    local backup_suffix="${2:-}"
    
    log_verbose "åˆ é™¤ä¸å®Œæ•´çš„å¤‡ä»½: ${backup_base}${backup_suffix}"
    
    # åˆ é™¤ç›®å½•å¤‡ä»½
    local backup_dir="${backup_base}${backup_suffix}"
    if [[ -d "$backup_dir" ]]; then
        rm -rf "$backup_dir"
        log_verbose "å·²åˆ é™¤å¤‡ä»½ç›®å½•: $backup_dir"
    fi
    
    # åˆ é™¤MD5æ ¡éªŒæ–‡ä»¶
    local md5_file="${backup_dir}.md5"
    if [[ -f "$md5_file" ]]; then
        rm -f "$md5_file"
        log_verbose "å·²åˆ é™¤MD5æ ¡éªŒæ–‡ä»¶: $md5_file"
    fi
    
    # åˆ é™¤å¤‡ä»½ä¿¡æ¯æ–‡ä»¶
    local info_file="${backup_base}${backup_suffix}_info.txt"
    if [[ -f "$info_file" ]]; then
        rm -f "$info_file"
        log_verbose "å·²åˆ é™¤å¤‡ä»½ä¿¡æ¯æ–‡ä»¶: $info_file"
    fi
}


# å®‰å…¨çš„ä¸´æ—¶æ–‡ä»¶åˆ›å»º
create_temp_file() {
    local prefix="${1:-temp}"
    local temp_file
    temp_file=$(mktemp) || {
        log_error "æ— æ³•åˆ›å»ºä¸´æ—¶æ–‡ä»¶"
        return 1
    }
    echo "$temp_file"
}


# åˆ›å»ºæ¨¡å‹å¤‡ä»½ç›®å½•
create_model_backup_dir() {
    local model_spec="$1"
    local base_backup_dir="$2"
    local model_safe_name=$(get_safe_model_name "$model_spec")
    local model_backup_dir="${base_backup_dir}/${model_safe_name}"
    
    # åˆ›å»ºå¤‡ä»½ç›®å½•
    if ! mkdir -p "$model_backup_dir"; then
        log_error "æ— æ³•åˆ›å»ºå¤‡ä»½ç›®å½•: $model_backup_dir"
        return 1
    fi
    echo "$model_backup_dir"
}

# ç”Ÿæˆå¤‡ä»½åŸºç¡€è·¯å¾„
get_backup_base_path() {
    local model_spec="$1"
    local backup_dir="$2"
    local suffix="${3:-}"
    local model_safe_name=$(get_safe_model_name "$model_spec")
    echo "${backup_dir}/${model_safe_name}${suffix}"
}

# åˆ›å»ºtaræ–‡ä»¶çš„é€šç”¨å‡½æ•°ï¼ˆç”¨äºHuggingFaceæ¨¡å‹ï¼‰
create_hf_tar_file() {
    local output_file="$1"
    local source_dir="$2"
    
    # åˆ›å»ºæ’é™¤æ–‡ä»¶åˆ—è¡¨
    local temp_exclude="/tmp/tar_exclude_$$.txt"
    cat > "$temp_exclude" << 'EOF'
*.aria2
*.tmp
*.part
EOF
    
    # ä½¿ç”¨Dockeråˆ›å»ºtaræ–‡ä»¶
    local output_dir="$(dirname "$output_file")"
    local output_basename="$(basename "$output_file")"
    local source_parent="$(dirname "$source_dir")"
    local source_name="$(basename "$source_dir")"
    
    # ç¡®ä¿hf_downloaderé•œåƒå­˜åœ¨
    ensure_hf_downloader_image || { rm -f "$temp_exclude"; return 1; }
    
    if docker run --rm --entrypoint="" \
        -v "$source_parent:/source:ro" \
        -v "$output_dir:/output" \
        -v "$temp_exclude:/exclude.txt:ro" \
        "$FULL_IMAGE_NAME" \
        tar -cf "/output/$output_basename" -C /source --exclude-from=/exclude.txt "$source_name" 2>/dev/null; then
        rm -f "$temp_exclude"
        return 0
    else
        log_error "åˆ›å»ºtaræ–‡ä»¶å¤±è´¥: $output_file"
        rm -f "$temp_exclude"
        return 1
    fi
}

# å¤‡ä»½ä¿¡æ¯å’Œç®¡ç†å‡½æ•°

# åˆ›å»ºå¤‡ä»½ä¿¡æ¯æ–‡ä»¶
create_backup_info() {
    local model_spec="$1"
    local backup_base="$2"
    local backup_type="$3"  # "directory", "single" æˆ– "split"
    local volume_count="$4"
    local backup_extension="${5:-original}"
    
    local info_file="${backup_base}_info.txt"
    local current_time=$(date '+%Y-%m-%d %H:%M:%S %Z')
    local model_safe_name=$(get_safe_model_name "$model_spec")
    
    # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶åˆ›å»ºå¤‡ä»½ä¿¡æ¯
    local temp_info=$(mktemp)
    cat > "$temp_info" << EOF
================================================================================
                           æ¨¡å‹å¤‡ä»½ä¿¡æ¯
================================================================================

å¤‡ä»½åŸºæœ¬ä¿¡æ¯:
  æ¨¡å‹è§„æ ¼: $model_spec
  å¤‡ä»½åç§°: ${model_safe_name}
  å¤‡ä»½ç±»å‹: $backup_type
  åˆ›å»ºæ—¶é—´: $current_time

å¤‡ä»½æ–‡ä»¶ä¿¡æ¯:
EOF

    # æ ¹æ®å¤‡ä»½ç±»å‹æ·»åŠ å…·ä½“çš„æ–‡ä»¶ä¿¡æ¯å’ŒMD5
    if [[ "$backup_type" == "directory" ]]; then
        local backup_dir="${backup_base}_${backup_extension}"
        # å¯¹äºollamaå¤‡ä»½ï¼Œbackup_baseå·²ç»æ˜¯å®Œæ•´è·¯å¾„ï¼Œä¸éœ€è¦æ·»åŠ åç¼€
        if [[ "$backup_extension" == "ollama" ]]; then
            backup_dir="$backup_base"
        fi
        local backup_size=$(get_file_size_human "$backup_dir" || echo "æœªçŸ¥")
        local md5_file="${backup_dir}.md5"
        local md5_status="æœ‰æ•ˆ"
        
        if [[ ! -f "$md5_file" ]]; then
            md5_status="ç¼ºå¤±"
        fi
        
        cat >> "$temp_info" << EOF
  å¤‡ä»½æ–¹å¼: ç›®å½•å¤åˆ¶
  å¤‡ä»½ç›®å½•: $(basename "$backup_dir")
  å¤‡ä»½å¤§å°: $backup_size
  MD5æ ¡éªŒæ–‡ä»¶: $md5_status

æ–‡ä»¶åˆ—è¡¨:
EOF
        
        # æ·»åŠ æ–‡ä»¶åˆ—è¡¨
        if [[ -d "$backup_dir" ]]; then
            find "$backup_dir" -type f -exec basename {} \; | sort >> "$temp_info"
        fi
        
        cat >> "$temp_info" << EOF

MD5æ ¡éªŒä¿¡æ¯:
EOF
        
        # æ·»åŠ MD5æ ¡éªŒä¿¡æ¯
        if [[ -f "$md5_file" ]]; then
            cat "$md5_file" >> "$temp_info"
        else
            echo "  MD5æ ¡éªŒæ–‡ä»¶åˆ›å»ºå¤±è´¥æˆ–ä¸å­˜åœ¨" >> "$temp_info"
            echo "  æ–‡ä»¶è·¯å¾„: $md5_file" >> "$temp_info"
            echo "  å»ºè®®: é‡æ–°è¿è¡Œå¤‡ä»½ä»¥ç”ŸæˆMD5æ ¡éªŒæ–‡ä»¶" >> "$temp_info"
        fi
        
        cat >> "$temp_info" << EOF

æ¢å¤å‘½ä»¤:
  # ä½¿ç”¨omo.shæ¢å¤
  ./omo.sh --restore "$(basename "$backup_dir")"
  
  # æ‰‹åŠ¨æ¢å¤ï¼ˆOllamaæ¨¡å‹ï¼‰
  cp -r "$(basename "$backup_dir")/manifests/"* "\$OLLAMA_MODELS_DIR/manifests/"
  cp "$(basename "$backup_dir")/blobs/"* "\$OLLAMA_MODELS_DIR/blobs/"
  
  # æ‰‹åŠ¨æ¢å¤ï¼ˆHuggingFaceæ¨¡å‹ï¼‰
  cp -r "$(basename "$backup_dir")" "\$HF_DOWNLOAD_CACHE_DIR/"

EOF
    else
        log_error "ä¸æ”¯æŒçš„å¤‡ä»½ç±»å‹: $backup_type"
        rm -f "$temp_info"
        return 1
    fi
    
    cat >> "$temp_info" << EOF
================================================================================
                               éªŒè¯ä¿¡æ¯
================================================================================

å¤‡ä»½éªŒè¯:
1. æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§:
   - ä½¿ç”¨MD5æ ¡éªŒæ–‡ä»¶éªŒè¯æ¯ä¸ªæ–‡ä»¶çš„å®Œæ•´æ€§
   - md5sum -c $(basename "${backup_dir}.md5")

2. æ£€æŸ¥å¤‡ä»½ç»“æ„:
   - ç¡®ä¿å¤‡ä»½ç›®å½•åŒ…å«å®Œæ•´çš„æ–‡ä»¶ç»“æ„
   - å¯¹äºOllamaæ¨¡å‹: manifests/ å’Œ blobs/ ç›®å½•
   - å¯¹äºHuggingFaceæ¨¡å‹: æ¨¡å‹æ–‡ä»¶å’Œé…ç½®æ–‡ä»¶

å¤‡ä»½ç‰¹æ€§:
   - ç›´æ¥å¤åˆ¶: æå¿«çš„å¤‡ä»½å’Œæ¢å¤é€Ÿåº¦ï¼Œæ— éœ€å‹ç¼©/è§£å‹ç¼©
   - MD5æ ¡éªŒ: ç¡®ä¿æ–‡ä»¶å®Œæ•´æ€§å’Œä¸€è‡´æ€§
   - ç®€åŒ–ç®¡ç†: å¤‡ä»½æ–‡ä»¶å¯ç›´æ¥è®¿é—®å’Œæ£€æŸ¥

ä½¿ç”¨è¯´æ˜:
- æ­¤å¤‡ä»½åŒ…å«æ¨¡å‹çš„å®Œæ•´æ–‡ä»¶ç»“æ„
- æ¢å¤åå¯ç›´æ¥ä½¿ç”¨ï¼Œæ— éœ€é¢å¤–å¤„ç†
- æ”¯æŒå¢é‡å¤‡ä»½å’Œå·®å¼‚æ£€æŸ¥

ç”Ÿæˆæ—¶é—´: $current_time
================================================================================
EOF

    # ç›´æ¥å†™å…¥ä¿¡æ¯æ–‡ä»¶
    if mv "$temp_info" "$info_file"; then
        log_verbose_success "å¤‡ä»½ä¿¡æ¯æ–‡ä»¶åˆ›å»ºå®Œæˆ: $(basename "$info_file")"
    else
        log_error "æ— æ³•å†™å…¥å¤‡ä»½ä¿¡æ¯æ–‡ä»¶: $info_file"
        rm -f "$temp_info"
        return 1
    fi
}

# åˆ›å»ºHuggingFaceåŸå§‹æ¨¡å‹å¤‡ä»½ï¼ˆç›´æ¥å¤åˆ¶ï¼‰
backup_hf_original_model() {
    local model_name="$1"
    local source_dir="$2"
    
    log_info "å¤‡ä»½æ¨¡å‹: $model_name"
    log_verbose "æºç›®å½•: $source_dir"
    
    # æ£€æŸ¥æºç›®å½•æ˜¯å¦å­˜åœ¨
    if [[ ! -d "$source_dir" ]]; then
        log_error "æºç›®å½•ä¸å­˜åœ¨: $source_dir"
        return 1
    fi
    
    # æ£€æŸ¥æœ¬åœ°æ¨¡å‹å®Œæ•´æ€§
    log_info "æ£€æŸ¥æ¨¡å‹å®Œæ•´æ€§..."
    if ! verify_integrity "hf_model" "$source_dir" "check_files:true"; then
        log_error "æœ¬åœ°æ¨¡å‹ä¸å®Œæ•´ï¼Œå–æ¶ˆå¤‡ä»½æ“ä½œ"
        return 1
    fi
    
    # åˆ›å»ºå¤‡ä»½ç›®å½•å’Œç”Ÿæˆè·¯å¾„
    local model_backup_dir
    model_backup_dir=$(create_model_backup_dir "$model_name" "$ABS_HF_ORIGINAL_BACKUP_DIR") || return 1
    local model_safe_name=$(get_safe_model_name "$model_name")
    local backup_dir="$model_backup_dir/${model_safe_name}_original"
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å¤‡ä»½ç›®å½•
    if [[ -d "$backup_dir" ]]; then
        log_info "æ¨¡å‹å¤‡ä»½å·²å­˜åœ¨"
        return 0
    fi
    
    log_verbose "æ¨¡å‹å¤‡ä»½ç›®å½•: $backup_dir"
    
    # è®¡ç®—æºç›®å½•å¤§å°
    local source_size_human=$(get_file_size_human "$source_dir")
    log_verbose "æºç›®å½•å¤§å°: $source_size_human"
    
    # å¤åˆ¶æºç›®å½•åˆ°å¤‡ä»½ç›®å½•ï¼Œæ’é™¤ .hfd ä¸´æ—¶ç›®å½•
    log_info "æ­£åœ¨å¤åˆ¶æ¨¡å‹æ–‡ä»¶..."
    mkdir -p "$backup_dir"
    if rsync -av --exclude='.hfd' "$source_dir/" "$backup_dir/" 2>/dev/null || {
        # å¦‚æœæ²¡æœ‰ rsyncï¼Œä½¿ç”¨ cp åŠ æ‰‹åŠ¨æ’é™¤
        log_verbose "rsync ä¸å¯ç”¨ï¼Œä½¿ç”¨ cp å¤åˆ¶ï¼ˆæ’é™¤ .hfd ç›®å½•ï¼‰"
        # ä½¿ç”¨ find å¤åˆ¶ï¼Œæ’é™¤ .hfd ç›®å½•
        (cd "$source_dir" && find . -type d -name '.hfd' -prune -o -type f -print0 | cpio -0pdm "$backup_dir/") 2>/dev/null
    }; then
        # è®¡ç®—MD5æ ¡éªŒ
        log_verbose "è®¡ç®—MD5æ ¡éªŒå€¼..."
        local md5_file="${backup_dir}.md5"
        if calculate_directory_md5 "$backup_dir" "$md5_file"; then
            log_verbose "MD5æ ¡éªŒæ–‡ä»¶å·²åˆ›å»º: $md5_file"
        else
            log_warning "MD5æ ¡éªŒæ–‡ä»¶åˆ›å»ºå¤±è´¥"
        fi
        
        # åˆ›å»ºå¤‡ä»½ä¿¡æ¯æ–‡ä»¶
        create_backup_info "$model_name" "${model_backup_dir}/${model_safe_name}" "directory" 1 "original"
        
        log_success "HuggingFaceåŸå§‹æ¨¡å‹å¤‡ä»½å®Œæˆ: $model_name"
        return 0
    else
        log_error "å¤åˆ¶æ–‡ä»¶å¤±è´¥"
        rm -rf "$backup_dir" 2>/dev/null
        return 1
    fi
}

# åˆ—å‡ºå·²å®‰è£…çš„Ollamaæ¨¡å‹åŠè¯¦ç»†ä¿¡æ¯
list_installed_models() {
    log_info "æ‰«æå·²å®‰è£…çš„æ¨¡å‹..."
    
    # åˆå§‹åŒ–ç¼“å­˜ä»¥æé«˜å®Œæ•´æ€§æ£€æŸ¥æ€§èƒ½
    ensure_cache_initialized
    
    # æ£€æŸ¥Ollamaæ¨¡å‹ç›®å½•æ˜¯å¦å­˜åœ¨
    if [[ ! -d "$OLLAMA_MODELS_DIR" ]]; then
        log_error "Ollamaæ¨¡å‹ç›®å½•ä¸å­˜åœ¨: $OLLAMA_MODELS_DIR"
        return 1
    fi
    
    local blobs_dir="$OLLAMA_MODELS_DIR/blobs"
    local manifests_base_dir="$OLLAMA_MODELS_DIR/manifests"
    
    # æ£€æŸ¥manifestsåŸºç¡€ç›®å½•æ˜¯å¦å­˜åœ¨
    if [[ ! -d "$manifests_base_dir" ]]; then
        log_warning "æœªå‘ç°å·²å®‰è£…çš„æ¨¡å‹"
        return 0
    fi
    
    echo ""
    echo "=================================================================================="
    echo "                             å·²å®‰è£…çš„Ollamaæ¨¡å‹"
    echo "=================================================================================="
    echo ""
    
    local model_count=0
    local total_size=0
    local total_version_count=0
    
    # é€’å½’æŸ¥æ‰¾æ‰€æœ‰ manifest æ–‡ä»¶
    local manifest_files=()
    while IFS= read -r -d '' manifest_file; do
        manifest_files+=("$manifest_file")
    done < <(find "$manifests_base_dir" -type f -print0 2>/dev/null)
    
    # æŒ‰æ¨¡å‹ç»„ç»‡ manifest æ–‡ä»¶
    declare -A model_manifests
    
    for manifest_file in "${manifest_files[@]}"; do
        # æå–ç›¸å¯¹äº manifests_base_dir çš„è·¯å¾„
        local relative_path="${manifest_file#$manifests_base_dir/}"
        
        # æ ¹æ®è·¯å¾„ç»“æ„æå–æ¨¡å‹åå’Œç‰ˆæœ¬
        local model_name=""
        local version=""
        local full_model_path=""
        
        if [[ "$relative_path" =~ ^registry\.ollama\.ai/library/([^/]+)/(.+)$ ]]; then
            # ä¼ ç»Ÿ Ollama æ¨¡å‹: registry.ollama.ai/library/model_name/version
            model_name="${BASH_REMATCH[1]}"
            version="${BASH_REMATCH[2]}"
            full_model_path="registry.ollama.ai/library/$model_name"
        elif [[ "$relative_path" =~ ^hf\.co/([^/]+)/([^/]+)/(.+)$ ]]; then
            # HF-GGUF æ¨¡å‹: hf.co/user/repo/version
            local user="${BASH_REMATCH[1]}"
            local repo="${BASH_REMATCH[2]}"
            version="${BASH_REMATCH[3]}"
            model_name="hf.co/$user/$repo"
            full_model_path="hf.co/$user/$repo"
        else
            # å…¶ä»–æœªçŸ¥æ ¼å¼ï¼Œå°è¯•é€šç”¨è§£æ
            local path_parts
            IFS='/' read -ra path_parts <<< "$relative_path"
            if [[ ${#path_parts[@]} -ge 2 ]]; then
                version="${path_parts[-1]}"
                unset path_parts[-1]
                model_name=$(IFS='/'; echo "${path_parts[*]}")
                full_model_path="$model_name"
            else
                continue
            fi
        fi
        
        # å°† manifest æ·»åŠ åˆ°å¯¹åº”æ¨¡å‹ç»„
        if [[ -n "$model_name" && -n "$version" ]]; then
            local key="$model_name"
            if [[ -z "${model_manifests[$key]:-}" ]]; then
                model_manifests[$key]="$manifest_file|$version|$full_model_path"
            else
                model_manifests[$key]="${model_manifests[$key]};;$manifest_file|$version|$full_model_path"
            fi
        fi
    done
    
    # æ˜¾ç¤ºæ¯ä¸ªæ¨¡å‹çš„ä¿¡æ¯
    for model_name in "${!model_manifests[@]}"; do
        local model_data="${model_manifests[$model_name]}"
        
        # è§£æç¬¬ä¸€ä¸ªæ¡ç›®ä»¥è·å–è·¯å¾„ä¿¡æ¯
        local first_entry="${model_data%%;*}"
        local full_model_path="${first_entry##*|}"
        local model_dir="$manifests_base_dir/$full_model_path"
        
        echo "ğŸ“¦ æ¨¡å‹: $model_name"
        [[ "${VERBOSE}" == "true" ]] && echo "   â”œâ”€ ä½ç½®: $model_dir"
        
        local version_count=0
        
        # å¤„ç†æ‰€æœ‰ç‰ˆæœ¬
        IFS=';;' read -ra entries <<< "$model_data"
        for entry in "${entries[@]}"; do
            IFS='|' read -r manifest_file version _ <<< "$entry"
            
            if [[ ! -f "$manifest_file" ]]; then
                continue
            fi
            
            # æ£€æŸ¥æ¨¡å‹å®Œæ•´æ€§ï¼ˆä½¿ç”¨ç¼“å­˜ä¼˜åŒ–ï¼‰
            local integrity_status=""
            local check_model_spec="${model_name}:${version}"
            if verify_integrity "model" "$check_model_spec" "use_cache:true,check_blobs:true"; then
                integrity_status=" âœ“(å®Œæ•´)"
            else
                integrity_status=" âš ï¸(ä¸å®Œæ•´)"
            fi
            
            echo "   â”œâ”€ ç‰ˆæœ¬: $version$integrity_status"
            
            # è¯»å–manifestæ–‡ä»¶è·å–blobä¿¡æ¯
            if [[ "${VERBOSE}" == "true" ]] && [[ -f "$manifest_file" ]]; then
                local manifest_content
                if manifest_content=$(cat "$manifest_file" 2>/dev/null); then
                    # manifestæ˜¯JSONæ ¼å¼ï¼Œè§£æè·å–æ‰€æœ‰å±‚çš„å¤§å°
                    local total_model_size=0
                    local blob_count=0
                    local model_type="æœªçŸ¥"
                    
                    # å°è¯•ä»JSONä¸­æå–æ¨¡å‹ç±»å‹
                    if echo "$manifest_content" | grep -q "application/vnd.ollama.image.model"; then
                        model_type="Ollamaæ¨¡å‹"
                    fi
                    
                    # æå–configå¤§å°
                    local config_size
                    if config_size=$(echo "$manifest_content" | grep -o '"config":{[^}]*"size":[0-9]*' | grep -o '[0-9]*$' 2>/dev/null); then
                        total_model_size=$((total_model_size + config_size))
                        blob_count=$((blob_count + 1))
                    fi
                    
                    # æå–æ‰€æœ‰layersçš„å¤§å°
                    local layer_sizes
                    if layer_sizes=$(echo "$manifest_content" | grep -o '"size":[0-9]*' | grep -o '[0-9]*' 2>/dev/null); then
                        while IFS= read -r size; do
                            if [[ -n "$size" && "$size" -gt 0 ]]; then
                                total_model_size=$((total_model_size + size))
                                blob_count=$((blob_count + 1))
                            fi
                        done <<< "$layer_sizes"
                    fi
                    
                    # æ ¼å¼åŒ–å¤§å°æ˜¾ç¤º
                    local human_size=$(format_bytes "$total_model_size")
                    
                    echo "   â”œâ”€ å¤§å°: $human_size"
                    
                    total_size=$((total_size + total_model_size))
                fi
            fi
            
            version_count=$((version_count + 1))
        done
        
        echo "   â””â”€ ç‰ˆæœ¬æ•°é‡: $version_count"
        echo ""
        model_count=$((model_count + 1))
        total_version_count=$((total_version_count + version_count))
    done
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    echo "=================================================================================="
    echo "ç»Ÿè®¡ä¿¡æ¯:"
    echo "  ğŸ“Š æ€»æ¨¡å‹æ•°: $model_count"
    echo "  ğŸ”¢ æ€»ç‰ˆæœ¬æ•°: $total_version_count"
    
    # æ ¼å¼åŒ–æ€»å¤§å°
    if [[ "${VERBOSE}" == "true" ]]; then
        local total_human_size=$(format_bytes "$total_size")
        echo "  ğŸ’¾ æ€»å¤§å°: $total_human_size"
    fi
    echo "  ğŸ“ ç›®å½•: $OLLAMA_MODELS_DIR"
    
    # æ˜¾ç¤ºç£ç›˜ä½¿ç”¨æƒ…å†µ
    local disk_usage
    if disk_usage=$(du -sh "$OLLAMA_MODELS_DIR" 2>/dev/null); then
        echo "  ğŸ—„ï¸ ç£ç›˜å ç”¨: $(echo "$disk_usage" | cut -f1)"
    fi
    
    echo "=================================================================================="
    echo ""
    
    return 0
}

# å¤‡ä»½Ollamaæ¨¡å‹ï¼ˆç›´æ¥å¤åˆ¶ï¼‰
backup_ollama_model() {
    local model_spec="$1"
    local backup_dir="$2"
    
    # åˆå§‹åŒ–ç¼“å­˜ä»¥æé«˜å®Œæ•´æ€§æ£€æŸ¥æ€§èƒ½
    ensure_cache_initialized
    
    # è§£ææ¨¡å‹åç§°å’Œç‰ˆæœ¬
    local model_name model_version
    if ! parse_model_spec "$model_spec" model_name model_version; then
        return 1
    fi
    
    log_verbose "å¤‡ä»½æ¨¡å‹: $model_name:$model_version"
    local model_spec="${model_name}:${model_version}"
    if ! verify_integrity "model" "$model_spec" "use_cache:true,check_blobs:true"; then
        log_error "æœ¬åœ°æ¨¡å‹ä¸å®Œæ•´ï¼Œå–æ¶ˆå¤‡ä»½æ“ä½œ"
        return 1
    fi
    
    # åˆ›å»ºå¤‡ä»½ç›®å½•å’Œç”Ÿæˆè·¯å¾„
    local model_backup_dir
    model_backup_dir=$(create_model_backup_dir "$model_spec" "$backup_dir") || return 1
    local model_safe_name=$(get_safe_model_name "$model_spec")
    local backup_model_dir="$model_backup_dir/$model_safe_name"
    
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å¤‡ä»½ç›®å½•
    if [[ -d "$backup_model_dir" ]]; then
        log_success "æ¨¡å‹å¤‡ä»½å·²å­˜åœ¨"
        return 0
    fi
    
    # ç¡®å®šmanifestæ–‡ä»¶è·¯å¾„
    local manifest_file
    if [[ "$model_name" == hf.co/* ]]; then
        # HuggingFace GGUFæ¨¡å‹ï¼Œå¦‚ hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF
        manifest_file="$OLLAMA_MODELS_DIR/manifests/$model_name/$model_version"
    elif [[ "$model_name" == *"/"* ]]; then
        # ç”¨æˆ·åˆ†äº«çš„æ¨¡å‹ï¼Œå¦‚ lrs33/bce-embedding-base_v1
        local user_name="${model_name%/*}"
        local repo_name="${model_name#*/}"
        manifest_file="$OLLAMA_MODELS_DIR/manifests/registry.ollama.ai/$user_name/$repo_name/$model_version"
    else
        # å®˜æ–¹æ¨¡å‹
        manifest_file="$OLLAMA_MODELS_DIR/manifests/registry.ollama.ai/library/$model_name/$model_version"
    fi
    
    # æ£€æŸ¥manifestæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if [[ ! -f "$manifest_file" ]]; then
        log_error "æ¨¡å‹ä¸å­˜åœ¨: $model_spec"
        return 1
    fi
    
    # è·å–blobæ–‡ä»¶è·¯å¾„
    local blob_files
    blob_files=$(get_model_blob_paths "$manifest_file" "$OLLAMA_MODELS_DIR")
    
    if [[ -z "$blob_files" ]]; then
        log_error "æœªæ‰¾åˆ°æ¨¡å‹ç›¸å…³çš„blobæ–‡ä»¶"
        return 1
    fi
    
    # åˆ›å»ºå¤‡ä»½ç›®å½•ç»“æ„
    mkdir -p "$backup_model_dir/manifests"
    mkdir -p "$backup_model_dir/blobs"
    
    log_verbose "å¼€å§‹å¤åˆ¶æ–‡ä»¶..."
    
    # å¤åˆ¶manifestæ–‡ä»¶
    local manifest_rel_path="${manifest_file#$OLLAMA_MODELS_DIR/manifests/}"
    local manifest_backup_dir="$backup_model_dir/manifests/$(dirname "$manifest_rel_path")"
    mkdir -p "$manifest_backup_dir"
    if ! cp "$manifest_file" "$manifest_backup_dir/"; then
        log_error "å¤åˆ¶manifestæ–‡ä»¶å¤±è´¥: $manifest_file"
        rm -rf "$backup_model_dir"
        return 1
    fi
    
    # å¤åˆ¶blobæ–‡ä»¶
    while IFS= read -r blob_file; do
        if [[ -f "$blob_file" ]]; then
            local blob_name=$(basename "$blob_file")
            if ! cp "$blob_file" "$backup_model_dir/blobs/"; then
                log_error "å¤åˆ¶blobæ–‡ä»¶å¤±è´¥: $blob_file"
                rm -rf "$backup_model_dir"
                return 1
            fi
        fi
    done <<< "$blob_files"
    
    # è®¡ç®—MD5æ ¡éªŒ
    log_verbose "è®¡ç®—MD5æ ¡éªŒå€¼..."
    local md5_file="${backup_model_dir}.md5"
    if calculate_directory_md5 "$backup_model_dir" "$md5_file"; then
        log_verbose "MD5æ ¡éªŒæ–‡ä»¶å·²åˆ›å»º: $md5_file"
    else
        log_warning "MD5æ ¡éªŒæ–‡ä»¶åˆ›å»ºå¤±è´¥"
    fi
    
    # åˆ›å»ºå¤‡ä»½ä¿¡æ¯æ–‡ä»¶
    create_backup_info "$model_spec" "$backup_model_dir" "directory" 1 "ollama"
    
    log_verbose_success "æ¨¡å‹å¤‡ä»½å®Œæˆ: $model_spec"
    return 0
}



# æ™ºèƒ½åˆ é™¤æ¨¡å‹ï¼ˆè‡ªåŠ¨è¯†åˆ«æ¨¡å‹ç±»å‹ï¼‰
remove_model_smart() {
    local model_input="$1"
    local force_delete="${2:-false}"
    
    log_info "åˆ é™¤æ¨¡å‹: $model_input"
    
    # æ£€æŸ¥è¾“å…¥æ ¼å¼ï¼Œåˆ¤æ–­æ˜¯ä»€ä¹ˆç±»å‹çš„æ¨¡å‹
    if [[ "$model_input" =~ ^([^:]+):(.+)$ ]]; then
        local model_name="${BASH_REMATCH[1]}"
        local model_tag_or_quant="${BASH_REMATCH[2]}"
        
        # å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯Ollamaæ¨¡å‹ï¼ˆç›´æ¥æ ¼å¼ï¼šmodel:tagï¼‰
        if check_ollama_model "$model_name" "$model_tag_or_quant"; then
            if remove_ollama_model "$model_input" "$force_delete"; then
                return 0
            else
                return 1
            fi
        fi
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯GGUFæ¨¡å‹ï¼ˆç”Ÿæˆçš„Ollamaæ¨¡å‹åï¼‰
        local generated_name=$(generate_ollama_model_name "$model_name" "$model_tag_or_quant")
        
        # ç»Ÿä¸€åˆ é™¤å¤„ç†
        if remove_ollama_model "$generated_name" "$force_delete"; then
            return 0
        else
            return 1
        fi
        
    else
        log_error "æ¨¡å‹æ ¼å¼é”™è¯¯ï¼Œåº”ä¸º 'æ¨¡å‹å:ç‰ˆæœ¬' æˆ– 'æ¨¡å‹å:é‡åŒ–ç±»å‹'"
        log_error "ä¾‹å¦‚: 'llama2:7b' æˆ– 'microsoft/DialoGPT-small:q4_0'"
        return 1
    fi
}

# æ£€æµ‹å¤‡ä»½æ–‡ä»¶ç±»å‹




# æ¢å¤Ollamaæ¨¡å‹ï¼ˆç›®å½•å¤‡ä»½ï¼‰
restore_ollama_model() {
    local backup_dir="$1"
    local force_restore="$2"
    
    log_info "æ¢å¤æ¨¡å‹: $(basename "$backup_dir")"
    
    # æ£€æŸ¥å¤‡ä»½ç›®å½•æ˜¯å¦å­˜åœ¨
    if [[ ! -d "$backup_dir" ]]; then
        log_error "å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: $backup_dir"
        return 1
    fi
    
    # æ£€æŸ¥å¤‡ä»½ç›®å½•ç»“æ„
    if [[ ! -d "$backup_dir/manifests" ]] || [[ ! -d "$backup_dir/blobs" ]]; then
        log_error "å¤‡ä»½æ–‡ä»¶æŸåæˆ–æ ¼å¼é”™è¯¯"
        return 1
    fi
    
    # MD5æ ¡éªŒ
    local md5_file="${backup_dir}.md5"
    if [[ -f "$md5_file" ]]; then
        log_info "æ ¡éªŒå¤‡ä»½æ–‡ä»¶..."
        if verify_directory_md5 "$backup_dir" "$md5_file"; then
            log_verbose_success "MD5æ ¡éªŒé€šè¿‡"
        else
            log_error "å¤‡ä»½æ–‡ä»¶æ ¡éªŒå¤±è´¥ï¼Œå¯èƒ½å·²æŸå"
            if [[ "$force_restore" != "true" ]]; then
                return 1
            fi
            log_warning "å¼ºåˆ¶æ¢å¤æ¨¡å¼ï¼Œç»§ç»­æ“ä½œ..."
        fi
    else
        log_warning "è·³è¿‡å®Œæ•´æ€§æ ¡éªŒ"
    fi
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶è¦†ç›–
    if [[ "$force_restore" != "true" ]]; then
        log_info "æ£€æŸ¥æ¨¡å‹å†²çª..."
        local conflicts_found=false
        
        # æ£€æŸ¥manifestså†²çª
        if find "$backup_dir/manifests" -type f 2>/dev/null | while read -r manifest_file; do
            local rel_path="${manifest_file#$backup_dir/manifests/}"
            local target_file="$OLLAMA_MODELS_DIR/manifests/$rel_path"
            if [[ -f "$target_file" ]]; then
                echo "conflict"
                break
            fi
        done | grep -q "conflict"; then
            conflicts_found=true
        fi
        
        # æ£€æŸ¥blobså†²çª
        if find "$backup_dir/blobs" -type f 2>/dev/null | while read -r blob_file; do
            local blob_name=$(basename "$blob_file")
            local target_file="$OLLAMA_MODELS_DIR/blobs/$blob_name"
            if [[ -f "$target_file" ]]; then
                echo "conflict"
                break
            fi
        done | grep -q "conflict"; then
            conflicts_found=true
        fi
        
        if [[ "$conflicts_found" == "true" ]]; then
            log_error "æ£€æµ‹åˆ°æ–‡ä»¶å†²çªï¼Œä½¿ç”¨ --force å¼ºåˆ¶è¦†ç›–"
            return 1
        fi
    fi
    
    # ä½¿ç”¨Dockerç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨å¹¶æœ‰æ­£ç¡®æƒé™
    ensure_hf_downloader_image || return 1
    
    # ä½¿ç”¨Dockeråˆ›å»ºOllamaç›®å½•å¹¶è®¾ç½®æƒé™
    if ! docker run --rm --entrypoint="" --user root \
        -v "$OLLAMA_MODELS_DIR:/ollama" \
        "$FULL_IMAGE_NAME" \
        sh -c "mkdir -p /ollama/manifests /ollama/blobs"; then
        log_error "æ— æ³•åˆ›å»ºOllamaç›®å½•"
        return 1
    fi
    
    # å¤åˆ¶manifests
    log_verbose "æ¢å¤æ¨¡å‹ä¿¡æ¯..."
    if ! docker run --rm --entrypoint="" --user root \
        -v "$backup_dir:/backup" \
        -v "$OLLAMA_MODELS_DIR:/ollama" \
        "$FULL_IMAGE_NAME" \
        sh -c "cp -r /backup/manifests/* /ollama/manifests/"; then
        log_error "manifestæ–‡ä»¶å¤åˆ¶å¤±è´¥"
        return 1
    fi
    
    # å¤åˆ¶blobs
    log_verbose "æ¢å¤æ¨¡å‹æ•°æ®..."
    if ! docker run --rm --entrypoint="" --user root \
        -v "$backup_dir:/backup" \
        -v "$OLLAMA_MODELS_DIR:/ollama" \
        "$FULL_IMAGE_NAME" \
        sh -c "cp /backup/blobs/* /ollama/blobs/"; then
        log_error "blobæ–‡ä»¶å¤åˆ¶å¤±è´¥"
        return 1
    fi
    
    log_verbose_success "æ¨¡å‹æ¢å¤å®Œæˆ"
    return 0
}

# è‡ªåŠ¨è¯†åˆ«å¤‡ä»½ç±»å‹å¹¶æ¢å¤
# æ‰¹é‡å¤‡ä»½æ¨¡å‹ï¼ˆæ ¹æ®models.listæ–‡ä»¶ï¼‰
backup_models_from_list() {
    local models_file="$1"
    local backup_dir="$2"
    
    log_verbose "æ‰¹é‡å¤‡ä»½æ¨¡å‹..."
    log_verbose "æ¨¡å‹åˆ—è¡¨æ–‡ä»¶: $models_file"
    log_verbose "å¤‡ä»½ç›®å½•: $backup_dir"
    
    # è§£ææ¨¡å‹åˆ—è¡¨
    local models=()
    parse_models_list "$models_file" models
    
    if [[ ${#models[@]} -eq 0 ]]; then
        log_warning "æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡å‹è¿›è¡Œå¤‡ä»½"
        return 1
    fi
    
    # åˆ›å»ºå¤‡ä»½ç›®å½•
    mkdir -p "$backup_dir"
    
    local total_models=${#models[@]}
    local processed=0
    local success=0
    local failed=0
    
    log_verbose "å…±æ‰¾åˆ° $total_models ä¸ªæ¨¡å‹è¿›è¡Œå¤‡ä»½"
    
    # é¢„å…ˆåˆå§‹åŒ–Ollamaç¼“å­˜ï¼Œé¿å…æ¯ä¸ªæ¨¡å‹éƒ½é‡æ–°åˆå§‹åŒ–
    local has_ollama_models=false
    for model in "${models[@]}"; do
        if [[ "$model" =~ ^ollama: ]] || [[ "$model" =~ ^hf-gguf: ]]; then
            has_ollama_models=true
            break
        fi
    done
    
    if [[ "$has_ollama_models" == "true" ]]; then
        log_verbose "æ£€æµ‹åˆ°Ollamaæ¨¡å‹ï¼Œé¢„å…ˆåˆå§‹åŒ–æ¨¡å‹ç¼“å­˜..."
        if ! init_ollama_cache; then
            log_error "Ollamaç¼“å­˜åˆå§‹åŒ–å¤±è´¥ï¼Œå¯èƒ½å½±å“å¤‡ä»½æ€§èƒ½"
        fi
    fi
    
    for model in "${models[@]}"; do
        ((processed++))
        log_info "å¤‡ä»½æ¨¡å‹ [$processed/$total_models]: $model"
        
        # è§£ææ¨¡å‹æ¡ç›®
        if [[ "$model" =~ ^ollama:([^:]+):(.+)$ ]]; then
            local model_name="${BASH_REMATCH[1]}"
            local model_tag="${BASH_REMATCH[2]}"
            local model_spec="${model_name}:${model_tag}"
            
            
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
            if check_ollama_model "$model_name" "$model_tag"; then
                if backup_ollama_model "$model_spec" "$backup_dir"; then
                    ((success++))
                else
                    ((failed++))
                fi
            else
                ((failed++))
            fi
            
        elif [[ "$model" =~ ^hf-gguf:(.+)$ ]]; then
            local model_full_name="${BASH_REMATCH[1]}"
            
            # è§£æHuggingFace GGUFæ¨¡å‹åç§°
            if [[ "$model_full_name" =~ ^(.+):(.+)$ ]]; then
                local model_name="${BASH_REMATCH[1]}"
                local model_tag="${BASH_REMATCH[2]}"
            else
                local model_name="$model_full_name"
                local model_tag="latest"
            fi
            
            local model_spec="${model_name}:${model_tag}"
            
            # æ£€æŸ¥HF GGUFæ¨¡å‹æ˜¯å¦å­˜åœ¨
            if check_hf_gguf_model "$model_name" "$model_tag"; then
                if backup_ollama_model "$model_spec" "$backup_dir"; then
                    ((success++))
                else
                    ((failed++))
                fi
            else
                ((failed++))
            fi
            
        elif [[ "$model" =~ ^huggingface:([^:]+):(.+)$ ]]; then
            local model_name="${BASH_REMATCH[1]}"
            local quantize_type="${BASH_REMATCH[2]}"
            
            log_verbose "å¤‡ä»½HuggingFaceæ¨¡å‹: $model_name (é‡åŒ–: $quantize_type)"
            
            # æ£€æŸ¥HuggingFaceæ¨¡å‹æ˜¯å¦å­˜åœ¨äºOllamaä¸­
            if check_huggingface_model_in_ollama "$model_name" "$quantize_type"; then
                local ollama_model_name=$(generate_ollama_model_name "$model_name" "$quantize_type")
                if backup_ollama_model "$ollama_model_name" "$backup_dir"; then
                    ((success++))
                    log_verbose_success "HuggingFaceæ¨¡å‹å¤‡ä»½æˆåŠŸ: $model_name"
                else
                    ((failed++))
                    log_error "HuggingFaceæ¨¡å‹å¤‡ä»½å¤±è´¥: $model_name"
                fi
            else
                log_warning "HuggingFaceæ¨¡å‹ä¸å­˜åœ¨ï¼Œè·³è¿‡å¤‡ä»½: $model_name"
                ((failed++))
            fi
            
        else
            log_error "æ— æ•ˆçš„æ¨¡å‹æ¡ç›®æ ¼å¼: $model"
            ((failed++))
        fi
        
        echo "" # æ·»åŠ ç©ºè¡Œåˆ†éš”
    done
    
    # æ˜¾ç¤ºå¤‡ä»½æ€»ç»“
    log_verbose_success "æ‰¹é‡å¤‡ä»½å®Œæˆ ($success/$total_models)"
    if [[ $failed -gt 0 ]]; then
        log_warning "å¤‡ä»½å¤±è´¥: $failed"
        return 1
    fi
    
    # æ˜¾ç¤ºå¤‡ä»½ç›®å½•ä¿¡æ¯
    if [[ "${VERBOSE}" == "true" ]] && [[ -d "$backup_dir" ]]; then
        # åªç»Ÿè®¡é¡¶çº§æ¨¡å‹ç›®å½•ï¼Œæ’é™¤å­ç›®å½•
        local backup_count=$(find "$backup_dir" -maxdepth 1 -type d ! -path "$backup_dir" | wc -l)
        local total_size=$(du -sh "$backup_dir" 2>/dev/null | cut -f1)
        log_info "å¤‡ä»½ç›®å½•ä¸‹å…±æœ‰: $backup_count ä¸ªæ¨¡å‹ï¼Œæ€»å¤§å°: $total_size"
    fi
    
    # æ¸…ç†å®Œæ•´æ€§æ£€æŸ¥ç¼“å­˜
    clear_integrity_cache
    
    if [[ $failed -eq 0 ]]; then
        log_verbose_success "å…¨éƒ¨æ¨¡å‹å¤‡ä»½å®Œæˆ"
        return 0
    else
        log_warning "éƒ¨åˆ†æ¨¡å‹å¤‡ä»½å¤±è´¥"
        return 1
    fi
}

# æ‰¹é‡åˆ é™¤æ¨¡å‹ï¼ˆæ ¹æ®models.listæ–‡ä»¶ï¼‰
remove_models_from_list() {
    local models_file="$1"
    local force_delete="${2:-false}"
    
    log_verbose "æ‰¹é‡åˆ é™¤æ¨¡å‹..."
    log_verbose "æ¨¡å‹åˆ—è¡¨æ–‡ä»¶: $models_file"
    log_info "å¼ºåˆ¶åˆ é™¤æ¨¡å¼: $force_delete"
    
    # è§£ææ¨¡å‹åˆ—è¡¨
    local models=()
    parse_models_list "$models_file" models
    
    if [[ ${#models[@]} -eq 0 ]]; then
        log_warning "æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡å‹è¿›è¡Œåˆ é™¤"
        return 1
    fi
    
    local total_models=${#models[@]}
    local processed=0
    local success=0
    local failed=0
    
    log_verbose "å…±æ‰¾åˆ° $total_models ä¸ªæ¨¡å‹è¿›è¡Œåˆ é™¤"
    
    # å¦‚æœä¸æ˜¯å¼ºåˆ¶åˆ é™¤ï¼Œæ˜¾ç¤ºè¦åˆ é™¤çš„æ¨¡å‹åˆ—è¡¨å¹¶è¯·æ±‚ç¡®è®¤
    if [[ "$force_delete" != "true" ]]; then
        log_warning "å³å°†åˆ é™¤ä»¥ä¸‹æ¨¡å‹ï¼š"
        for model in "${models[@]}"; do
            if [[ "$model" =~ ^ollama:([^:]+):(.+)$ ]]; then
                local model_name="${BASH_REMATCH[1]}"
                local model_tag="${BASH_REMATCH[2]}"
                echo "  - Ollamaæ¨¡å‹: ${model_name}:${model_tag}"
            elif [[ "$model" =~ ^huggingface:([^:]+):(.+)$ ]]; then
                local model_name="${BASH_REMATCH[1]}"
                local quantize_type="${BASH_REMATCH[2]}"
                echo "  - GGUFæ¨¡å‹: $model_name ($quantize_type)"
            elif [[ "$model" =~ ^hf-gguf:(.+)$ ]]; then
                local model_full_name="${BASH_REMATCH[1]}"
                echo "  - HuggingFace GGUFæ¨¡å‹: $model_full_name"
            fi
        done
        echo ""
        echo -n "ç¡®è®¤åˆ é™¤æ‰€æœ‰è¿™äº›æ¨¡å‹ï¼Ÿ[y/N]: "
        read -r confirm
        if [[ "$confirm" != "y" && "$confirm" != "Y" ]]; then
            log_info "å–æ¶ˆæ‰¹é‡åˆ é™¤æ“ä½œ"
            return 2  # ç‰¹æ®Šé€€å‡ºç è¡¨ç¤ºç”¨æˆ·å–æ¶ˆ
        fi
        echo ""
    fi
    
    for model in "${models[@]}"; do
        ((processed++))
        log_info "åˆ é™¤æ¨¡å‹ [$processed/$total_models]: $model"
        
        # è§£ææ¨¡å‹æ¡ç›®
        if [[ "$model" =~ ^ollama:([^:]+):(.+)$ ]]; then
            local model_name="${BASH_REMATCH[1]}"
            local model_tag="${BASH_REMATCH[2]}"
            local model_spec="${model_name}:${model_tag}"
            
            log_verbose "åˆ é™¤Ollamaæ¨¡å‹: $model_spec"
            
            if remove_ollama_model "$model_spec" "true"; then
                ((success++))
                log_verbose_success "Ollamaæ¨¡å‹åˆ é™¤æˆåŠŸ: $model_spec"
            else
                ((failed++))
                log_error "Ollamaæ¨¡å‹åˆ é™¤å¤±è´¥: $model_spec"
            fi
            
        elif [[ "$model" =~ ^huggingface:([^:]+):(.+)$ ]]; then
            local model_name="${BASH_REMATCH[1]}"
            local quantize_type="${BASH_REMATCH[2]}"
            
            log_verbose "åˆ é™¤HuggingFace GGUFæ¨¡å‹: $model_name ($quantize_type)"
            
            # ç”Ÿæˆå¯¹åº”çš„Ollamaæ¨¡å‹åç§°
            local ollama_model_name=$(generate_ollama_model_name "$model_name" "$quantize_type")
            if remove_ollama_model "$ollama_model_name" "true"; then
                ((success++))
                log_verbose_success "GGUFæ¨¡å‹åˆ é™¤æˆåŠŸ: $model_name ($quantize_type)"
            else
                ((failed++))
                log_error "GGUFæ¨¡å‹åˆ é™¤å¤±è´¥: $model_name ($quantize_type)"
            fi
            
        elif [[ "$model" =~ ^hf-gguf:(.+)$ ]]; then
            local model_full_name="${BASH_REMATCH[1]}"
            
            # è§£æHuggingFace GGUFæ¨¡å‹åç§°
            if [[ "$model_full_name" =~ ^(.+):(.+)$ ]]; then
                local model_name="${BASH_REMATCH[1]}"
                local model_tag="${BASH_REMATCH[2]}"
            else
                local model_name="$model_full_name"
                local model_tag="latest"
            fi
            
            local model_spec="${model_name}:${model_tag}"
            log_verbose "åˆ é™¤HuggingFace GGUFæ¨¡å‹: $model_spec"
            
            if remove_ollama_model "$model_spec" "true"; then
                ((success++))
                log_verbose_success "HuggingFace GGUFæ¨¡å‹åˆ é™¤æˆåŠŸ: $model_spec"
            else
                ((failed++))
                log_error "HuggingFace GGUFæ¨¡å‹åˆ é™¤å¤±è´¥: $model_spec"
            fi
            
        else
            log_error "æ— æ•ˆçš„æ¨¡å‹æ¡ç›®æ ¼å¼: $model"
            ((failed++))
        fi
        
        echo "" # æ·»åŠ ç©ºè¡Œåˆ†éš”
    done
    
    # æ˜¾ç¤ºåˆ é™¤æ€»ç»“  
    log_verbose_success "æ‰¹é‡åˆ é™¤å®Œæˆ ($success/$total_models)"
    if [[ $failed -gt 0 ]]; then
        log_warning "åˆ é™¤å¤±è´¥: $failed"
    fi
    
    if [[ $failed -eq 0 ]]; then
        log_verbose_success "å…¨éƒ¨æ¨¡å‹åˆ é™¤å®Œæˆ"
        return 0
    else
        log_warning "éƒ¨åˆ†æ¨¡å‹åˆ é™¤å¤±è´¥"
        return 1
    fi
}

# æ£€æŸ¥Ollamaä¸­æ˜¯å¦å­˜åœ¨æŒ‡å®šæ¨¡å‹
# æ£€æŸ¥HuggingFaceæ¨¡å‹æ˜¯å¦å·²å­˜åœ¨äºOllamaä¸­ï¼ˆæ™ºèƒ½åŒ¹é…ï¼‰
check_huggingface_model_in_ollama() {
    local model_name="$1"
    local quantize_type="$2"
    
    log_verbose "æ£€æŸ¥HuggingFaceæ¨¡å‹: $model_name ($quantize_type)"
    
    # ç”ŸæˆæœŸæœ›çš„Ollamaæ¨¡å‹åç§°ï¼ˆå¸¦hf-å‰ç¼€ï¼‰
    local expected_ollama_name=$(generate_ollama_model_name "$model_name" "$quantize_type")
    
    # ä½¿ç”¨ç®€åŒ–çš„å®¹å™¨æ£€æŸ¥
    if check_ollama_model_exists "$expected_ollama_name"; then
        log_verbose_success "æ‰¾åˆ°åŒ¹é…çš„Ollamaæ¨¡å‹: $expected_ollama_name"
        return 0
    fi
    
    log_verbose_warning "æœªæ‰¾åˆ°åŒ¹é…çš„Ollamaæ¨¡å‹: $expected_ollama_name"
    return 1
}

# ä»HuggingFaceåŸå§‹å¤‡ä»½æ¢å¤å¹¶é‡æ–°è½¬æ¢
restore_and_reconvert_hf_model() {
    local model_name="$1"
    local quantize_type="$2"
    local skip_md5_check="${3:-false}"  # æ–°å¢å‚æ•°ï¼Œé»˜è®¤ä¸ºfalse
    
    log_info "ä»åŸå§‹å¤‡ä»½æ¢å¤å¹¶é‡æ–°è½¬æ¢: $model_name ($quantize_type)"
    
    # ç”Ÿæˆæ–‡ä»¶ç³»ç»Ÿå®‰å…¨çš„æ¨¡å‹åç§°
    local model_safe_name=$(get_safe_model_name "$model_name" "filesystem")
    local model_backup_dir="${ABS_HF_ORIGINAL_BACKUP_DIR}/${model_safe_name}"
    local backup_dir="${model_backup_dir}/${model_safe_name}_original"
    
    # æ£€æŸ¥å¤‡ä»½ç›®å½•
    if [[ ! -d "$backup_dir" ]]; then
        log_verbose_warning "æœªæ‰¾åˆ°å¤‡ä»½ç›®å½•: $backup_dir"
        return 1
    fi
    
    # MD5æ ¡éªŒï¼ˆå¦‚æœæ²¡æœ‰è·³è¿‡çš„è¯ï¼‰
    if [[ "$skip_md5_check" != "true" ]]; then
        local md5_file="${backup_dir}.md5"
        if [[ -f "$md5_file" ]]; then
            log_info "æ­£åœ¨éªŒè¯MD5æ ¡éªŒå€¼..."
            if verify_directory_md5 "$backup_dir" "$md5_file"; then
                log_verbose_success "MD5æ ¡éªŒé€šè¿‡"
            else
                log_error "MD5æ ¡éªŒå¤±è´¥ï¼Œå¤‡ä»½å¯èƒ½å·²æŸå"
                return 1
            fi
        else
            log_warning "æœªæ‰¾åˆ°MD5æ ¡éªŒæ–‡ä»¶ï¼Œè·³è¿‡æ ¡éªŒ"
        fi
    fi
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•è¿›è¡Œæ¢å¤
    local restore_temp_dir=$(mktemp -d) || { log_error "æ— æ³•åˆ›å»ºä¸´æ—¶ç›®å½•"; return 1; }
    
    cleanup_restore_temp() { [[ -d "${restore_temp_dir:-}" ]] && rm -rf "$restore_temp_dir"; }
    add_cleanup_function "cleanup_restore_temp"
    
    # ç›´æ¥å¤åˆ¶å¤‡ä»½ç›®å½•åˆ°ä¸´æ—¶ç›®å½•
    log_info "æ¢å¤æ¨¡å‹æ–‡ä»¶..."
    local restored_model_dir="$restore_temp_dir/restored_model"
    if ! cp -r "$backup_dir" "$restored_model_dir"; then
        log_error "å¤‡ä»½æ¢å¤å¤±è´¥"
        cleanup_restore_temp
        remove_cleanup_function "cleanup_restore_temp"
        return 1
    fi
    
    # å°†æ¢å¤çš„æ¨¡å‹å¤åˆ¶åˆ°ç¼“å­˜ç›®å½•ä¾›è½¬æ¢è„šæœ¬ä½¿ç”¨
    local cache_model_dir="${ABS_HF_DOWNLOAD_CACHE_DIR}/${model_safe_name}"
    
    # æ¸…ç†æ—§ç¼“å­˜å¹¶å¤åˆ¶æ¢å¤çš„æ¨¡å‹
    [[ -d "$cache_model_dir" ]] && rm -rf "$cache_model_dir"
    if ! cp -r "$restored_model_dir" "$cache_model_dir"; then
        log_error "æ¨¡å‹å¤åˆ¶å¤±è´¥"
        cleanup_restore_temp
        remove_cleanup_function "cleanup_restore_temp"
        return 1
    fi
    
    # æ„å»ºå¹¶æ‰§è¡Œè½¬æ¢å‘½ä»¤ï¼Œç›´æ¥ä½¿ç”¨restore_temp_dirä½œä¸ºè¾“å‡ºç›®å½•
    local container_name="llm-reconvert-$$"
    local docker_cmd=()
    mapfile -t docker_cmd < <(build_full_docker_cmd "$container_name" "true" "false" \
        --volume "${restore_temp_dir}:/app/models" \
        --volume "${ABS_HF_DOWNLOAD_CACHE_DIR}:/app/download_cache")
    
    [[ ${#docker_cmd[@]} -eq 0 ]] && { log_error "Dockerå‘½ä»¤æ„å»ºå¤±è´¥"; return 1; }
    
    docker_cmd+=("${FULL_IMAGE_NAME}" "${model_name}" "--quantize" "${quantize_type}" "--gguf-dir" "/app/models")
    [[ "${VERBOSE}" == "true" ]] && docker_cmd+=("--verbose")
    
    # æ‰§è¡Œè½¬æ¢
    local conversion_result=0
    log_info "å¼€å§‹é‡æ–°è½¬æ¢æ¨¡å‹..."
    
    if "${docker_cmd[@]}" >/dev/null 2>&1; then
        # å¯¼å…¥åˆ°Ollamaï¼Œä½¿ç”¨restore_temp_diræŸ¥æ‰¾GGUFæ–‡ä»¶
        if import_gguf_to_ollama_from_temp "$model_name" "$quantize_type" "$restore_temp_dir"; then
            log_success "æ¨¡å‹æ¢å¤ã€è½¬æ¢å¹¶å¯¼å…¥å®Œæˆ: $model_name"
            conversion_result=0
        else
            log_error "è½¬æ¢æˆåŠŸä½†å¯¼å…¥Ollamaå¤±è´¥"
            conversion_result=1
        fi
    else
        log_error "æ¨¡å‹è½¬æ¢å¤±è´¥: $model_name"
        conversion_result=1
    fi
    
    # æ¸…ç†ç¼“å­˜
    [[ -d "$cache_model_dir" ]] && rm -rf "$cache_model_dir" 2>/dev/null
    cleanup_restore_temp
    remove_cleanup_function "cleanup_restore_temp"
    
    return $conversion_result
}

# æ£€æŸ¥Ollamaä¸­æ˜¯å¦å­˜åœ¨æŒ‡å®šæ¨¡å‹ï¼ˆé€šç”¨å‡½æ•°ï¼‰


# ä»ä¸´æ—¶ç›®å½•å¯¼å…¥GGUFæ¨¡å‹åˆ°Ollama
import_gguf_to_ollama_from_temp() {
    local model_name="$1"
    local quantize_type="$2"
    local temp_dir="$3"
    
    log_verbose "å¼€å§‹ä»ä¸´æ—¶ç›®å½•å¯¼å…¥GGUFæ¨¡å‹åˆ°Ollama: $model_name ($quantize_type)"
    
    # æŸ¥æ‰¾ä¸´æ—¶ç›®å½•ä¸­çš„GGUFæ–‡ä»¶
    local gguf_file=$(find "$temp_dir" -name "*.gguf" -type f | head -n1)
    if [[ ! -f "$gguf_file" ]]; then
        log_error "åœ¨ä¸´æ—¶ç›®å½•ä¸­æœªæ‰¾åˆ°GGUFæ–‡ä»¶: $temp_dir"
        return 1
    fi
    
    log_info "æ‰¾åˆ°GGUFæ–‡ä»¶: $gguf_file"
    
    # ç”ŸæˆOllamaæ¨¡å‹åç§°ï¼ˆå¸¦hf-å‰ç¼€ï¼‰
    local ollama_model_name=$(generate_ollama_model_name "$model_name" "$quantize_type")
    log_verbose "Ollamaæ¨¡å‹åç§°: $ollama_model_name"
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨äºOllamaä¸­
    if check_ollama_model_exists "$ollama_model_name"; then
        log_success "æ¨¡å‹å·²å­˜åœ¨äºOllamaä¸­ï¼Œè·³è¿‡å¯¼å…¥: $ollama_model_name"
        return 0
    fi
    
    # åˆ›å»ºä¸´æ—¶Modelfile
    local temp_modelfile
    temp_modelfile=$(mktemp) || {
        log_error "æ— æ³•åˆ›å»ºä¸´æ—¶Modelfile"
        return 1
    }
    cat > "$temp_modelfile" << EOF
FROM ${gguf_file}
TEMPLATE """{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
"""
PARAMETER stop "<|im_end|>"
PARAMETER temperature 0.7
PARAMETER top_p 0.9
EOF
    
    log_verbose "åˆ›å»ºModelfile: $temp_modelfile"
    
    # ä½¿ç”¨ä¸´æ—¶å®¹å™¨å¯¼å…¥GGUFæ¨¡å‹
    log_verbose "å¯åŠ¨ä¸´æ—¶å®¹å™¨å¯¼å…¥GGUFæ¨¡å‹"
    local import_name="ollama-import-$$"
    
    # å®šä¹‰æ¸…ç†å‡½æ•°
    cleanup_import_container() {
        if docker ps -a --format "{{.Names}}" | grep -q "^${import_name}$"; then
            docker rm -f "$import_name" > /dev/null 2>&1
        fi
        rm -f "$temp_modelfile"
    }
    
    
    # è®¾ç½®ä¿¡å·å¤„ç†
    add_cleanup_function "cleanup_import_container"
    
    # è·å–ç»å¯¹è·¯å¾„
    local abs_ollama_dir
    # å¯¹äºOllamaå®¹å™¨ï¼Œéœ€è¦æŒ‚è½½çš„æ˜¯.ollamaç›®å½•ï¼ˆå³dataç›®å½•ï¼‰ï¼Œè€Œä¸æ˜¯data/models
    abs_ollama_dir="$ABS_OLLAMA_DATA_DIR"
    
    # å¯åŠ¨ä¸´æ—¶å®¹å™¨
    local import_cmd=("docker" "run" "-d" "--name" "$import_name")
    
    # æ·»åŠ GPUé…ç½®
    import_cmd+=("--gpus" "all")
    
    # æ·»åŠ å·æŒ‚è½½
    import_cmd+=("-v" "${abs_ollama_dir}:/root/.ollama")
    import_cmd+=("-p" "11434:11434")  # ä½¿ç”¨å›ºå®šç«¯å£æ˜ å°„
    import_cmd+=("$DOCKER_IMAGE_OLLAMA")
    
    if ! "${import_cmd[@]}"; then
        log_error "æ— æ³•å¯åŠ¨ä¸´æ—¶å¯¼å…¥å®¹å™¨"
        rm -f "$temp_modelfile"
        return 1
    fi
    
    # ç­‰å¾…æœåŠ¡å°±ç»ª
    local max_attempts=30
    local attempt=0
    
    while (( attempt < max_attempts )); do
        if docker exec "$import_name" ollama list > /dev/null 2>&1; then
            break
        fi
        sleep 2
        ((attempt++))
    done
    
    if (( attempt >= max_attempts )); then
        log_error "ç­‰å¾…å¯¼å…¥å®¹å™¨æœåŠ¡è¶…æ—¶"
        docker rm -f "$import_name" > /dev/null 2>&1
        rm -f "$temp_modelfile"
        return 1
    fi
    
    # å°†GGUFæ–‡ä»¶å’ŒModelfileå¤åˆ¶åˆ°å®¹å™¨ä¸­
    local container_gguf_path="/tmp/$(basename "$gguf_file")"
    local container_modelfile="/tmp/Modelfile-$$"
    
    if ! docker cp "$gguf_file" "$import_name:$container_gguf_path"; then
        log_error "æ— æ³•å°†GGUFæ–‡ä»¶å¤åˆ¶åˆ°å®¹å™¨"
        docker rm -f "$import_name" > /dev/null 2>&1
        rm -f "$temp_modelfile"
        return 1
    fi
    
    # æ›´æ–°Modelfileä¸­çš„è·¯å¾„ä¸ºå®¹å™¨å†…è·¯å¾„
    sed -i "s|FROM .*|FROM $container_gguf_path|" "$temp_modelfile"
    
    if ! docker cp "$temp_modelfile" "$import_name:$container_modelfile"; then
        log_error "æ— æ³•å°†Modelfileå¤åˆ¶åˆ°å®¹å™¨"
        docker rm -f "$import_name" > /dev/null 2>&1
        rm -f "$temp_modelfile"
        return 1
    fi
    
    # åœ¨å®¹å™¨ä¸­æ‰§è¡Œollama createå‘½ä»¤
    log_verbose "æ‰§è¡Œå‘½ä»¤: docker exec $import_name ollama create $ollama_model_name -f $container_modelfile"
    local result=1
    if docker exec "$import_name" ollama create "$ollama_model_name" -f "$container_modelfile"; then
        log_success "GGUFæ¨¡å‹å·²å¯¼å…¥Ollama: $ollama_model_name"
        result=0
    else
        log_error "GGUFæ¨¡å‹å¯¼å…¥å¤±è´¥: $ollama_model_name"
    fi
    
    # æ¸…ç†å®¹å™¨å’Œä¸´æ—¶æ–‡ä»¶
    cleanup_import_container
    remove_cleanup_function "cleanup_import_container"
    
    return $result
}

# ä¸‹è½½å¹¶è½¬æ¢HuggingFaceæ¨¡å‹
download_huggingface_model() {
    local model_name="$1"
    local quantize_type="$2"
    
    log_info "å¼€å§‹ä¸‹è½½å¹¶è½¬æ¢HuggingFaceæ¨¡å‹: $model_name (é‡åŒ–: $quantize_type)"
    
    # æ£€æµ‹æœ€ä¼˜HuggingFaceç«¯ç‚¹
    detect_optimal_hf_endpoint
    
    # å¦‚æœåŸå§‹å¤‡ä»½æ¢å¤å¤±è´¥ï¼Œè¿›è¡Œæ­£å¸¸çš„ä¸‹è½½æµç¨‹
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºå­˜å‚¨GGUFæ–‡ä»¶
    local temp_dir
    temp_dir=$(mktemp -d) || {
        log_error "æ— æ³•åˆ›å»ºä¸´æ—¶ç›®å½•"
        return 1
    }
    
    # å®šä¹‰æ¸…ç†å‡½æ•°
    cleanup_temp_dir() {
        if [[ -d "${temp_dir:-}" ]]; then
            log_verbose "æ¸…ç†ä¸´æ—¶ç›®å½•: $temp_dir"
            docker_rm_rf "$temp_dir"
        fi
    }
    
    # å®šä¹‰å®¹å™¨æ¸…ç†å‡½æ•°
    cleanup_converter_container() {
        local container_name="llm-converter-$$"
        if docker ps -a --format "{{.Names}}" | grep -q "^${container_name}$"; then
            log_warning "æ£€æµ‹åˆ°ä¸­æ–­ï¼Œæ­£åœ¨åœæ­¢å¹¶æ¸…ç†è½¬æ¢å®¹å™¨: $container_name"
            docker stop "$container_name" > /dev/null 2>&1
            docker rm -f "$container_name" > /dev/null 2>&1
        fi
        cleanup_temp_dir
    }
    
    # è®¾ç½®ä¿¡å·å¤„ç†ï¼Œç¡®ä¿å®¹å™¨è¢«æ­£ç¡®æ¸…ç†
    add_cleanup_function "cleanup_converter_container"
    
    # æ„å»ºdocker runå‘½ä»¤ï¼Œä½¿ç”¨æŒ‡å®šçš„å®¹å™¨å
    local container_name="llm-converter-$$"
    mapfile -t docker_cmd < <(build_full_docker_cmd "$container_name" "true" "true" \
        --volume "$temp_dir:/app/models" \
        --volume "${ABS_HF_DOWNLOAD_CACHE_DIR}:/app/download_cache")
    
    
    
    # é•œåƒå’Œå‚æ•°
    docker_cmd+=("${FULL_IMAGE_NAME}")
    docker_cmd+=("${model_name}")
    docker_cmd+=("--quantize" "${quantize_type}")
    docker_cmd+=("--gguf-dir" "/app/models")
    
    # æ·»åŠ verboseå‚æ•°æ”¯æŒ
    if [[ "${VERBOSE}" == "true" ]]; then
        docker_cmd+=("--verbose")
    fi
    
    # æ‰§è¡Œè½¬æ¢å‘½ä»¤ï¼Œä½¿ç”¨å®æ—¶è¾“å‡º
    local conversion_result=0
    log_info "æ­£åœ¨ä¸‹è½½å’Œè½¬æ¢æ¨¡å‹..."
    echo "----------------------------------------"
    
    # ä½¿ç”¨ unbuffer æˆ–è€…ç›´æ¥ç®¡é“è¾“å‡ºæ¥ç¡®ä¿å®æ—¶æ˜¾ç¤º
    if "${docker_cmd[@]}" 2>&1 | while IFS= read -r line; do
        echo "[HF-DOCKER] $line"
    done; then
        echo "----------------------------------------"
        log_success "HuggingFaceæ¨¡å‹ä¸‹è½½å¹¶è½¬æ¢å®Œæˆ: $model_name"
        
        # è‡ªåŠ¨å¯¼å…¥åˆ°Ollama
        log_info "å¼€å§‹å¯¼å…¥GGUFæ¨¡å‹åˆ°Ollama..."
        if import_gguf_to_ollama_from_temp "$model_name" "$quantize_type" "$temp_dir"; then
            log_success "æ¨¡å‹å·²æˆåŠŸå¯¼å…¥åˆ°Ollama: $ollama_model_name"
            
            # éªŒè¯å¯¼å…¥åçš„æ¨¡å‹å®Œæ•´æ€§
            local final_model_name="${ollama_model_name%:*}"
            local final_model_tag="${ollama_model_name#*:}"
            if verify_model_after_installation "$final_model_name" "$final_model_tag"; then
                log_verbose_success "æ¨¡å‹å®Œæ•´æ€§éªŒè¯é€šè¿‡: $ollama_model_name"
            else
                log_error "æ¨¡å‹å®Œæ•´æ€§éªŒè¯å¤±è´¥ï¼Œæ¨¡å‹å·²è¢«æ¸…ç†: $ollama_model_name"
            fi
            
            # æ–°æµç¨‹ï¼šåœ¨å¯¼å…¥æˆåŠŸåè¿›è¡Œå¤‡ä»½å’Œæ¸…ç†
            # æ­¥éª¤1: åˆ›å»ºåŸå§‹æ¨¡å‹å¤‡ä»½
            log_info "åˆ›å»ºåŸå§‹æ¨¡å‹å¤‡ä»½..."
            local model_safe_name=$(get_safe_model_name "$model_name" "filesystem")
            local cache_dir="${ABS_HF_DOWNLOAD_CACHE_DIR}/${model_safe_name}"
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç¼“å­˜ç›®å½•
            if [[ -d "$cache_dir" ]]; then
                if backup_hf_original_model "$model_name" "$cache_dir"; then
                    log_verbose_success "åŸå§‹æ¨¡å‹å¤‡ä»½åˆ›å»ºæˆåŠŸ"
                    
                    # æ­¥éª¤2: åˆ é™¤å·²å¤‡ä»½çš„åŸå§‹æ¨¡å‹ç¼“å­˜
                    log_info "åˆ é™¤å·²å¤‡ä»½çš„åŸå§‹æ¨¡å‹ç¼“å­˜..."
                    if docker_rm_rf "$cache_dir"; then
                        log_verbose_success "åŸå§‹æ¨¡å‹ç¼“å­˜å·²æ¸…ç†: $cache_dir"
                    else
                        log_warning "æ¸…ç†åŸå§‹æ¨¡å‹ç¼“å­˜å¤±è´¥ï¼Œä½†ä¸å½±å“ä¸»è¦åŠŸèƒ½"
                    fi
                else
                    log_warning "åŸå§‹æ¨¡å‹å¤‡ä»½åˆ›å»ºå¤±è´¥ï¼Œä¿ç•™ç¼“å­˜ç›®å½•"
                fi
            else
                log_info "æœªæ‰¾åˆ°ç¼“å­˜ç›®å½•ï¼Œè·³è¿‡å¤‡ä»½å’Œæ¸…ç†"
            fi
        else
            log_warning "GGUFä¸‹è½½è½¬æ¢æˆåŠŸï¼Œä½†å¯¼å…¥Ollamaå¤±è´¥"
            conversion_result=1
        fi
    else
        echo "----------------------------------------"
        log_error "HuggingFaceæ¨¡å‹ä¸‹è½½è½¬æ¢å¤±è´¥: $model_name"
        conversion_result=1
    fi
    
    # æ‰‹åŠ¨æ¸…ç†å¹¶ç§»é™¤æ¸…ç†å‡½æ•°
    cleanup_converter_container
    remove_cleanup_function "cleanup_converter_container"
    
    return $conversion_result
}

# æ£€æŸ¥Ollamaæ¨¡å‹åœ¨backupsç›®å½•ä¸­æ˜¯å¦æœ‰å¤‡ä»½
check_ollama_backup_exists() {
    local model_name="$1"
    local model_tag="$2"
    
    # ä½¿ç”¨ä¸get_safe_model_nameç›¸åŒçš„é€»è¾‘ç”Ÿæˆå®‰å…¨åç§°
    local model_spec="${model_name}:${model_tag}"
    local model_safe_name=$(get_safe_model_name "$model_spec")
    local backup_parent_dir="$BACKUP_OUTPUT_DIR/${model_safe_name}"
    local backup_model_dir="$backup_parent_dir/${model_safe_name}"
    
    # æ£€æŸ¥å¤‡ä»½ç›®å½•æ˜¯å¦å­˜åœ¨
    if [[ -d "$backup_model_dir" ]]; then
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„ç›®å½•å¤‡ä»½ç»“æ„
        if [[ -d "$backup_model_dir/manifests" ]] && [[ -d "$backup_model_dir/blobs" ]]; then
            echo "$backup_parent_dir"
            return 0
        fi
    fi
    
    return 1
}

# æ£€æŸ¥HuggingFaceæ¨¡å‹åœ¨hf_originalsç›®å½•ä¸­æ˜¯å¦æœ‰å¤‡ä»½
check_hf_original_backup_exists() {
    local model_name="$1"
    
    # ä½¿ç”¨ç»Ÿä¸€çš„æ–‡ä»¶ç³»ç»Ÿå®‰å…¨å‘½å
    local model_safe_name=$(get_safe_model_name "$model_name" "filesystem")
    local backup_dir="$ABS_HF_ORIGINAL_BACKUP_DIR/${model_safe_name}"
    local backup_source_dir="$backup_dir/${model_safe_name}_original"
    
    # æ£€æŸ¥å¤‡ä»½ç›®å½•æ˜¯å¦å­˜åœ¨
    if [[ -d "$backup_dir" ]]; then
        # æ£€æŸ¥æ˜¯å¦æœ‰åŸå§‹å¤‡ä»½ç›®å½•
        if [[ -d "$backup_source_dir" ]]; then
            echo "$backup_dir"
            return 0
        fi
    fi
    
    return 1
}

# å°è¯•ä»å¤‡ä»½æ¢å¤Ollamaæ¨¡å‹
try_restore_ollama_from_backup() {
    local model_name="$1"
    local model_tag="$2"
    
    log_verbose "æ£€æŸ¥Ollamaæ¨¡å‹å¤‡ä»½: ${model_name}:${model_tag}"
    
    local backup_dir
    if backup_dir=$(check_ollama_backup_exists "$model_name" "$model_tag"); then
        log_verbose_success "æ‰¾åˆ°Ollamaæ¨¡å‹å¤‡ä»½: $backup_dir"
        
        # ä½¿ç”¨ä¸get_safe_model_nameç›¸åŒçš„é€»è¾‘ç”Ÿæˆå®‰å…¨åç§°
        local model_spec="${model_name}:${model_tag}"
        local model_safe_name=$(get_safe_model_name "$model_spec")
        
        # æŸ¥æ‰¾å¤‡ä»½ç›®å½•ï¼ˆæ–°çš„ç›´æ¥å¤åˆ¶æ ¼å¼ï¼‰
        local backup_model_dir="$backup_dir/$model_safe_name"
        if [[ -d "$backup_model_dir" ]]; then
            # æ¢å¤æ¨¡å‹
            log_info "æ­£åœ¨ä»å¤‡ä»½æ¢å¤æ¨¡å‹..."
            if restore_ollama_model "$backup_model_dir" "true"; then
                log_success "ä»å¤‡ä»½æˆåŠŸæ¢å¤æ¨¡å‹: ${model_name}:${model_tag}"
                return 0
            else
                log_warning "ä»å¤‡ä»½æ¢å¤æ¨¡å‹å¤±è´¥ï¼Œå°†å°è¯•é‡æ–°ä¸‹è½½"
                return 1
            fi
        else
            log_error "æœªæ‰¾åˆ°æœ‰æ•ˆçš„å¤‡ä»½ç›®å½•: $backup_model_dir"
            return 1
        fi
    else
        log_verbose "æœªæ‰¾åˆ°Ollamaæ¨¡å‹å¤‡ä»½"
        return 1
    fi
}

# å°è¯•ä»HuggingFaceåŸå§‹å¤‡ä»½æ¢å¤æ¨¡å‹
try_restore_hf_from_original() {
    local model_name="$1"
    
    log_verbose "æ£€æŸ¥HuggingFaceåŸå§‹æ¨¡å‹å¤‡ä»½: $model_name"
    
    local backup_dir
    if backup_dir=$(check_hf_original_backup_exists "$model_name"); then
        log_verbose_success "æ‰¾åˆ°HuggingFaceåŸå§‹æ¨¡å‹å¤‡ä»½: $backup_dir"
        
        # ä½¿ç”¨ç»Ÿä¸€çš„æ–‡ä»¶ç³»ç»Ÿå®‰å…¨å‘½å
        local model_safe_name=$(get_safe_model_name "$model_name" "filesystem")
        
        # æŸ¥æ‰¾åŸå§‹å¤‡ä»½ç›®å½•ï¼ˆæ–°çš„ç›´æ¥å¤åˆ¶æ ¼å¼ï¼‰
        local backup_source_dir="$backup_dir/${model_safe_name}_original"
        if [[ -d "$backup_source_dir" ]]; then
            # æ¢å¤åˆ°ç¼“å­˜ç›®å½•
            local cache_dir="$ABS_HF_DOWNLOAD_CACHE_DIR/$model_safe_name"
            log_info "æ­£åœ¨æ¢å¤HuggingFaceåŸå§‹æ¨¡å‹åˆ°ç¼“å­˜ç›®å½•..."
            
            # MD5æ ¡éªŒ
            local md5_file="${backup_source_dir}.md5"
            if [[ -f "$md5_file" ]]; then
                log_info "æ­£åœ¨éªŒè¯MD5æ ¡éªŒå€¼..."
                if verify_directory_md5 "$backup_source_dir" "$md5_file"; then
                    log_verbose_success "MD5æ ¡éªŒé€šè¿‡"
                else
                    log_error "MD5æ ¡éªŒå¤±è´¥ï¼Œå¤‡ä»½å¯èƒ½å·²æŸå"
                    return 1
                fi
            else
                log_warning "æœªæ‰¾åˆ°MD5æ ¡éªŒæ–‡ä»¶ï¼Œè·³è¿‡æ ¡éªŒ"
            fi
            
            # åˆ›å»ºç¼“å­˜ç›®å½•
            mkdir -p "$(dirname "$cache_dir")"
            
            # ç›´æ¥å¤åˆ¶å¤‡ä»½ç›®å½•åˆ°ç¼“å­˜ç›®å½•
            if cp -r "$backup_source_dir" "$cache_dir"; then
                log_success "ä»åŸå§‹å¤‡ä»½æˆåŠŸæ¢å¤æ¨¡å‹åˆ°ç¼“å­˜: $model_name"
                return 0
            else
                log_warning "ä»åŸå§‹å¤‡ä»½æ¢å¤å¤±è´¥"
                return 1
            fi
        else
            log_error "æœªæ‰¾åˆ°æœ‰æ•ˆçš„åŸå§‹å¤‡ä»½ç›®å½•: $backup_source_dir"
            return 1
        fi
    else
        log_verbose "æœªæ‰¾åˆ°HuggingFaceåŸå§‹æ¨¡å‹å¤‡ä»½"
        return 1
    fi
}

# å¤„ç†å•ä¸ªæ¨¡å‹
process_model() {
    local model_entry="$1"
    local force_download="$2"
    local check_only="$3"
    
    # è§£ææ¨¡å‹æ¡ç›®
    local -A model_info
    if ! parse_model_entry "$model_entry" model_info; then
        log_error "æ— æ•ˆçš„æ¨¡å‹æ¡ç›®æ ¼å¼: $model_entry"
        return 1
    fi
    
    log_verbose "å¤„ç†æ¨¡å‹: ${model_info[display]}"
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if [[ "$force_download" != "true" ]] && check_model_exists model_info; then
        log_success "æ¨¡å‹å·²å­˜åœ¨"
        return 0
    fi
    
    # æ¨¡å‹ä¸å­˜åœ¨æˆ–å¼ºåˆ¶ä¸‹è½½
    if [[ "$check_only" == "true" ]]; then
        log_warning "éœ€è¦ä¸‹è½½: ${model_info[display]}"
        return 0
    fi
    
    # å°è¯•ä»å¤‡ä»½æ¢å¤
    if try_restore_model model_info; then
        log_success "ä»å¤‡ä»½æ¢å¤æˆåŠŸ"
        # æ¸…é™¤ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°æ£€æŸ¥
        OLLAMA_CACHE_INITIALIZED="false"
        OLLAMA_MODELS_CACHE=""
        return 0
    fi
    
    # æ‰§è¡Œä¸‹è½½
    if download_model model_info; then
        log_success "æ¨¡å‹ä¸‹è½½å®Œæˆ"
        # æ¸…é™¤ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°æ£€æŸ¥
        OLLAMA_CACHE_INITIALIZED="false"
        OLLAMA_MODELS_CACHE=""
        return 0
    else
        log_error "æ¨¡å‹å¤„ç†å¤±è´¥: ${model_info[display]}"
        return 1
    fi
}

# ä¸»å‡½æ•°
main() {
    # è·å–ä¸»æœºæ—¶åŒº
    HOST_TIMEZONE=$(get_host_timezone)
    
    # æ£€æŸ¥å‚æ•° - æ”¯æŒhelpåœ¨ä»»ä½•ä½ç½®
    for arg in "$@"; do
        if [[ "$arg" = "--help" || "$arg" = "-h" ]]; then
            show_help
            exit 0
        fi
    done
    
    # é»˜è®¤å€¼
    MODELS_FILE="$MODELS_LIST_FILE"
    CHECK_ONLY="true"
    FORCE_DOWNLOAD="false"
    REBUILD="false"
    HF_TOKEN=""
    BACKUP_MODEL=""
    BACKUP_ALL="false"
    LIST_MODELS="false"
    RESTORE_FILE=""
    GENERATE_COMPOSE="false"
    FORCE_RESTORE="false"
    REMOVE_MODEL=""
    REMOVE_ALL="false"
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    while [[ $# -gt 0 ]]; do
        case $1 in
            --models-file)
                MODELS_FILE="$2"
                shift 2
                ;;
            --ollama-dir)
                # å¤„ç†ç”¨æˆ·æŒ‡å®šçš„Ollamaç›®å½•
                local user_ollama_dir="$2"
                user_ollama_dir="${user_ollama_dir%/}"  # ç§»é™¤æœ«å°¾æ–œæ 
                
                # è®¾ç½®æ•°æ®ç›®å½•å’Œæ¨¡å‹ç›®å½•
                if [[ "$user_ollama_dir" == */models ]]; then
                    OLLAMA_MODELS_DIR="$user_ollama_dir"
                    OLLAMA_DATA_DIR="${user_ollama_dir%/models}"
                else
                    OLLAMA_DATA_DIR="$user_ollama_dir"
                    OLLAMA_MODELS_DIR="$user_ollama_dir/models"
                fi
                shift 2
                ;;
            --hf-backup-dir)
                HF_ORIGINAL_BACKUP_DIR="$2"
                shift 2
                ;;
            --backup)
                BACKUP_MODEL="$2"
                shift 2
                ;;
            --backup-all)
                BACKUP_ALL="true"
                shift
                ;;
            --list)
                LIST_MODELS="true"
                shift
                ;;
            --restore)
                RESTORE_FILE="$2"
                shift 2
                ;;
            --remove)
                REMOVE_MODEL="$2"
                shift 2
                ;;
            --remove-all)
                REMOVE_ALL="true"
                shift
                ;;
            --backup-dir)
                BACKUP_OUTPUT_DIR="$2"
                shift 2
                ;;
            --check-only)
                CHECK_ONLY="true"
                shift
                ;;
            --install)
                CHECK_ONLY="false"
                shift
                ;;
            --force-download)
                FORCE_DOWNLOAD="true"
                CHECK_ONLY="false"  # å¼ºåˆ¶ä¸‹è½½æ—¶åº”è¯¥å®é™…æ‰§è¡Œä¸‹è½½
                shift
                ;;
            --force)
                FORCE_RESTORE="true"
                shift
                ;;
            --hf-token)
                HF_TOKEN="$2"
                shift 2
                ;;
            --verbose)
                VERBOSE="true"
                shift
                ;;
            --rebuild)
                REBUILD="true"
                shift
                ;;
            --generate-compose)
                GENERATE_COMPOSE="true"
                shift
                ;;
            *)
                log_error "æœªçŸ¥å‚æ•°: $1"
                show_help
                exit 1
                ;;
        esac
    done
    
    # æ˜¾ç¤ºå½“å‰ä»»åŠ¡ï¼ˆç®€åŒ–ï¼‰
    local current_task=""
    if [[ -n "$BACKUP_MODEL" ]]; then
        current_task="å¤‡ä»½æ¨¡å‹: $BACKUP_MODEL"
    elif [[ "$BACKUP_ALL" == "true" ]]; then
        current_task="æ‰¹é‡å¤‡ä»½æ‰€æœ‰æ¨¡å‹"
    elif [[ -n "$RESTORE_FILE" ]]; then
        current_task="æ¢å¤æ¨¡å‹: $RESTORE_FILE"
    elif [[ -n "$REMOVE_MODEL" ]]; then
        current_task="åˆ é™¤æ¨¡å‹: $REMOVE_MODEL"
    elif [[ "$REMOVE_ALL" == "true" ]]; then
        current_task="æ‰¹é‡åˆ é™¤æ‰€æœ‰æ¨¡å‹"
    elif [[ "$LIST_MODELS" == "true" ]]; then
        current_task="åˆ—å‡ºå·²å®‰è£…çš„æ¨¡å‹"
    elif [[ "$GENERATE_COMPOSE" == "true" ]]; then
        current_task="ç”ŸæˆDocker Composeé…ç½®"
    elif [[ "$CHECK_ONLY" == "true" ]]; then
        current_task="æ£€æŸ¥æ¨¡å‹çŠ¶æ€"
    else
        current_task="å®‰è£…/ä¸‹è½½æ¨¡å‹"
    fi
    
    log_info "ğŸš€ ä»»åŠ¡: $current_task"
    log_verbose "æ¨¡å‹åˆ—è¡¨æ–‡ä»¶: $MODELS_FILE"
    log_verbose "Ollamaç›®å½•: $OLLAMA_MODELS_DIR"
    [[ -n "$BACKUP_OUTPUT_DIR" ]] && log_verbose "å¤‡ä»½ç›®å½•: $BACKUP_OUTPUT_DIR"
    
    # åˆå§‹åŒ–è·¯å¾„
    init_paths
    
    # ç¡®ä¿Ollamaç›®å½•å­˜åœ¨
    if [[ ! -d "$OLLAMA_MODELS_DIR" ]]; then
        log_verbose "åˆ›å»ºOllamaæ¨¡å‹ç›®å½•..."
        if ! mkdir -p "$OLLAMA_MODELS_DIR" 2>/dev/null; then
            log_warning "æ— æ³•åˆ›å»ºOllamaæ¨¡å‹ç›®å½•ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨"
        fi
    fi
    
    # æ‰§è¡Œç‰¹å®šä»»åŠ¡å¹¶é€€å‡º
    if [[ -n "$BACKUP_MODEL" ]]; then
        execute_task "æ¨¡å‹å¤‡ä»½" backup_single_model "$BACKUP_MODEL" "$BACKUP_OUTPUT_DIR"
    elif [[ "$BACKUP_ALL" == "true" ]]; then
        execute_task "æ‰¹é‡å¤‡ä»½" backup_models_from_list "$MODELS_FILE" "$BACKUP_OUTPUT_DIR"
    elif [[ "$LIST_MODELS" == "true" ]]; then
        execute_task "æ¨¡å‹åˆ—è¡¨" list_installed_models
    elif [[ "$GENERATE_COMPOSE" == "true" ]]; then
        execute_task "Dockeré…ç½®ç”Ÿæˆ" generate_docker_compose
    elif [[ -n "$RESTORE_FILE" ]]; then
        execute_task "æ¨¡å‹æ¢å¤" restore_model "$RESTORE_FILE" "$FORCE_RESTORE"
    elif [[ -n "$REMOVE_MODEL" ]]; then
        execute_task "æ¨¡å‹åˆ é™¤" remove_model_smart "$REMOVE_MODEL" "$FORCE_RESTORE"
    elif [[ "$REMOVE_ALL" == "true" ]]; then
        execute_task "æ‰¹é‡åˆ é™¤" remove_models_from_list "$MODELS_FILE" "$FORCE_RESTORE"
    fi
    
    # æ£€æŸ¥ä¾èµ–
    check_dependencies
    
    # è§£ææ¨¡å‹åˆ—è¡¨
    local models=()
    parse_models_list "$MODELS_FILE" models
    
    if [[ ${#models[@]} -eq 0 ]]; then
        log_warning "æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡å‹ï¼Œé€€å‡º"
        exit 0
    fi
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦Dockeré•œåƒï¼ˆä»…åœ¨æœ‰HuggingFaceæ¨¡å‹æ—¶ï¼‰
    local has_hf_models=false
    for model in "${models[@]}"; do
        if [[ "$model" =~ ^huggingface:|^hf-gguf: ]]; then
            has_hf_models=true
            break
        fi
    done
    
    if [[ "$has_hf_models" == "true" ]]; then
        if [[ "$REBUILD" == "true" ]]; then
            build_docker_image
        else
            # ç¡®ä¿Dockeré•œåƒå­˜åœ¨
            ensure_hf_downloader_image
        fi
    fi
    
    # å¤„ç†æ¯ä¸ªæ¨¡å‹
    local total_models=${#models[@]}
    local processed=0
    local failed=0
    
    for model in "${models[@]}"; do
        processed=$((processed + 1))
        log_verbose "å¤„ç†æ¨¡å‹ [$processed/$total_models]: $model"
        
        # å¤„ç†å•ä¸ªæ¨¡å‹é”™è¯¯ï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
        if ! process_model "$model" "$FORCE_DOWNLOAD" "$CHECK_ONLY"; then
            failed=$((failed + 1))
        fi
    done
    
    # æ˜¾ç¤ºæ€»ç»“
    log_info "=== å¤„ç†å®Œæˆ ==="
    log_info "æ€»æ¨¡å‹æ•°: $total_models"
    log_info "å·²å¤„ç†: $processed"
    if [[ $failed -gt 0 ]]; then
        log_warning "å¤±è´¥: $failed"
    else
        log_success "å…¨éƒ¨æˆåŠŸå®Œæˆ"
    fi
    
    if [[ "$CHECK_ONLY" == "true" ]]; then
        log_info "æ£€æŸ¥æ¨¡å¼å®Œæˆï¼Œæœªæ‰§è¡Œå®é™…ä¸‹è½½"
    fi
}

# ==================================================================================
#                           Docker Composeç”ŸæˆåŠŸèƒ½
# ==================================================================================

# ç”Ÿæˆdocker-compose.yamlæ–‡ä»¶
update_existing_compose() {
    local output_file="$1"
    local custom_models="$2"
    local default_model="$3"
    
    log_info "æ›´æ–°ç°æœ‰docker-compose.yamlæ–‡ä»¶ä¸­çš„CUSTOM_MODELSé…ç½®"
    
    # åˆ›å»ºå¤‡ä»½
    local backup_file="${output_file}.backup.$(date +%Y%m%d_%H%M%S)"
    cp "$output_file" "$backup_file"
    log_info "å·²å¤‡ä»½ç°æœ‰æ–‡ä»¶: $backup_file"
    
    # ä½¿ç”¨Pythonè„šæœ¬æ›´æ–°CUSTOM_MODELSç¯å¢ƒå˜é‡
    if grep -q "CUSTOM_MODELS=" "$output_file"; then
        # ä½¿ç”¨Pythonæ¥ç²¾ç¡®å¤„ç†YAMLæ–‡ä»¶ä¸­çš„å¤šè¡ŒCUSTOM_MODELS
        # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶å­˜å‚¨å¤šè¡Œå†…å®¹
        local temp_models_file=$(mktemp)
        echo "$custom_models" > "$temp_models_file"
        
        # ä½¿ç”¨çº¯shellå®ç°æ›¿æ¢åŠŸèƒ½
        update_docker_compose_models() {
            local file_path="$1"
            local models_file="$2"
            local default_model="$3"
            
            # è¯»å–æ–°çš„æ¨¡å‹é…ç½®
            local new_models
            new_models=$(cat "$models_file")
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            local temp_file=$(mktemp)
            
            # ä½¿ç”¨sedå’Œç®€å•çš„çŠ¶æ€æœºå¤„ç†å¤šè¡ŒCUSTOM_MODELSæ›¿æ¢
            # é¦–å…ˆæ ‡è®°å¼€å§‹å’Œç»“æŸä½ç½®
            start_line=$(grep -n '^[[:space:]]*-[[:space:]]*"CUSTOM_MODELS=' "$file_path" | cut -d: -f1)
            end_line=$(tail -n +$((start_line + 1)) "$file_path" | grep -n '"$' | head -1 | cut -d: -f1)
            end_line=$((start_line + end_line))
            
            if [[ -n "$start_line" && -n "$end_line" ]]; then
                # æå–å‰ç¼€ï¼ˆç¼©è¿›å’Œ"CUSTOM_MODELS="ï¼‰
                prefix=$(sed -n "${start_line}p" "$file_path" | sed 's/\(^[[:space:]]*-[[:space:]]*"CUSTOM_MODELS=\).*/\1/')
                
                # æ„å»ºæ–°æ–‡ä»¶ï¼šå¤´éƒ¨ + æ–°è¡Œ + å°¾éƒ¨
                head -n $((start_line - 1)) "$file_path" > "$temp_file"
                echo "${prefix}${new_models}\"" >> "$temp_file"
                tail -n +$((end_line + 1)) "$file_path" >> "$temp_file"
            else
                # å¦‚æœæ‰¾ä¸åˆ°å¤šè¡Œæ ¼å¼ï¼Œå›é€€åˆ°ç®€å•æ›¿æ¢
                cp "$file_path" "$temp_file"
            fi
            
            # å¤„ç†DEFAULT_MODELæ›¿æ¢  
            sed -E "s|(^[[:space:]]*-[[:space:]]*DEFAULT_MODEL=)[^[:space:]#]*(.*)|\\1${default_model}  # è‡ªåŠ¨è®¾ç½®ä¸ºmodels.listç¬¬ä¸€ä¸ªæ¨¡å‹|" "$temp_file" > "$file_path"
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            rm -f "$temp_file"
            return 0
        }
        
        if update_docker_compose_models "$output_file" "$temp_models_file" "$default_model"; then
            echo "SUCCESS"
        else
            echo "ERROR: Failed to update docker-compose.yaml"
            exit 1
        fi
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        rm -f "$temp_models_file"
        
        if [[ $? -eq 0 ]]; then
            log_success "æˆåŠŸæ›´æ–°docker-compose.yamlä¸­çš„CUSTOM_MODELSé…ç½®"
            log_info "æ›´æ–°å†…å®¹: $custom_models"
        else
            log_error "ä½¿ç”¨Pythonæ›´æ–°å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨sedæ–¹æ³•"
            
            # å¤‡ç”¨æ–¹æ³•ï¼šä½¿ç”¨sedè¿›è¡Œç®€å•æ›¿æ¢
            sed -i.tmp "s|CUSTOM_MODELS=[^\"]*|CUSTOM_MODELS=$custom_models|g" "$output_file"
            rm -f "${output_file}.tmp"
            
            log_success "ä½¿ç”¨sedæˆåŠŸæ›´æ–°CUSTOM_MODELSé…ç½®"
        fi
    else
        log_error "æœªåœ¨docker-compose.yamlä¸­æ‰¾åˆ°CUSTOM_MODELSé…ç½®"
        return 1
    fi
    
    return 0
}

generate_docker_compose() {
    local output_file="${1:-./docker-compose.yaml}"
    local models_file="${MODELS_FILE:-./models.list}"
    
    # æ£€æŸ¥æ¨¡å‹åˆ—è¡¨æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if [[ ! -f "$models_file" ]]; then
        log_error "æ¨¡å‹åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: $models_file"
        return 1
    fi
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨docker-compose.yamlæ–‡ä»¶
    if [[ -f "$output_file" ]]; then
        log_info "æ£€æµ‹åˆ°ç°æœ‰docker-compose.yamlæ–‡ä»¶ï¼Œå°†æ›´æ–°CUSTOM_MODELSé…ç½®"
        
        # ç”ŸæˆCUSTOM_MODELSå†…å®¹
        local custom_models_content
        custom_models_content=$(generate_custom_models_list "$models_file")
        
        if [[ -z "$custom_models_content" ]]; then
            log_warning "æœªæ‰¾åˆ°æ¿€æ´»çš„æ¨¡å‹ï¼Œå°†ç”Ÿæˆé»˜è®¤é…ç½®"
            custom_models_content="-all"
        fi
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„æ¨¡å‹
        if [[ "$custom_models_content" == "-all" ]]; then
            log_error "é”™è¯¯: models.list ä¸­æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹é…ç½®"
            log_error "è¯·ç¡®ä¿ models.list ä¸­è‡³å°‘æœ‰ä¸€ä¸ªæœªè¢«æ³¨é‡Šçš„æ¨¡å‹é…ç½®"
            return 1
        fi
        
        # è‡ªåŠ¨æ£€æµ‹é»˜è®¤æ¨¡å‹
        local default_model
        default_model=$(detect_default_model "$models_file")
        
        [[ -n "${VERBOSE}" ]] && log_info "ç”Ÿæˆçš„CUSTOM_MODELS: $custom_models_content"
        [[ -n "${VERBOSE}" ]] && log_info "æ£€æµ‹åˆ°çš„é»˜è®¤æ¨¡å‹: $default_model"
        
        # æ›´æ–°ç°æœ‰æ–‡ä»¶
        update_existing_compose "$output_file" "$custom_models_content" "$default_model"
    else
        log_info "åŸºäºæ¨¡å‹åˆ—è¡¨ç”Ÿæˆdocker-compose.yaml: $models_file"
        
        # ç”ŸæˆCUSTOM_MODELSå†…å®¹
        local custom_models_content
        custom_models_content=$(generate_custom_models_list "$models_file")
        
        if [[ -z "$custom_models_content" ]]; then
            log_warning "æœªæ‰¾åˆ°æ¿€æ´»çš„æ¨¡å‹ï¼Œå°†ç”Ÿæˆé»˜è®¤é…ç½®"
            custom_models_content="-all"
        fi
        
        # è‡ªåŠ¨æ£€æµ‹é»˜è®¤æ¨¡å‹
        local default_model
        default_model=$(detect_default_model "$models_file")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„æ¨¡å‹ (CUSTOM_MODELSåªæœ‰-allè¯´æ˜æ²¡æœ‰æ¿€æ´»çš„æ¨¡å‹)
        if [[ "$custom_models_content" == "-all" ]]; then
            log_error "é”™è¯¯: models.list ä¸­æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹é…ç½®"
            log_error "è¯·ç¡®ä¿ models.list ä¸­è‡³å°‘æœ‰ä¸€ä¸ªæœªè¢«æ³¨é‡Šçš„æ¨¡å‹é…ç½®"
            return 1
        fi
        
        [[ -n "${VERBOSE}" ]] && log_info "ç”Ÿæˆçš„CUSTOM_MODELS: $custom_models_content"
        [[ -n "${VERBOSE}" ]] && log_info "æ£€æµ‹åˆ°çš„é»˜è®¤æ¨¡å‹: $default_model"
        
        # ç”Ÿæˆdocker-compose.yamlå†…å®¹
        generate_compose_content "$output_file" "$custom_models_content" "$default_model"
    fi
}

# ç”ŸæˆCUSTOM_MODELSåˆ—è¡¨
generate_custom_models_list() {
    local models_file="$1"
    local custom_models_entries=()
    
    # æ·»åŠ  -all ä½œä¸ºç¬¬ä¸€ä¸ªæ¡ç›®ï¼ˆéšè—æ‰€æœ‰é»˜è®¤æ¨¡å‹ï¼‰
    custom_models_entries+=("-all")
    
    while IFS= read -r line || [[ -n "$line" ]]; do
        # è·³è¿‡æ³¨é‡Šè¡Œå’Œç©ºè¡Œ
        [[ "$line" =~ ^[[:space:]]*# ]] && continue
        [[ -z "${line// }" ]] && continue
        
        # è§£æè¡Œå†…å®¹
        read -r model_type model_spec quantize_type <<< "$line"
        
        case "$model_type" in
            "ollama")
                if [[ -n "$model_spec" ]]; then
                    local alias=$(generate_model_alias "$model_spec" "ollama")
                    local entry="+${model_spec}@OpenAI=${alias}"
                    custom_models_entries+=("$entry")
                fi
                ;;
            "hf-gguf")
                if [[ -n "$model_spec" ]]; then
                    local alias=$(generate_model_alias "$model_spec" "hf-gguf")
                    local entry="+${model_spec}@OpenAI=${alias}"
                    custom_models_entries+=("$entry")
                fi
                ;;
            "huggingface")
                if [[ -n "$model_spec" && -n "$quantize_type" ]]; then
                    local ollama_name=$(echo "$model_spec" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9._-]/_/g')
                    local alias=$(generate_model_alias "${ollama_name}:latest" "huggingface")
                    local entry="+${ollama_name}:latest@OpenAI=${alias}"
                    custom_models_entries+=("$entry")
                fi
                ;;
        esac
    done < "$models_file"
    
    # è¾“å‡ºCUSTOM_MODELSæ ¼å¼
    if [[ ${#custom_models_entries[@]} -gt 1 ]]; then
        printf '%s' "${custom_models_entries[0]}"
        for ((i=1; i<${#custom_models_entries[@]}; i++)); do
            printf ',\\\n        %s' "${custom_models_entries[i]}"
        done
    else
        echo "-all"
    fi
}

# ç”Ÿæˆç®€å•çš„æ¨¡å‹åˆ«å
generate_model_alias() {
    local model_spec="$1"
    local model_type="$2"
    
    # æ ¹æ®æ¨¡å‹ç±»å‹æå–å®é™…çš„æ¨¡å‹åç§°
    local model_name=""
    local model_version=""
    
    case "$model_type" in
        "hf-gguf")
            # å¯¹äº hf-gguf æ¨¡å‹ï¼Œä»è·¯å¾„ä¸­æå–æ¨¡å‹åç§°
            # æ ¼å¼å¦‚: hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF:latest
            if [[ "$model_spec" =~ hf\.co/[^/]+/([^/:]+) ]]; then
                model_name="${BASH_REMATCH[1]}"
                # ç§»é™¤å¸¸è§çš„ GGUF åç¼€
                model_name=$(echo "$model_name" | sed 's/-GGUF$//' | sed 's/_GGUF$//')
            fi
            ;;
        "huggingface")
            # å¯¹äº huggingface æ¨¡å‹ï¼Œä½¿ç”¨ä¼ é€’çš„å·²å¤„ç†åç§°
            model_name="$model_spec"
            model_name="${model_name%:*}"
            ;;
        *)
            # å¯¹äº ollama å’Œå…¶ä»–ç±»å‹ï¼Œä½¿ç”¨åŸºç¡€åç§°
            model_name="${model_spec%:*}"
            ;;
    esac
    
    # ä»æ¨¡å‹è§„æ ¼ä¸­æå–ç‰ˆæœ¬ä¿¡æ¯
    if [[ "$model_spec" =~ :(.+)$ ]]; then
        model_version="${BASH_REMATCH[1]}"
    fi
    
    # å¦‚æœæ²¡æœ‰æå–åˆ°æ¨¡å‹åç§°ï¼Œä½¿ç”¨ç±»å‹ä½œä¸ºåå¤‡
    if [[ -z "$model_name" ]]; then
        model_name="$model_type"
    fi
    
    # æ¸…ç†æ¨¡å‹åç§°å’Œç‰ˆæœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦
    local clean_name=$(echo "$model_name" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9]/-/g' | sed 's/--*/-/g' | sed 's/^-\|-$//g')
    
    if [[ -n "$model_version" && "$model_version" != "latest" ]]; then
        local clean_version=$(echo "$model_version" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9.]/-/g' | sed 's/--*/-/g' | sed 's/^-\|-$//g')
        echo "${clean_name}-${clean_version}"
    else
        echo "$clean_name"
    fi
}

# æ£€æµ‹é»˜è®¤æ¨¡å‹
detect_default_model() {
    local models_file="$1"
    local first_active_model=""
    
    while IFS= read -r line || [[ -n "$line" ]]; do
        # è·³è¿‡æ³¨é‡Šè¡Œå’Œç©ºè¡Œ
        [[ "$line" =~ ^[[:space:]]*# ]] && continue
        [[ -z "${line// }" ]] && continue
        
        # è§£æè¡Œå†…å®¹
        read -r model_type model_spec quantize_type <<< "$line"
        
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ¿€æ´»çš„æ¨¡å‹å¹¶ç”Ÿæˆå…¶åˆ«å
        if [[ -n "$model_spec" && -z "$first_active_model" ]]; then
            case "$model_type" in
                "ollama")
                    first_active_model=$(generate_model_alias "$model_spec" "ollama")
                    break
                    ;;
                "hf-gguf")
                    first_active_model=$(generate_model_alias "$model_spec" "hf-gguf")
                    break
                    ;;
                "huggingface")
                    if [[ -n "$quantize_type" ]]; then
                        local ollama_name=$(echo "$model_spec" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9._-]/_/g')
                        first_active_model=$(generate_model_alias "${ollama_name}:latest" "huggingface")
                        break
                    fi
                    ;;
            esac
        fi
    done < "$models_file"
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ¿€æ´»çš„æ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤å€¼
    echo "${first_active_model:-qwen3-14b}"
}

# ç”Ÿæˆdocker-compose.yamlæ–‡ä»¶å†…å®¹
detect_gpus() {
    local gpu_indices=""
    
    if command -v nvidia-smi &>/dev/null; then
        gpu_indices=$(nvidia-smi --query-gpu=index --format=csv,noheader,nounits 2>/dev/null | tr '\n' ',' | sed 's/,$//')
        if [[ -n "$gpu_indices" ]]; then
            echo "$gpu_indices"
        else
            echo "0,1,2,3"
        fi
    else
        echo "0,1,2,3"
    fi
}

generate_compose_content() {
    local output_file="$1"
    local custom_models="$2"
    local default_model="$3"
    
    local cuda_devices
    cuda_devices=$(detect_gpus)
    
    # è·å–ä¸»æœºæ—¶åŒº
    local host_timezone=$(get_host_timezone)
    [[ -z "$host_timezone" ]] && host_timezone="UTC"
    
    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆ›å»ºå¤‡ä»½
    if [[ -f "$output_file" ]]; then
        local backup_file="${output_file}.backup.$(date +%Y%m%d_%H%M%S)"
        cp "$output_file" "$backup_file"
        log_info "å·²å¤‡ä»½ç°æœ‰æ–‡ä»¶: $backup_file"
    fi
    
    # ç”Ÿæˆdocker-compose.yamlå†…å®¹
    cat > "$output_file" << EOF
services:
  ollama:
    image: $DOCKER_IMAGE_OLLAMA
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ./ollama:/root/.ollama
    networks:
      - llms-tools-network
    environment:
      # Ollamaä¼˜åŒ–é…ç½®
      - CUDA_VISIBLE_DEVICES=$cuda_devices # è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU
      - OLLAMA_NEW_ENGINE=1 # æ–°çš„å¼•æ“, ollamarunner
      - OLLAMA_SCHED_SPREAD=1 # å¯ç”¨å¤šGPUè´Ÿè½½å‡è¡¡
      - OLLAMA_KEEP_ALIVE=5m # æ¨¡å‹åœ¨å†…å­˜ä¸­ä¿æŒåŠ è½½çš„æ—¶é•¿, åˆ†é’Ÿ
      - OLLAMA_NUM_PARALLEL=3 # å¹¶å‘è¯·æ±‚æ•°
      - OLLAMA_FLASH_ATTENTION=1 # flash attention, ç”¨äºä¼˜åŒ–æ³¨æ„åŠ›è®¡ç®—, é™ä½æ˜¾å­˜ä½¿ç”¨
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    command: [ "serve"]
    restart: unless-stopped

  one-api:
    image: $DOCKER_IMAGE_ONE_API
    container_name: one-api
    volumes:
      - ./one-api:/data
    networks:
      - llms-tools-network
    ports:
      - "3001:3001"
    depends_on:
      - ollama
    environment:
      - TZ=${host_timezone}
      - SESSION_SECRET=xxxxxxxxxxxxxxxxxxxxxx  # ä¿®æ”¹ä¸ºéšæœºç”Ÿæˆçš„ä¼šè¯å¯†é’¥
    command: [ "--port", "3001" ]
    restart: unless-stopped

  prompt-optimizer:
    image: $DOCKER_IMAGE_PROMPT_OPTIMIZER
    container_name: prompt-optimizer
    ports:
      - "8501:80"
    environment:
      - VITE_CUSTOM_API_BASE_URL=http://YOUR_SERVER_IP:3001/v1  # ä¿®æ”¹ä¸ºä½ çš„æœåŠ¡å™¨IPåœ°å€
      - VITE_CUSTOM_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx  # ä¿®æ”¹ä¸ºä½ çš„APIå¯†é’¥
      - VITE_CUSTOM_API_MODEL=$default_model  # è‡ªåŠ¨è®¾ç½®ä¸ºmodels.listç¬¬ä¸€ä¸ªæ¨¡å‹
      - ACCESS_USERNAME=admin  # ä¿®æ”¹ä¸ºä½ çš„ç”¨æˆ·å
      - ACCESS_PASSWORD=xxxxxxxxxxxxxxxxxxxxxx  # ä¿®æ”¹ä¸ºä½ çš„å¯†ç 
    networks:
      - llms-tools-network
    depends_on:
      - one-api
    restart: unless-stopped

  chatgpt-next-web:
    image: $DOCKER_IMAGE_CHATGPT_NEXT_WEB
    container_name: chatgpt-next-web
    ports:
      - "3000:3000"
    environment:
      - OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx  # ä¿®æ”¹ä¸ºä½ çš„OpenAI APIå¯†é’¥
      - BASE_URL=http://one-api:3001
      - PROXY_URL=
      - "CUSTOM_MODELS=$custom_models"
      - DEFAULT_MODEL=$default_model  # è‡ªåŠ¨è®¾ç½®ä¸ºmodels.listç¬¬ä¸€ä¸ªæ¨¡å‹
      - CODE=xxxxxxxxxxxxxxxxxxxxxx  # ä¿®æ”¹ä¸ºä½ çš„è®¿é—®å¯†ç 
    networks:
      - llms-tools-network
    depends_on:
      - one-api
    restart: unless-stopped

networks:
  llms-tools-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.28.0/24
EOF

    log_success "æˆåŠŸç”Ÿæˆdocker-compose.yamlæ–‡ä»¶: $output_file"
    log_info "åŒ…å«æ¨¡å‹é…ç½®: $custom_models"
    log_info "é»˜è®¤æ¨¡å‹: $default_model"
    log_info "æ£€æµ‹åˆ°GPUè®¾å¤‡: $cuda_devices"
    echo ""
    log_info "âš ï¸  é‡è¦æç¤º: ç”Ÿæˆçš„é…ç½®æ–‡ä»¶ä¸­åŒ…å«å ä½ç¬¦ï¼Œè¯·æ ¹æ®ä»¥ä¸‹è¯´æ˜ä¿®æ”¹ï¼š"
    log_info "== å¿…é¡»ä¿®æ”¹çš„é…ç½® =="
    log_info "1. VITE_CUSTOM_API_BASE_URL: å°† YOUR_SERVER_IP æ›¿æ¢ä¸ºå®é™…æœåŠ¡å™¨IPåœ°å€"
    log_info "2. VITE_CUSTOM_API_KEY: æ›¿æ¢ä¸º one-api ä¸­çš„æœ‰æ•ˆAPIå¯†é’¥"
    log_info "3. ACCESS_USERNAME/ACCESS_PASSWORD: è®¾ç½® prompt-optimizer çš„ç™»å½•å‡­æ®"
    log_info "4. OPENAI_API_KEY: æ›¿æ¢ä¸º one-api ä¸­çš„æœ‰æ•ˆAPIå¯†é’¥"
    log_info "5. SESSION_SECRET: æ›¿æ¢ä¸ºéšæœºç”Ÿæˆçš„ä¼šè¯å¯†é’¥ï¼ˆå»ºè®®32ä½éšæœºå­—ç¬¦ä¸²ï¼‰"
    log_info "6. CODE: è®¾ç½® ChatGPT-Next-Web çš„è®¿é—®å¯†ç "
    log_info "7. VITE_CUSTOM_API_MODEL/DEFAULT_MODEL: å·²è‡ªåŠ¨è®¾ç½®ä¸º $default_modelï¼Œå¯æ ¹æ®éœ€è¦ä¿®æ”¹"
    echo ""
    log_info "== å¯é€‰ä¿®æ”¹çš„é…ç½® =="
    log_info "â€¢ ç«¯å£æ˜ å°„: å¦‚éœ€é¿å…ç«¯å£å†²çªï¼Œå¯ä¿®æ”¹ ports éƒ¨åˆ†çš„ä¸»æœºç«¯å£"
    log_info "  - Ollama: 11434 -> è‡ªå®šä¹‰ç«¯å£"
    log_info "  - One-API: 3001 -> è‡ªå®šä¹‰ç«¯å£" 
    log_info "  - Prompt-Optimizer: 8501 -> è‡ªå®šä¹‰ç«¯å£"
    log_info "  - ChatGPT-Next-Web: 3000 -> è‡ªå®šä¹‰ç«¯å£"
    log_info "â€¢ Dockeré•œåƒ: å¦‚éœ€ä½¿ç”¨ç‰¹å®šç‰ˆæœ¬ï¼Œå¯ä¿®æ”¹ image éƒ¨åˆ†çš„æ ‡ç­¾"
    log_info "â€¢ ç½‘ç»œé…ç½®: å¯ä¿®æ”¹ subnet ä»¥é¿å…IPåœ°å€å†²çª"
    echo ""
    log_info "é…ç½®å®Œæˆåè¿è¡Œ: docker-compose up -d æ¥å¯åŠ¨æœåŠ¡"
    
    return 0
}

# åªæœ‰åœ¨ç›´æ¥è¿è¡Œè„šæœ¬æ—¶æ‰æ‰§è¡Œmainå‡½æ•°
if [[ "${BASH_SOURCE[0]:-$0}" == "${0}" ]]; then
    main "$@"
fi