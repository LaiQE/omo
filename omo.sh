#!/bin/bash
# =============================================================================
# OMO (oh-my-ollama or Ollama Models Organizer)
# =============================================================================
#
# ğŸ¤– åŠŸèƒ½æ¦‚è§ˆï¼š
#   ğŸ“¥ æ¨¡å‹ä¸‹è½½ï¼š
#       â€¢ ä»Ollamaå®˜æ–¹ä»“åº“ä¸‹è½½æ¨¡å‹
#       â€¢ ç›´æ¥ä¸‹è½½HuggingFaceçš„GGUFæ ¼å¼æ¨¡å‹
#
#   ğŸ’¾ æ¨¡å‹å¤‡ä»½ï¼š
#       â€¢ å®Œæ•´å¤‡ä»½Ollamaæ¨¡å‹ï¼ˆmanifest + blobsï¼‰
#       â€¢ MD5æ ¡éªŒç¡®ä¿æ•°æ®å®Œæ•´æ€§
#       â€¢ ç”Ÿæˆè¯¦ç»†å¤‡ä»½ä¿¡æ¯æ–‡ä»¶
#
#   ğŸ”„ æ¨¡å‹æ¢å¤ï¼š
#       â€¢ ä»å¤‡ä»½æ¢å¤Ollamaæ¨¡å‹
#       â€¢ æ”¯æŒå¼ºåˆ¶è¦†ç›–æ¨¡å¼
#       â€¢ è‡ªåŠ¨éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
#
#   ğŸ“‹ æ¨¡å‹ç®¡ç†ï¼š
#       â€¢ åˆ—å‡ºå·²å®‰è£…æ¨¡å‹åŠè¯¦ç»†ä¿¡æ¯
#       â€¢ æ™ºèƒ½åˆ é™¤æ¨¡å‹ï¼ˆå•ä¸ª/æ‰¹é‡ï¼‰
#       â€¢ æ¨¡å‹å®Œæ•´æ€§æ£€æŸ¥å’ŒéªŒè¯
#       â€¢ ç£ç›˜ä½¿ç”¨æƒ…å†µç»Ÿè®¡
#
#   ğŸ³ å®¹å™¨åŒ–éƒ¨ç½²ï¼š
#       â€¢ ç”ŸæˆDocker Composeé…ç½®
#       â€¢ é›†æˆOllamaã€One-APIã€Prompt-Optimizerç­‰æœåŠ¡
#       â€¢ è‡ªåŠ¨GPUæ”¯æŒå’Œæ—¶åŒºé…ç½®
#       â€¢ æ™ºèƒ½ç«¯å£å’Œç½‘ç»œé…ç½®
#
#   âš™ï¸  é«˜çº§ç‰¹æ€§ï¼š
#       â€¢ å¹¶è¡Œå¤„ç†å’Œç¼“å­˜ä¼˜åŒ–
#       â€¢ è¯¦ç»†æ—¥å¿—å’Œé”™è¯¯å¤„ç†
#
# ğŸ“ æ”¯æŒçš„æ¨¡å‹æ ¼å¼ï¼š
#   â€¢ ollama [model]:[tag]     - Ollamaå®˜æ–¹æ¨¡å‹
#   â€¢ hf-gguf [model]:[tag]    - HuggingFace GGUFæ¨¡å‹(ç›´æ¥å¯¼å…¥)
#
# ğŸ”§ ç¯å¢ƒè¦æ±‚ï¼š
#   â€¢ Docker, nvidia gpu, rsync
#
# ğŸ‘¨â€ğŸ’» ä½œè€…ï¼šChain Lai
# ğŸ“– è¯¦ç»†ä½¿ç”¨è¯´æ˜è¯·è¿è¡Œï¼š./omo.sh --help
# =============================================================================

set -euo pipefail # å¯ç”¨ä¸¥æ ¼çš„é”™è¯¯å¤„ç†

#==============================================================================
# å…¨å±€é…ç½®å’Œå˜é‡å®šä¹‰
#==============================================================================
SCRIPT_DIR=""
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]:-$0}")" && pwd)"
readonly SCRIPT_DIR

# åŸºç¡€è·¯å¾„é…ç½®ï¼ˆå¯åœ¨mainå‡½æ•°ä¸­è¢«è¦†ç›–ï¼‰
MODELS_FILE="${SCRIPT_DIR}/models.list"
OLLAMA_DATA_DIR="${SCRIPT_DIR}/ollama" # .ollamaç›®å½•
OLLAMA_MODELS_DIR="${OLLAMA_DATA_DIR}/models"
BACKUP_OUTPUT_DIR="${SCRIPT_DIR}/backups"

# é¢„è®¡ç®—çš„ç»å¯¹è·¯å¾„ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
ABS_OLLAMA_DATA_DIR=""

# Dockeré•œåƒé…ç½®
readonly DOCKER_IMAGE_OLLAMA="ollama/ollama:latest"
readonly DOCKER_IMAGE_ONE_API="justsong/one-api:latest"
readonly DOCKER_IMAGE_PROMPT_OPTIMIZER="linshen/prompt-optimizer:latest"
readonly DOCKER_IMAGE_CHATGPT_NEXT_WEB="yidadaa/chatgpt-next-web:latest"

# è¿è¡Œæ—¶é…ç½®
VERBOSE="false" # è¯¦ç»†æ¨¡å¼å¼€å…³

#==============================================================================
# å·¥å…·å‡½æ•°
#==============================================================================

# æ˜¾ç¤ºå®¹å™¨æ—¥å¿—çš„å·¥å…·å‡½æ•°
show_container_logs() {
	local container_name="$1"
	log_error "Container logs:"
	docker logs "${container_name}" 2>&1 | tail -10
}

# è·å–ä¸»æœºæ—¶åŒº
get_host_timezone() {
	# å°è¯•å¤šç§æ–¹æ³•è·å–ä¸»æœºæ—¶åŒº
	if command_exists timedatectl; then
		# ä¼˜å…ˆä½¿ç”¨ timedatectlï¼ˆsystemd ç³»ç»Ÿï¼‰
		timedatectl show --property=Timezone --value 2>/dev/null
	elif [[ -L /etc/localtime ]]; then
		# é€šè¿‡ç¬¦å·é“¾æ¥è·å–æ—¶åŒº
		readlink /etc/localtime | sed 's|.*/zoneinfo/||'
	elif [[ -f /etc/timezone ]]; then
		# ä» /etc/timezone æ–‡ä»¶è¯»å–
		cat /etc/timezone
	else
		# é»˜è®¤å›é€€åˆ° UTC
		echo "UTC"
	fi
}

# é¢œè‰²å®šä¹‰
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly NC='\033[0m' # No Color

#==============================================================================
# ä»»åŠ¡æ‰§è¡Œæ¨¡å—
#==============================================================================
# ç»Ÿä¸€çš„ä»»åŠ¡æ‰§è¡Œå‡½æ•°
execute_task() {
	local task_name="$1"
	local task_function="$2"
	shift 2
	local task_args=("$@")

	log_info "Executing ${task_name}..."
	if "${task_function}" "${task_args[@]}"; then
		log_success "${task_name} completed"
		exit 0
	else
		local exit_code=$?
		if [[ ${exit_code} -eq 2 ]]; then
			# ç”¨æˆ·å–æ¶ˆæ“ä½œï¼Œä¸æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
			exit 0
		else
			log_error "${task_name} failed"
			exit 1
		fi
	fi
}

# å¤‡ä»½å•ä¸ªæ¨¡å‹çš„åŒ…è£…å‡½æ•°
backup_single_model() {
	local backup_model="$1"
	local backup_dir="$2"

	# å¤„ç†ä¸åŒç±»å‹çš„æ¨¡å‹å‰ç¼€
	local model_to_backup="${backup_model}"
	if [[ ${backup_model} =~ ^hf-gguf:(.+)$ ]]; then
		model_to_backup="${BASH_REMATCH[1]}"
	elif [[ ${backup_model} =~ ^ollama:(.+)$ ]]; then
		model_to_backup="${BASH_REMATCH[1]}"
	fi

	backup_ollama_model "${model_to_backup}" "${backup_dir}"
}

# æ¢å¤æ¨¡å‹çš„åŒ…è£…å‡½æ•°
restore_model() {
	local restore_file="$1"
	local force_restore="$2"

	# å¦‚æœæ¢å¤æ–‡ä»¶ä¸æ˜¯ç»å¯¹è·¯å¾„ï¼Œåˆ™åœ¨BACKUP_OUTPUT_DIRä¸­æŸ¥æ‰¾
	local restore_path="${restore_file}"
	if [[ ${restore_file} != /* ]]; then
		restore_path="${BACKUP_OUTPUT_DIR}/${restore_file}"
	fi

	restore_ollama_model "${restore_path}" "${force_restore}"
}

# æ¨¡å‹å¤„ç†å™¨ - è§£ææ¨¡å‹æ¡ç›®å¹¶è¿”å›å¤„ç†å‡½æ•°
parse_model_entry() {
	local model_entry="$1"
	local -n result_ref="$2"

	# æ¸…ç©ºç»“æœæ•°ç»„
	result_ref=()

	if [[ ${model_entry} =~ ^ollama:([^:]+):(.+)$ ]]; then
		result_ref[type]="ollama"
		result_ref[name]="${BASH_REMATCH[1]}"
		result_ref[tag]="${BASH_REMATCH[2]}"
		result_ref[display]="${result_ref[name]}:${result_ref[tag]} (Ollama)"

	elif [[ ${model_entry} =~ ^hf-gguf:(.+)$ ]]; then
		result_ref[type]="hf-gguf"
		local model_full_name="${BASH_REMATCH[1]}"
		if [[ ${model_full_name} =~ ^(.+):(.+)$ ]]; then
			result_ref[name]="${BASH_REMATCH[1]}"
			result_ref[tag]="${BASH_REMATCH[2]}"
		else
			result_ref[name]="${model_full_name}"
			result_ref[tag]="latest"
		fi
		result_ref[display]="${result_ref[name]}:${result_ref[tag]} (HF-GGUF)"
	else
		return 1
	fi

	return 0
}

# æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
check_model_exists() {
	local -n model_info_ref=$1

	case "${model_info_ref[type]}" in
	"ollama")
		check_ollama_model "${model_info_ref[name]}" "${model_info_ref[tag]}"
		;;
	"hf-gguf")
		check_hf_gguf_model "${model_info_ref[name]}" "${model_info_ref[tag]}"
		;;
	*)
		return 1
		;;
	esac
}

# ä¸‹è½½æ¨¡å‹
download_model() {
	local -n model_info_ref=$1

	case "${model_info_ref[type]}" in
	"ollama")
		download_ollama_model "${model_info_ref[name]}" "${model_info_ref[tag]}"
		;;
	"hf-gguf")
		download_hf_gguf_model "${model_info_ref[name]}" "${model_info_ref[tag]}"
		;;
	*)
		return 1
		;;
	esac
}

# å°è¯•ä»å¤‡ä»½æ¢å¤æ¨¡å‹
try_restore_model() {
	local -n model_info_ref=$1

	case "${model_info_ref[type]}" in
	"ollama")
		try_restore_ollama_from_backup "${model_info_ref[name]}" "${model_info_ref[tag]}"
		;;
	"hf-gguf")
		try_restore_ollama_from_backup "${model_info_ref[name]}" "${model_info_ref[tag]}"
		;;
	*)
		return 1
		;;
	esac
}

#==============================================================================
# æ—¥å¿—ç³»ç»Ÿæ¨¡å—
#==============================================================================
# æ—¥å¿—è§„åˆ™:
# 1. ä¸»æµç¨‹å‡½æ•°: ä½¿ç”¨æ ‡å‡†æ—¥å¿—å‡½æ•°(log_info, log_success, log_warning, log_error)
#    - åœ¨æ™®é€šæ¨¡å¼å’Œverboseæ¨¡å¼ä¸‹éƒ½æ˜¾ç¤º, ç”¨äºç”¨æˆ·å…³å¿ƒçš„æ ¸å¿ƒæ“ä½œè¿›åº¦
# 2. å·¥å…·å‡½æ•°: æ­£å¸¸è¿½è¸ªæ—¥å¿—ä½¿ç”¨verboseç‰ˆæœ¬(log_verbose, log_verbose_success)
#    - ä»…åœ¨verboseæ¨¡å¼æ˜¾ç¤º, è­¦å‘Šå’Œé”™è¯¯(log_warning, log_error)åœ¨ä»»ä½•æ¨¡å¼éƒ½æ˜¾ç¤º
# 3. é¿å…æ—¥å¿—é‡å¤: å·¥å…·å‡½æ•°çš„è¿½è¸ªä¿¡æ¯åªåœ¨verboseæ¨¡å¼æ˜¾ç¤º, ä¸»æµç¨‹ä¿æŒç®€æ´
#==============================================================================

log_info() {
	printf "${BLUE}[INFO]${NC} %s\n" "$1"
}

log_success() {
	printf "${GREEN}[SUCCESS]${NC} %s\n" "$1"
}

log_warning() {
	printf "${YELLOW}[WARNING]${NC} %s\n" "$1"
}

log_error() {
	printf "${RED}[ERROR]${NC} %s\n" "$1"
}

# Verbose-only logging functions
log_verbose() {
	if [[ ${VERBOSE} == "true" ]]; then
		printf "${BLUE}[INFO]${NC} %s\n" "$1"
	fi
	return 0
}

log_verbose_success() {
	[[ ${VERBOSE} == "true" ]] && printf "${GREEN}[SUCCESS]${NC} %s\n" "$1"
	return 0
}

log_verbose_warning() {
	[[ ${VERBOSE} == "true" ]] && printf "${YELLOW}[WARNING]${NC} %s\n" "$1"
	return 0
}

# æ ¼å¼åŒ–å­—èŠ‚å¤§å°ä¸ºäººç±»å¯è¯»æ ¼å¼
format_bytes() {
	local bytes="$1"

	# ä½¿ç”¨å•æ¬¡awkè°ƒç”¨å‡å°‘å¼€é”€ï¼Œé¢„å®šä¹‰å¸¸é‡æé«˜å¯è¯»æ€§
	awk -v b="${bytes}" '
    BEGIN {
        if (b >= 1073741824) printf "%.1fGB", b / 1073741824
        else if (b >= 1048576) printf "%.1fMB", b / 1048576  
        else printf "%.1fKB", b / 1024
    }'
}

# éªŒè¯æ¨¡å‹æ ¼å¼æ˜¯å¦æ­£ç¡®
validate_model_format() {
	local model_spec="$1"
	if [[ ${model_spec} != *":"* ]]; then
		log_error "Invalid model format, should be 'model_name:version', e.g. 'llama2:7b'"
		return 1
	fi
	return 0
}

# ç­‰å¾…Ollamaå®¹å™¨å°±ç»ª
wait_for_ollama_ready() {
	local container_name="$1"
	local max_attempts=120 # å¢åŠ åˆ°120ç§’
	local attempt=0

	log_verbose "Waiting for Ollama service to start..."

	while ((attempt < max_attempts)); do
		# é¦–å…ˆæ£€æŸ¥å®¹å™¨æ˜¯å¦è¿˜åœ¨è¿è¡Œ
		if ! docker ps -q --filter "name=^${container_name}$" | grep -q .; then
			log_error "Container ${container_name} has stopped running"
			show_container_logs "${container_name}"
			return 1
		fi

		# æ£€æŸ¥ollamaæœåŠ¡æ˜¯å¦å°±ç»ª
		if docker exec "${container_name}" ollama list &>/dev/null; then
			log_verbose_success "Ollama service is ready"
			return 0
		fi

		# æ¯10ç§’æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
		if ((attempt % 10 == 0 && attempt > 0)); then
			log_verbose "Waiting... (${attempt}/${max_attempts} seconds)"
		fi

		sleep 1
		((attempt++))
	done

	log_error "Timeout waiting for Ollama service to be ready (${max_attempts} seconds)"
	show_container_logs "${container_name}"
	return 1
}

# æ„å»ºå®Œæ•´çš„Dockerè¿è¡Œå‘½ä»¤
build_full_docker_cmd() {
	local container_name="$1"
	local use_gpu="${2:-true}"
	local _include_hf_token="${3:-false}" # Currently unused, reserved for future use
	local extra_env=()
	local extra_volumes=()

	# å¤„ç†é¢å¤–çš„ç¯å¢ƒå˜é‡å’ŒæŒ‚è½½å·å‚æ•°
	shift 3
	while [[ $# -gt 0 ]]; do
		case "$1" in
		--env)
			extra_env+=("$2")
			shift 2
			;;
		--volume)
			extra_volumes+=("$2")
			shift 2
			;;
		*)
			break
			;;
		esac
	done

	local docker_cmd=("docker" "run" "--name" "${container_name}" "--rm" "-t")

	# GPUæ”¯æŒ
	if [[ ${use_gpu} == "true" ]]; then
		docker_cmd+=("--gpus" "all")
	fi

	# åŸºç¡€ç¯å¢ƒå˜é‡
	docker_cmd+=("-e" "PYTHONUNBUFFERED=1")
	docker_cmd+=("-e" "TERM=xterm-256color")
	docker_cmd+=("-v" "/etc/localtime:/etc/localtime:ro")
	docker_cmd+=("-e" "TZ=${HOST_TIMEZONE:-UTC}")

	# æ·»åŠ é¢å¤–çš„ç¯å¢ƒå˜é‡
	for env_var in "${extra_env[@]}"; do
		docker_cmd+=("-e" "${env_var}")
	done

	# æ·»åŠ é¢å¤–çš„æŒ‚è½½å·
	for volume in "${extra_volumes[@]}"; do
		docker_cmd+=("-v" "${volume}")
	done

	printf '%s\n' "${docker_cmd[@]}"
}

# é€šç”¨cleanupå‡½æ•°ç”Ÿæˆå™¨
create_cleanup_function() {
	local cleanup_name="$1"
	shift
	local cleanup_items_str="$*"

	# åŠ¨æ€åˆ›å»ºcleanupå‡½æ•°
	eval "${cleanup_name}() {
        local item
        local cleanup_items=(${cleanup_items_str})
        for item in \"\${cleanup_items[@]}\"; do
            if [[ -f \"\$item\" ]]; then
                rm -f \"\$item\"
            elif [[ -d \"\$item\" ]]; then
                rm -rf \"\$item\"
            elif [[ \"\$item\" =~ ^[a-zA-Z0-9_-]+\$ ]]; then
                # å‡è®¾æ˜¯å®¹å™¨å
                docker rm -f \"\$item\" &>/dev/null || true
            fi
        done
    }"
}

# è®¾ç½®cleanup trapçš„é€šç”¨å‡½æ•°
setup_cleanup_trap() {
	local cleanup_function="$1"
	local signals="${2:-EXIT INT TERM}"
	trap '$cleanup_function' "${signals}"
}

# Ollamaæ¨¡å‹åˆ—è¡¨ç¼“å­˜
declare -g OLLAMA_MODELS_CACHE=""
declare -g OLLAMA_CACHE_INITIALIZED="false"

# ä¸´æ—¶Ollamaå®¹å™¨ç®¡ç†
declare -g TEMP_OLLAMA_CONTAINER=""
declare -g EXISTING_OLLAMA_CONTAINER=""

# å…¨å±€æ¸…ç†å‡½æ•°ç®¡ç†
declare -g GLOBAL_CLEANUP_FUNCTIONS=()
declare -g GLOBAL_CLEANUP_INITIALIZED="false"

# å…¨å±€æ¸…ç†å‡½æ•°ç®¡ç†
add_cleanup_function() {
	local func_name="$1"
	if [[ -z ${func_name} ]]; then
		log_error "Cleanup function name cannot be empty"
		return 1
	fi

	# æ£€æŸ¥å‡½æ•°æ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…é‡å¤æ·»åŠ 
	local func
	for func in "${GLOBAL_CLEANUP_FUNCTIONS[@]}"; do
		if [[ ${func} == "${func_name}" ]]; then
			return 0 # å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
		fi
	done

	GLOBAL_CLEANUP_FUNCTIONS+=("${func_name}")

	# å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡æ·»åŠ ï¼Œè®¾ç½®å…¨å±€ trap
	if [[ ${GLOBAL_CLEANUP_INITIALIZED} == "false" ]]; then
		trap 'execute_global_cleanup' EXIT INT TERM
		GLOBAL_CLEANUP_INITIALIZED="true"
		log_verbose "Initializing global cleanup mechanism"
	fi
}

# æ‰§è¡Œæ‰€æœ‰æ¸…ç†å‡½æ•°
execute_global_cleanup() {
	local exit_code=$?
	local func

	# å¦‚æœæ˜¯ä¸­æ–­ä¿¡å·ï¼Œæ˜¾ç¤ºä¸­æ–­æ¶ˆæ¯
	if [[ ${exit_code} -eq 130 ]]; then # Ctrl+C
		log_warning "Detected interrupt signal (Ctrl+C)"
	elif [[ ${exit_code} -eq 143 ]]; then # SIGTERM
		log_warning "Detected termination signal (SIGTERM)"
	fi

	for func in "${GLOBAL_CLEANUP_FUNCTIONS[@]}"; do
		if declare -f "${func}" >/dev/null 2>&1; then
			log_verbose "Executing cleanup function: ${func}"
			"${func}"
		fi
	done

	# å¦‚æœæ˜¯ä¸­æ–­ï¼Œé€€å‡º
	if [[ ${exit_code} -eq 130 || ${exit_code} -eq 143 ]]; then
		exit "${exit_code}"
	fi
}

# ç§»é™¤æ¸…ç†å‡½æ•°
remove_cleanup_function() {
	local func_name="$1"
	local new_array=()
	local func

	for func in "${GLOBAL_CLEANUP_FUNCTIONS[@]}"; do
		if [[ ${func} != "${func_name}" ]]; then
			new_array+=("${func}")
		fi
	done

	GLOBAL_CLEANUP_FUNCTIONS=("${new_array[@]}")
}

# åˆå§‹åŒ–Ollamaæ¨¡å‹åˆ—è¡¨ç¼“å­˜
init_ollama_cache() {
	if [[ ${OLLAMA_CACHE_INITIALIZED} == "true" ]]; then
		return 0
	fi

	log_verbose "Initializing Ollama model list cache..."

	# ä½¿ç”¨ç»Ÿä¸€çš„å®¹å™¨é€»è¾‘è·å–æ¨¡å‹åˆ—è¡¨
	log_verbose "Getting Ollama model list..."

	# è·å–æ¨¡å‹åˆ—è¡¨å¹¶ç¼“å­˜
	OLLAMA_MODELS_CACHE=$(execute_ollama_command_with_output "list" | awk 'NR>1 {print $1}' | sort)
	if [[ -n ${OLLAMA_MODELS_CACHE} ]]; then
		OLLAMA_CACHE_INITIALIZED="true"
		log_verbose_success "Ollama model list cache initialization completed"
	else
		log_verbose "Ollama model list is empty"
		OLLAMA_MODELS_CACHE=""
		OLLAMA_CACHE_INITIALIZED="true"
	fi

	return 0
}

# æ£€æŸ¥Ollamaæ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
check_ollama_model_exists() {
	local model_name="$1"

	# ç¡®ä¿ç¼“å­˜å·²åˆå§‹åŒ–
	if ! init_ollama_cache; then
		log_error "Failed to initialize Ollama model cache"
		return 1
	fi

	# åœ¨ç¼“å­˜ä¸­æŸ¥æ‰¾æ¨¡å‹
	if echo "${OLLAMA_MODELS_CACHE}" | grep -q "^${model_name}$"; then
		return 0
	else
		return 1
	fi
}

# éªŒè¯æ¨¡å‹ä¸šåŠ¡é€»è¾‘å®Œæ•´æ€§
validate_model_business_integrity() {
	local backup_file="$1"

	# åˆ›å»ºä¸´æ—¶ç›®å½•æå–å¤‡ä»½æ–‡ä»¶
	local temp_dir
	temp_dir=$(mktemp -d) || {
		log_error "Failed to create temporary directory"
		return 1
	}

	# æ¸…ç†å‡½æ•°
	cleanup_temp_business() {
		[[ -d ${temp_dir-} ]] && docker_rm_rf "${temp_dir}"
	}
	add_cleanup_function "cleanup_temp_business"

	# æå–å¤‡ä»½æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
	if ! tar -xf "${backup_file}" -C "${temp_dir}" 2>/dev/null; then
		log_error "Unable to extract backup file for business logic verification"
		cleanup_temp_business
		return 1
	fi

	# æŸ¥æ‰¾manifestæ–‡ä»¶
	local manifest_files=()
	while IFS= read -r -d '' manifest; do
		manifest_files+=("${manifest}")
	done < <(find "${temp_dir}" -path "*/manifests/*" -type f -print0 2>/dev/null || true)

	if [[ ${#manifest_files[@]} -eq 0 ]]; then
		log_error "Manifest file not found in backup"
		cleanup_temp_business
		return 1
	fi

	# æ£€æŸ¥æ¯ä¸ªmanifestå¼•ç”¨çš„blobæ–‡ä»¶
	local missing_blobs=0
	local total_blobs=0

	for manifest_file in "${manifest_files[@]}"; do
		if [[ -f ${manifest_file} ]]; then
			# è§£æmanifestæ–‡ä»¶ä¸­çš„blobå¼•ç”¨
			local blob_digests
			blob_digests=$(grep -o '"digest":"sha256:[a-f0-9]\{64\}"' "${manifest_file}" 2>/dev/null | sed 's/"digest":"sha256:\([a-f0-9]\{64\}\)"/\1/g')

			for digest in ${blob_digests}; do
				((total_blobs++))
				local blob_path="${temp_dir}/blobs/sha256-${digest}"
				if [[ ! -f ${blob_path} ]]; then
					log_error "Missing blob file: sha256-${digest}"
					((missing_blobs++))
				fi
			done
		fi
	done

	cleanup_temp_business
	remove_cleanup_function "cleanup_temp_business"

	if [[ ${missing_blobs} -gt 0 ]]; then
		log_error "Found ${missing_blobs}/${total_blobs} missing blob files"
		return 1
	fi

	log_verbose_success "Model business logic integrity verification passed (${total_blobs} blob files)"
	return 0
}

# æ¸…ç†ä¸å®Œæ•´çš„æ¨¡å‹
cleanup_incomplete_model() {
	local model_name="$1"
	local model_tag="$2"
	local full_model_name="${model_name}:${model_tag}"

	log_verbose_warning "Detected incomplete model, cleaning up: ${full_model_name}"

	# ç¡®å®šmanifestæ–‡ä»¶è·¯å¾„
	local manifest_file
	if [[ ${model_name} == hf.co/* ]]; then
		# HuggingFace GGUFæ¨¡å‹
		manifest_file="${OLLAMA_MODELS_DIR}/manifests/${model_name}/${model_tag}"
	elif [[ ${model_name} == *"/"* ]]; then
		# ç”¨æˆ·åˆ†äº«çš„æ¨¡å‹
		local user_name="${model_name%/*}"
		local repo_name="${model_name#*/}"
		manifest_file="${OLLAMA_MODELS_DIR}/manifests/registry.ollama.ai/${user_name}/${repo_name}/${model_tag}"
	else
		# å®˜æ–¹æ¨¡å‹
		manifest_file="${OLLAMA_MODELS_DIR}/manifests/registry.ollama.ai/library/${model_name}/${model_tag}"
	fi

	# åˆ é™¤manifestæ–‡ä»¶
	if [[ -f ${manifest_file} ]]; then
		if docker_rm_rf "${manifest_file}"; then
			log_verbose "Deleted incomplete manifest file: ${manifest_file}"
		else
			log_warning "Unable to delete manifest file: ${manifest_file}"
		fi
	fi

	# æ¸…é™¤ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°æ£€æŸ¥
	OLLAMA_CACHE_INITIALIZED="false"
	OLLAMA_MODELS_CACHE=""

	log_verbose_success "Incomplete model cleanup completed: ${full_model_name}"
}

# éªŒè¯æ¨¡å‹å®‰è£…åçš„å®Œæ•´æ€§
verify_model_after_installation() {
	local model_name="$1"
	local model_tag="$2"
	local full_model_name="${model_name}:${model_tag}"

	log_verbose "Verifying model installation integrity: ${full_model_name}"

	# åˆå§‹åŒ–ç¼“å­˜ä»¥æé«˜å®Œæ•´æ€§æ£€æŸ¥æ€§èƒ½
	ensure_cache_initialized

	# ç­‰å¾…ä¸€ä¸‹è®©æ–‡ä»¶ç³»ç»ŸåŒæ­¥
	sleep 2

	# æ£€æŸ¥æ¨¡å‹å®Œæ•´æ€§ï¼ˆä½¿ç”¨ç¼“å­˜ä¼˜åŒ–ï¼‰
	local model_spec="${model_name}:${model_tag}"
	if verify_integrity "model" "${model_spec}" "use_cache:true,check_blobs:true"; then
		log_verbose_success "Model installation integrity verification passed: ${full_model_name}"
		return 0
	else
		log_error "Model installation incomplete, cleaning up: ${full_model_name}"
		cleanup_incomplete_model "${model_name}" "${model_tag}"
		return 1
	fi
}

# ç®€åŒ–çš„æ¨¡å‹æ£€æŸ¥å‡½æ•°
check_ollama_model() {
	local model_name="$1"
	local model_tag="$2"
	local full_model_name="${model_name}:${model_tag}"

	# é¦–å…ˆå°è¯•é€šè¿‡Ollamaå®¹å™¨æ£€æŸ¥ï¼ˆæœ€å‡†ç¡®ï¼‰
	if check_ollama_model_exists "${full_model_name}"; then
		log_verbose_success "Ollama model already exists: ${full_model_name}"
		return 0
	fi

	# å¦‚æœOllamaå®¹å™¨æ£€æŸ¥å¤±è´¥ï¼Œè¿›è¡Œå®Œæ•´æ€§æ£€æŸ¥ï¼ˆä½¿ç”¨ç¼“å­˜ä¼˜åŒ–ï¼‰
	local model_spec="${model_name}:${model_tag}"
	if verify_integrity "model" "${model_spec}" "use_cache:true,check_blobs:true"; then
		log_verbose_success "Ollama model exists (filesystem verification): ${full_model_name}"
		return 0
	else
		log_verbose_warning "Ollama model does not exist or is incomplete: ${full_model_name}"
		return 1
	fi
}

# è§£ææ¨¡å‹è§„æ ¼ï¼ˆmodel:versionæ ¼å¼ï¼‰
# shellcheck disable=SC2034  # nameref variables are used by reference
parse_model_spec() {
	local model_spec="$1"
	local -n name_var="$2"
	local -n version_var="$3"

	if ! validate_model_format "${model_spec}"; then
		return 1
	fi

	name_var="${model_spec%:*}"
	version_var="${model_spec#*:}"
	return 0
}

# åˆå§‹åŒ–ç»å¯¹è·¯å¾„
init_paths() {
	# è·å–ç»å¯¹è·¯å¾„ï¼Œå¦‚æœç›®å½•ä¸å­˜åœ¨åˆ™å…ˆåˆ›å»ºçˆ¶ç›®å½•
	mkdir -p "${OLLAMA_DATA_DIR}" || {
		log_error "Unable to create necessary directories"
		return 1
	}

	ABS_OLLAMA_DATA_DIR="$(realpath "${OLLAMA_DATA_DIR}")"

}

# Docker backup helper functions

# Dockerè¾…åŠ©å‡½æ•° - é‡å‘½ååˆ†å·æ–‡ä»¶ï¼ˆä».000,.001,.002æ ¼å¼åˆ°.001,.002,.003æ ¼å¼ï¼‰

# Docker helper function - list tar content directly

# æ–‡ä»¶ç³»ç»Ÿæ“ä½œè¾…åŠ©å‡½æ•°
docker_rm_rf() {
	local target_path="$1"

	# å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢åˆ é™¤ç©ºè·¯å¾„æˆ–æ ¹ç›®å½•
	if [[ -z ${target_path} || ${target_path} == "/" ]]; then
		log_error "Safe delete: path is empty or root directory, deletion refused"
		return 1
	fi

	# ç›´æ¥ä½¿ç”¨ç³»ç»Ÿrmå‘½ä»¤
	rm -rf "${target_path}" 2>/dev/null
}

docker_mkdir_p() {
	local target_path="$1"

	# ç›´æ¥ä½¿ç”¨ç³»ç»Ÿmkdirå‘½ä»¤
	mkdir -p "${target_path}" 2>/dev/null
}

# ç¡®ä¿ollama/ollamaé•œåƒå­˜åœ¨
ensure_ollama_image() {
	if ! docker image inspect "${DOCKER_IMAGE_OLLAMA}" &>/dev/null; then
		log_verbose "Pulling ${DOCKER_IMAGE_OLLAMA} image..."
		if ! docker pull "${DOCKER_IMAGE_OLLAMA}"; then
			log_error "${DOCKER_IMAGE_OLLAMA} image pull failed"
			return 1
		fi
		log_verbose_success "${DOCKER_IMAGE_OLLAMA} image pull completed"
	fi
	return 0
}

# æŸ¥æ‰¾è¿è¡Œä¸­çš„Ollamaå®¹å™¨
find_running_ollama_container() {
	# æ£€æŸ¥æ˜¯å¦æœ‰è¿è¡Œä¸­çš„ Ollama å®¹å™¨
	local running_containers
	running_containers=$(docker ps --format "{{.Names}}" --filter "ancestor=ollama/ollama")

	if [[ -n ${running_containers} ]]; then
		# æ‰¾åˆ°ç¬¬ä¸€ä¸ªè¿è¡Œä¸­çš„å®¹å™¨
		EXISTING_OLLAMA_CONTAINER=$(echo "${running_containers}" | head -n1)
		log_verbose "Found running Ollama container: ${EXISTING_OLLAMA_CONTAINER}"
		return 0
	fi

	# æ£€æŸ¥æœ¬åœ°11434ç«¯å£æ˜¯å¦æœ‰æœåŠ¡å“åº”ï¼ˆå¯èƒ½æ˜¯å¤–éƒ¨å®¹å™¨ï¼‰
	if command -v curl >/dev/null 2>&1; then
		if curl -s --connect-timeout 2 http://localhost:11434/api/version >/dev/null 2>&1; then
			# æ‰¾åˆ°ä½¿ç”¨11434ç«¯å£çš„å®¹å™¨
			local port_container
			port_container=$(docker ps --format "{{.Names}}" --filter "publish=11434")
			if [[ -n ${port_container} ]]; then
				EXISTING_OLLAMA_CONTAINER=$(echo "${port_container}" | head -n1)
				log_verbose "Found Ollama container using port 11434: ${EXISTING_OLLAMA_CONTAINER}"
				return 0
			fi
		fi
	fi

	EXISTING_OLLAMA_CONTAINER=""
	return 1
}

# å¯åŠ¨ä¸´æ—¶Ollamaå®¹å™¨
start_temp_ollama_container() {
	if [[ -n ${TEMP_OLLAMA_CONTAINER} ]]; then
		# æ£€æŸ¥ä¸´æ—¶å®¹å™¨æ˜¯å¦è¿˜åœ¨è¿è¡Œ
		if docker ps -q --filter "name=^${TEMP_OLLAMA_CONTAINER}$" | grep -q .; then
			log_verbose "Temporary Ollama container still running: ${TEMP_OLLAMA_CONTAINER}"
			return 0
		else
			log_verbose "Temporary Ollama container stopped, restarting"
			TEMP_OLLAMA_CONTAINER=""
		fi
	fi

	# ç¡®ä¿ Ollama é•œåƒå­˜åœ¨
	ensure_ollama_image || return 1

	TEMP_OLLAMA_CONTAINER="ollama-temp-$$"

	log_verbose "Starting temporary Ollama container: ${TEMP_OLLAMA_CONTAINER}"

	# æ„å»ºå®¹å™¨å¯åŠ¨å‘½ä»¤
	local cmd=("docker" "run" "-d" "--name" "${TEMP_OLLAMA_CONTAINER}")
	cmd+=("--gpus" "all")
	cmd+=("-v" "${ABS_OLLAMA_DATA_DIR}:/root/.ollama")
	cmd+=("-p" "11435:11434") # ä½¿ç”¨ä¸åŒç«¯å£é¿å…å†²çª
	cmd+=("${DOCKER_IMAGE_OLLAMA}")

	# å¯åŠ¨å®¹å™¨
	local start_output
	if start_output=$("${cmd[@]}" 2>&1); then
		log_verbose "Temporary container started successfully, ID: ${start_output:0:12}"

		# ç­‰å¾…æœåŠ¡å°±ç»ª
		if wait_for_ollama_ready "${TEMP_OLLAMA_CONTAINER}"; then
			log_verbose_success "Temporary Ollama container ready: ${TEMP_OLLAMA_CONTAINER}"
			# è®¾ç½®æ¸…ç†é™·é˜±
			setup_temp_container_cleanup
			return 0
		else
			log_error "Temporary Ollama container startup failed"
			docker rm -f "${TEMP_OLLAMA_CONTAINER}" &>/dev/null
			TEMP_OLLAMA_CONTAINER=""
			return 1
		fi
	else
		log_error "Unable to start temporary Ollama container"
		log_error "Docker startup error: ${start_output}"
		TEMP_OLLAMA_CONTAINER=""
		return 1
	fi
}

# æ¸…ç†ä¸´æ—¶Ollamaå®¹å™¨
cleanup_temp_ollama_container() {
	if [[ -n ${TEMP_OLLAMA_CONTAINER} ]]; then
		log_verbose "Cleaning up temporary Ollama container: ${TEMP_OLLAMA_CONTAINER}"
		docker rm -f "${TEMP_OLLAMA_CONTAINER}" &>/dev/null
		TEMP_OLLAMA_CONTAINER=""
	fi
}

# è®¾ç½®ä¸´æ—¶å®¹å™¨æ¸…ç†é™·é˜±
setup_temp_container_cleanup() {
	add_cleanup_function "cleanup_temp_ollama_container"
}

# ç»Ÿä¸€çš„Ollamaå‘½ä»¤æ‰§è¡Œå‡½æ•°
execute_ollama_command() {
	local action="$1"
	shift
	local args=("$@")

	log_verbose "Executing Ollama command: ${action} ${args[*]}"

	# é¦–å…ˆæŸ¥æ‰¾è¿è¡Œä¸­çš„Ollamaå®¹å™¨
	if find_running_ollama_container; then
		log_verbose "Using existing Ollama container: ${EXISTING_OLLAMA_CONTAINER}"
		log_verbose "Executing command: docker exec ${EXISTING_OLLAMA_CONTAINER} ollama ${action} ${args[*]}"
		if docker exec "${EXISTING_OLLAMA_CONTAINER}" ollama "${action}" "${args[@]}"; then
			return 0
		else
			log_error "Failed to execute Ollama command in existing container: ${action} ${args[*]}"
			return 1
		fi
	else
		# æ²¡æœ‰æ‰¾åˆ°è¿è¡Œä¸­çš„å®¹å™¨ï¼Œå¯åŠ¨ä¸´æ—¶å®¹å™¨
		log_verbose "No running Ollama container found, starting a temporary container"
		if start_temp_ollama_container; then
			log_verbose "Executing command in temporary container: docker exec ${TEMP_OLLAMA_CONTAINER} ollama ${action} ${args[*]}"
			if docker exec "${TEMP_OLLAMA_CONTAINER}" ollama "${action}" "${args[@]}"; then
				return 0
			else
				log_error "Failed to execute Ollama command in temporary container: ${action} ${args[*]}"
				return 1
			fi
		else
			log_error "Unable to start temporary Ollama container"
			return 1
		fi
	fi
}

# æ‰§è¡ŒOllamaå‘½ä»¤å¹¶è·å–è¾“å‡º
execute_ollama_command_with_output() {
	local action="$1"
	shift
	local args=("$@")

	# é¦–å…ˆæŸ¥æ‰¾è¿è¡Œä¸­çš„Ollamaå®¹å™¨
	if find_running_ollama_container; then
		docker exec "${EXISTING_OLLAMA_CONTAINER}" ollama "${action}" "${args[@]}" 2>/dev/null
	else
		# æ²¡æœ‰æ‰¾åˆ°è¿è¡Œä¸­çš„å®¹å™¨ï¼Œå¯åŠ¨ä¸´æ—¶å®¹å™¨
		if start_temp_ollama_container; then
			docker exec "${TEMP_OLLAMA_CONTAINER}" ollama "${action}" "${args[@]}" 2>/dev/null
		else
			return 1
		fi
	fi
}

# æ˜¾ç¤ºä½¿ç”¨å¸®åŠ©
show_help() {
	cat <<'EOF'
ğŸ¤– OMO - Oh My Ollama / Ollama Models Organizer

ä½¿ç”¨æ–¹æ³•:
  ./omo.sh [OPTIONS]

é€‰é¡¹:
  --models-file FILE    æŒ‡å®šæ¨¡å‹åˆ—è¡¨æ–‡ä»¶ (é»˜è®¤: ./models.list)
  --ollama-dir DIR      æŒ‡å®šOllamaæ•°æ®ç›®å½• (é»˜è®¤: ./ollama)
  --backup-dir DIR      å¤‡ä»½ç›®å½• (é»˜è®¤: ./backups)
  --install             å®‰è£…/ä¸‹è½½æ¨¡å‹ (è¦†ç›–é»˜è®¤çš„ä»…æ£€æŸ¥è¡Œä¸º)
  --check-only          ä»…æ£€æŸ¥æ¨¡å‹çŠ¶æ€ï¼Œä¸ä¸‹è½½ (é»˜è®¤è¡Œä¸º)
  --force-download      å¼ºåˆ¶é‡æ–°ä¸‹è½½æ‰€æœ‰æ¨¡å‹ (è‡ªåŠ¨å¯ç”¨å®‰è£…æ¨¡å¼)
  --verbose             æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
  --list                åˆ—å‡ºå·²å®‰è£…çš„Ollamaæ¨¡å‹åŠè¯¦ç»†ä¿¡æ¯
  --backup MODEL        å¤‡ä»½æŒ‡å®šæ¨¡å‹ (æ ¼å¼: æ¨¡å‹å:ç‰ˆæœ¬)
  --backup-all          å¤‡ä»½æ‰€æœ‰æ¨¡å‹
  --restore FILE        æ¢å¤æŒ‡å®šå¤‡ä»½æ–‡ä»¶
  --remove MODEL        åˆ é™¤æŒ‡å®šæ¨¡å‹
  --remove-all          åˆ é™¤æ‰€æœ‰æ¨¡å‹
  --force               å¼ºåˆ¶æ“ä½œï¼ˆè·³è¿‡ç¡®è®¤ï¼‰
  --generate-compose    ç”Ÿæˆdocker-compose.yamlæ–‡ä»¶ï¼ˆåŸºäºmodels.listï¼‰
  --help                æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯

æ¨¡å‹åˆ—è¡¨æ–‡ä»¶æ ¼å¼:
  ollama deepseek-r1:1.5b
  hf-gguf hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF:latest

ä¸‹è½½ç¼“å­˜:
  HuggingFace GGUFæ¨¡å‹ä¸‹è½½æ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œç¼“å­˜å¤ç”¨
  æ¯ä¸ªæ¨¡å‹æœ‰ç‹¬ç«‹çš„ç¼“å­˜å­ç›®å½•
  ä¸­æ–­åé‡æ–°è¿è¡Œè„šæœ¬å°†æ¢å¤ä¸‹è½½ï¼Œå®Œæˆåè‡ªåŠ¨ç¼“å­˜

EOF
	cat <<'EOF'

Ollamaæ¨¡å‹å¤‡ä»½:
  æ”¯æŒå®Œæ•´çš„Ollamaæ¨¡å‹å¤‡ä»½å’Œæ¢å¤
  å¤‡ä»½ç›®å½•: ./backups (é»˜è®¤ï¼Œå¯é€šè¿‡--backup-diræŒ‡å®š)
  å¤‡ä»½æ ¼å¼: ç›®å½•å¤åˆ¶ (æ¨¡å‹å/)
  åŒ…å«å†…å®¹: manifestæ–‡ä»¶å’Œæ‰€æœ‰blobæ•°æ®
  è‡ªåŠ¨ç”Ÿæˆ: MD5æ ¡éªŒæ–‡ä»¶å’Œè¯¦ç»†ä¿¡æ¯æ–‡ä»¶
  
å¤‡ä»½ç‰¹æ€§:
  - ç›´æ¥å¤åˆ¶å¤‡ä»½ï¼Œæ— å‹ç¼©å¤„ç†ï¼Œå¤‡ä»½å’Œæ¢å¤é€Ÿåº¦æå¿«
  - MD5æ ¡éªŒç¡®ä¿æ–‡ä»¶å®Œæ•´æ€§
  - æ¯ä¸ªæ¨¡å‹ç‹¬ç«‹æ–‡ä»¶å¤¹ï¼Œä¾¿äºç®¡ç†

ç¤ºä¾‹:
  # æ£€æŸ¥æ¨¡å‹çŠ¶æ€ (é»˜è®¤è¡Œä¸º)
  ./omo.sh
  
  # å®‰è£…/ä¸‹è½½ç¼ºå¤±çš„æ¨¡å‹
  ./omo.sh --install
  
  # ä»…æ£€æŸ¥çŠ¶æ€ (åŒé»˜è®¤è¡Œä¸º)
  ./omo.sh --check-only
  
  # åˆ—å‡ºå·²å®‰è£…çš„æ¨¡å‹
  ./omo.sh --list
  
  # å¤‡ä»½æ¨¡å‹
  ./omo.sh --backup tinyllama:latest
  
  # åˆ é™¤æ¨¡å‹
  ./omo.sh --remove llama2:7b --force

EOF
}

# æ£€æŸ¥ä¾èµ–
# æ£€æŸ¥GPUæ”¯æŒ
check_gpu_support() {
	# æ£€æŸ¥æ˜¯å¦æ”¯æŒNVIDIA GPU
	if command_exists nvidia-smi && nvidia-smi &>/dev/null; then
		return 0 # æ”¯æŒGPU
	fi
	return 1 # ä¸æ”¯æŒGPU
}

check_dependencies() {
	local missing_deps=()

	# æ£€æŸ¥ docker
	if ! command_exists docker; then
		missing_deps+=("docker")
		log_error "Docker not installed or not in PATH"
	else
		# æ£€æŸ¥ Docker å®ˆæŠ¤è¿›ç¨‹æ˜¯å¦è¿è¡Œ
		if ! docker info &>/dev/null; then
			log_error "Docker is installed but daemon is not running, please start Docker service"
			return 1
		fi
	fi

	# æ£€æŸ¥ tar
	if ! command_exists tar; then
		missing_deps+=("tar")
		log_error "tar not installed, required for model file packing/unpacking"
	fi

	# å¦‚æœæœ‰ç¼ºå¤±çš„ä¾èµ–ï¼Œç»™å‡ºæç¤ºå¹¶é€€å‡º
	if [[ ${#missing_deps[@]} -gt 0 ]]; then
		log_error "Missing required system dependencies: ${missing_deps[*]}"
		log_error "Please install the missing dependencies and rerun the script"
		return 1
	fi

	# æ£€æŸ¥GPUæ”¯æŒï¼ˆå¿…éœ€é¡¹ï¼‰
	if ! check_gpu_support; then
		log_error "No NVIDIA GPU support detected. This script requires a GPU environment."
		log_error "Please ensure: 1) NVIDIA drivers are installed  2) nvidia-smi tool is installed"
		return 1
	fi

	log_verbose "NVIDIA GPU support detected, GPU acceleration will be enabled"

	# æ‰€æœ‰ä¾èµ–æ£€æŸ¥é€šè¿‡ï¼Œé™é»˜è¿”å›
	return 0
}

# è§£ææ¨¡å‹åˆ—è¡¨æ–‡ä»¶
parse_models_list() {
	local models_file="$1"
	local -n models_array=${2:-models}

	if [[ ! -f ${models_file} ]]; then
		log_error "Model list file does not exist: ${models_file}"
		return 1
	fi

	log_verbose "Parsing model list file: ${models_file}"

	while IFS= read -r line || [[ -n ${line} ]]; do
		# è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Šè¡Œ
		[[ -z ${line} || ${line} =~ ^[[:space:]]*# ]] && continue

		# ä½¿ç”¨ç©ºæ ¼åˆ†éš”è§£ææ¨¡å‹ä¿¡æ¯: æ¨¡å‹ç±»å‹ æ¨¡å‹åç§° [é‡åŒ–ç±»å‹]
		read -r model_type model_name quantization <<<"${line}"

		if [[ -n ${model_type} && -n ${model_name} ]]; then
			if [[ ${model_type} == "ollama" || ${model_type} == "hf-gguf" ]]; then
				# å¦‚æœæœ‰é‡åŒ–ç±»å‹ï¼Œæ·»åŠ åˆ°æ¨¡å‹ä¿¡æ¯ä¸­
				if [[ -n ${quantization} ]]; then
					models_array+=("${model_type}:${model_name}:${quantization}")
					log_verbose "Added model: ${model_type} -> ${model_name}:${quantization}"
				else
					models_array+=("${model_type}:${model_name}")
					log_verbose "Added model: ${model_type} -> ${model_name}"
				fi
			else
				log_warning "Unknown model type: ${model_type} (line: ${line})"
			fi
		else
			log_warning "Ignoring invalid line: ${line}"
		fi
	done <"${models_file}"

	# æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°æœ‰æ•ˆæ¨¡å‹
	if [[ ${#models_array[@]} -eq 0 ]]; then
		log_warning "================================ WARNING ================================"
		log_warning "No valid models found in models.list file!"
		log_warning "All model entries are either commented out or invalid."
		log_warning ""
		log_warning "Please edit the models.list file:"
		log_warning "1. Uncomment (remove # at the beginning) the models you need"
		log_warning "2. Add your own model configurations"
		log_warning "3. Check model search URLs in the comments for available models"
		log_warning ""
		log_warning "Examples:"
		log_warning "  ollama deepseek-r1:1.5b"
		log_warning "  hf-gguf hf.co/MaziyarPanahi/gemma-3-1b-it-GGUF"
		log_warning "====================================================================="
		echo
	else
		log_verbose "Total models parsed: ${#models_array[@]}"
	fi
}

# æ£€æŸ¥HuggingFace GGUFæ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼ˆé€šè¿‡Ollamaæ£€æŸ¥ï¼‰
check_hf_gguf_model() {
	local model_name="$1"
	local model_tag="$2"
	local full_model_name="${model_name}:${model_tag}"

	# ä½¿ç”¨å®¹å™¨æ£€æŸ¥
	if check_ollama_model_exists "${full_model_name}"; then
		log_verbose_success "HuggingFace GGUF model already exists: ${full_model_name}"
		return 0
	fi

	log_verbose_warning "HuggingFace GGUF model does not exist: ${full_model_name}"
	return 1
}

# ä¸‹è½½Ollamaæ¨¡å‹
download_ollama_model() {
	local model_name="$1"
	local model_tag="$2"

	log_info "Downloading model: ${model_name}:${model_tag}"

	if execute_ollama_command "pull" "${model_name}:${model_tag}"; then
		log_verbose_success "Ollama model download completed: ${model_name}:${model_tag}"

		# éªŒè¯ä¸‹è½½åçš„æ¨¡å‹å®Œæ•´æ€§
		if verify_model_after_installation "${model_name}" "${model_tag}"; then
			log_verbose_success "Model integrity check passed: ${model_name}:${model_tag}"
			return 0
		else
			log_verbose_warning "Model integrity check failed, model has been removed: ${model_name}:${model_tag}"
			return 1
		fi
	else
		log_error "Failed to download Ollama model: ${model_name}:${model_tag}"
		return 1
	fi
}

# ä¸‹è½½HuggingFace GGUFæ¨¡å‹ï¼ˆé€šè¿‡Ollamaç›´æ¥ä¸‹è½½ï¼‰
download_hf_gguf_model() {
	local model_name="$1"
	local model_tag="$2"
	local full_model_name="${model_name}:${model_tag}"

	log_verbose "Starting download of HuggingFace GGUF model: ${full_model_name}"

	if execute_ollama_command "pull" "${full_model_name}"; then
		log_verbose_success "HuggingFace GGUF model download completed: ${full_model_name}"

		# éªŒè¯ä¸‹è½½åçš„æ¨¡å‹å®Œæ•´æ€§
		if verify_model_after_installation "${model_name}" "${model_tag}"; then
			log_verbose_success "Model integrity check passed: ${full_model_name}"
			return 0
		else
			log_error "Model integrity check failed, model has been removed: ${full_model_name}"
			return 1
		fi
	else
		log_error "Failed to download HuggingFace GGUF model: ${full_model_name}"
		return 1
	fi
}

# åˆ é™¤Ollamaæ¨¡å‹
remove_ollama_model() {
	local model_spec="$1"
	local force_delete="${2:-false}"

	# è§£ææ¨¡å‹åç§°å’Œç‰ˆæœ¬
	if ! validate_model_format "${model_spec}"; then
		return 1
	fi

	log_verbose "Preparing to remove Ollama model: ${model_spec}"

	# æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
	local model_name model_version
	if ! parse_model_spec "${model_spec}" model_name model_version; then
		return 1
	fi
	if ! check_ollama_model "${model_name}" "${model_version}"; then
		log_warning "Model does not exist, no need to delete: ${model_spec}"
		return 0
	fi

	# å¦‚æœä¸æ˜¯å¼ºåˆ¶åˆ é™¤ï¼Œè¯¢é—®ç”¨æˆ·ç¡®è®¤
	if [[ ${force_delete} != "true" ]]; then
		log_warning "About to delete model: ${model_spec}"
		echo -n "Confirm deletion? [y/N]: "
		read -r confirm
		if [[ ${confirm} != "y" && ${confirm} != "Y" ]]; then
			log_verbose "Delete operation cancelled"
			return 0
		fi
	fi

	if execute_ollama_command "rm" "${model_spec}"; then
		log_verbose_success "Ollama model deleted successfully: ${model_spec}"
		return 0
	else
		log_error "Failed to delete Ollama model: ${model_spec}"
		return 1
	fi
}

# è·å–æ¨¡å‹ç›¸å…³çš„blobæ–‡ä»¶è·¯å¾„
get_model_blob_paths() {
	local manifest_file="$1"
	local models_dir="$2"
	local blob_paths=()

	if [[ ! -f ${manifest_file} ]]; then
		log_error "Model manifest file does not exist: ${manifest_file}"
		return 1
	fi

	# ä½¿ç”¨hf_downloaderé•œåƒä¸­çš„jqè§£æJSONæ–‡ä»¶
	local layers
	layers=$(docker run --rm --entrypoint="" -v "$(dirname "${manifest_file}"):/data" hf_downloader jq -r '.layers[].digest, .config.digest' "/data/$(basename "${manifest_file}")" 2>/dev/null | sort -u)

	# æ„å»ºblobæ–‡ä»¶è·¯å¾„
	while IFS= read -r digest; do
		if [[ -n ${digest} ]]; then
			# å°† sha256:xxx æ ¼å¼è½¬æ¢ä¸º sha256-xxx
			local blob_name="${digest//:/-}"
			local blob_file="${models_dir}/blobs/${blob_name}"
			blob_paths+=("${blob_file}")
		fi
	done <<<"${layers}"

	# è¾“å‡ºè·¯å¾„
	printf '%s\n' "${blob_paths[@]}"
}

# ===== å¤‡ä»½å·¥å…·å‡½æ•° =====

# é€šç”¨å‘½ä»¤æ£€æŸ¥å‡½æ•°
command_exists() {
	command -v "$1" >/dev/null 2>&1
}

# æ¨¡å‹åç§°å®‰å…¨åŒ–å¤„ç†
# ç»Ÿä¸€çš„æ¨¡å‹åç§°è½¬æ¢å‡½æ•°
# å‚æ•°1: æ¨¡å‹åç§°
# å‚æ•°2: è½¬æ¢ç±»å‹ (backup|ollama|filesystem)
get_safe_model_name() {
	local model_spec="$1"
	local conversion_type="${2:-backup}"

	case "${conversion_type}" in
	"backup")
		# ç”¨äºå¤‡ä»½ç›®å½•å‘½åï¼š/ å’Œ : â†’ _
		echo "${model_spec}" | sed 's/[\/:]/_/g'
		;;
	"ollama")
		# ç”¨äºOllamaæ¨¡å‹å‘½åï¼šå¤æ‚è½¬æ¢è§„åˆ™ï¼ˆä¸€æ¬¡æ€§å¤„ç†ï¼‰
		local full_name_clean
		full_name_clean=$(echo "${model_spec}" | tr '[:upper:]' '[:lower:]' | sed -e 's/\//_/g' -e 's/[^a-z0-9_-]/_/g' -e 's/__*/_/g' -e 's/--*/-/g' -e 's/^[-_]\+\|[-_]\+$//g')
		# é•¿åº¦é™åˆ¶
		if [[ ${#full_name_clean} -gt 50 ]]; then
			local prefix="${full_name_clean:0:30}"
			local suffix="${full_name_clean: -15}"
			full_name_clean="${prefix}_${suffix}"
		fi
		echo "${full_name_clean}"
		;;
	"filesystem")
		# ç”¨äºæ–‡ä»¶ç³»ç»Ÿå®‰å…¨å‘½åï¼š/ â†’ _ï¼Œå…¶ä»–éæ³•å­—ç¬¦ â†’ -
		echo "${model_spec}" | sed -e 's/\//_/g' -e 's/[^a-zA-Z0-9._-]/-/g'
		;;
	*)
		# é»˜è®¤ä½¿ç”¨backupè§„åˆ™
		echo "${model_spec}" | sed 's/[\/:]/_/g'
		;;
	esac
}

# æ–‡ä»¶å¤§å°å·¥å…·å‡½æ•°
get_file_size() {
	local file_path="$1"
	local format="${2:-mb}" # mb, human

	case "${format}" in
	"mb")
		du -sm "${file_path}" 2>/dev/null | cut -f1
		;;
	"human")
		du -sh "${file_path}" 2>/dev/null | cut -f1
		;;
	*)
		log_error "Unknown format: ${format}. Use 'mb' or 'human'"
		return 1
		;;
	esac
}

# å‘åå…¼å®¹çš„åŒ…è£…å‡½æ•°
get_file_size_mb() {
	get_file_size "$1" "mb"
}

get_file_size_human() {
	get_file_size "$1" "human"
}

# è®¡ç®—ç›®å½•çš„MD5æ ¡éªŒå€¼
calculate_directory_md5() {
	local dir_path="$1"
	local md5_file="$2"

	if [[ ! -d ${dir_path} ]]; then
		log_error "Directory does not exist: ${dir_path}"
		return 1
	fi

	log_verbose "Calculating directory MD5 checksum: ${dir_path}"

	# ä½¿ç”¨findå’Œmd5sumè®¡ç®—æ‰€æœ‰æ–‡ä»¶çš„MD5å€¼ï¼Œä½¿ç”¨ç›¸å¯¹è·¯å¾„
	# æŒ‰æ–‡ä»¶è·¯å¾„æ’åºä»¥ç¡®ä¿ç»“æœä¸€è‡´æ€§
	if (cd "${dir_path}" && find . -type f -print0 | sort -z | xargs -0 md5sum) >"${md5_file}" 2>/dev/null; then
		log_verbose "MD5 checksum file generated: ${md5_file}"
		return 0
	else
		log_error "Failed to calculate MD5 checksum"
		return 1
	fi
}

# éªŒè¯ç›®å½•çš„MD5æ ¡éªŒå€¼
verify_directory_md5() {
	local dir_path="$1"
	local md5_file="$2"

	if [[ ! -d ${dir_path} ]]; then
		log_error "Directory does not exist: ${dir_path}"
		return 1
	fi

	if [[ ! -f ${md5_file} ]]; then
		log_error "MD5 checksum file does not exist: ${md5_file}"
		return 1
	fi

	log_verbose "Verifying directory MD5 checksum: ${dir_path}"

	# ä¸´æ—¶è®¡ç®—å½“å‰ç›®å½•çš„MD5å€¼
	local temp_md5
	temp_md5=$(mktemp)
	if ! calculate_directory_md5 "${dir_path}" "${temp_md5}"; then
		rm -f "${temp_md5}"
		return 1
	fi

	# æ¯”è¾ƒMD5æ–‡ä»¶
	if diff "${md5_file}" "${temp_md5}" >/dev/null 2>&1; then
		log_verbose "MD5 checksum verified"
		rm -f "${temp_md5}"
		return 0
	else
		log_error "MD5 checksum verification failed"
		rm -f "${temp_md5}"
		return 1
	fi
}

# å…¨å±€ç¼“å­˜å˜é‡
declare -A BACKUP_CONTENT_CACHE
declare -A MODEL_BLOB_CACHE

# æ£€æŸ¥å¤‡ä»½å®Œæ•´æ€§ï¼ˆæ£€æŸ¥å¤‡ä»½ä¸­æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…éœ€çš„blobæ–‡ä»¶ï¼‰

# è·å–æ¨¡å‹blobåˆ—è¡¨ï¼ˆå¸¦ç¼“å­˜ï¼‰
get_model_blobs_cached() {
	local model_spec="$1"

	# æ£€æŸ¥ç¼“å­˜
	if [[ -n ${MODEL_BLOB_CACHE[${model_spec}]-} ]]; then
		echo "${MODEL_BLOB_CACHE[${model_spec}]}"
		return 0
	fi

	# è§£ææ¨¡å‹åç§°å’Œç‰ˆæœ¬
	local model_name model_version
	if ! parse_model_spec "${model_spec}" model_name model_version; then
		return 1
	fi

	# ç¡®å®šmanifestæ–‡ä»¶è·¯å¾„
	local manifest_file
	if [[ ${model_name} == hf.co/* ]]; then
		manifest_file="${OLLAMA_MODELS_DIR}/manifests/${model_name}/${model_version}"
	elif [[ ${model_name} == *"/"* ]]; then
		local user_name="${model_name%/*}"
		local repo_name="${model_name#*/}"
		manifest_file="${OLLAMA_MODELS_DIR}/manifests/registry.ollama.ai/${user_name}/${repo_name}/${model_version}"
	else
		manifest_file="${OLLAMA_MODELS_DIR}/manifests/registry.ollama.ai/library/${model_name}/${model_version}"
	fi

	# è·å–blobæ–‡ä»¶åˆ—è¡¨
	if [[ -f ${manifest_file} ]]; then
		local blobs
		blobs=$(get_model_blob_paths "${manifest_file}" "${OLLAMA_MODELS_DIR}" | sed "s|^${OLLAMA_MODELS_DIR}/||")
		if [[ -n ${blobs} ]]; then
			# ç¼“å­˜ç»“æœ
			MODEL_BLOB_CACHE[${model_spec}]="${blobs}"
			echo "${blobs}"
			return 0
		fi
	fi

	return 1
}

# å¿«é€Ÿæ£€æŸ¥å•æ–‡ä»¶å¤‡ä»½å®Œæ•´æ€§

# æ¸…ç†å®Œæ•´æ€§æ£€æŸ¥ç¼“å­˜
clear_integrity_cache() {
	[[ -n ${VERBOSE} ]] && log_verbose "Clearing integrity check cache"
	unset BACKUP_CONTENT_CACHE
	unset MODEL_BLOB_CACHE
	declare -g -A BACKUP_CONTENT_CACHE
	declare -g -A MODEL_BLOB_CACHE
}

# ç¡®ä¿å®Œæ•´æ€§æ£€æŸ¥ç¼“å­˜å·²åˆå§‹åŒ–
ensure_cache_initialized() {
	# Initialize cache arrays if they do not exist
	if [[ ! -v BACKUP_CONTENT_CACHE ]] || [[ ! -v MODEL_BLOB_CACHE ]]; then
		declare -g -A BACKUP_CONTENT_CACHE
		declare -g -A MODEL_BLOB_CACHE
		[[ -n ${VERBOSE} ]] && log_verbose "Integrity check cache initialized"
	fi
}

# ==================================================================================
#                           ç»Ÿä¸€å®Œæ•´æ€§éªŒè¯æ¶æ„
# ==================================================================================

# é€šç”¨å®Œæ•´æ€§éªŒè¯å‡½æ•° - ç»Ÿä¸€æ‰€æœ‰éªŒè¯é€»è¾‘çš„å…¥å£ç‚¹
verify_integrity() {
	local verification_type="$1" # model, backup, hf_model
	local target="$2"            # ç›®æ ‡æ–‡ä»¶/è·¯å¾„/æ¨¡å‹è§„æ ¼
	local options="${3-}"        # é™„åŠ é€‰é¡¹ (use_cache:true, check_blobs:true, etc.)

	# è§£æé€‰é¡¹
	local use_cache="true"
	local check_blobs="true"
	local model_spec=""

	# è§£æé€‰é¡¹å­—ç¬¦ä¸²
	if [[ -n ${options} ]]; then
		while IFS=',' read -ra ADDR; do
			for i in "${ADDR[@]}"; do
				case "${i}" in
				use_cache:*)
					use_cache="${i#*:}"
					;;
				check_blobs:*)
					check_blobs="${i#*:}"
					;;
				model_spec:*)
					model_spec="${i#*:}"
					;;
				*)
					# å¿½ç•¥æœªçŸ¥é€‰é¡¹
					;;
				esac
			done
		done <<<"${options}"
	fi

	# ç¡®ä¿ç¼“å­˜å·²åˆå§‹åŒ–
	[[ ${use_cache} == "true" ]] && ensure_cache_initialized

	# æ ¹æ®éªŒè¯ç±»å‹è°ƒç”¨ç›¸åº”çš„éªŒè¯é€»è¾‘
	case "${verification_type}" in
	"model")
		_verify_local_model "${target}" "${use_cache}" "${check_blobs}"
		;;
	"backup")
		_verify_backup_target "${target}" "${model_spec}" "${use_cache}" "${check_blobs}"
		;;
	"backup_file")
		_verify_backup_file "${target}" "${use_cache}"
		;;
	*)
		log_error "Unknown verification type: ${verification_type}"
		return 1
		;;
	esac
}

# å†…éƒ¨å‡½æ•°ï¼šéªŒè¯æœ¬åœ°æ¨¡å‹å®Œæ•´æ€§
_verify_local_model() {
	local model_spec="$1"
	local use_cache="$2"
	local check_blobs="$3"

	# è§£ææ¨¡å‹è§„æ ¼
	local model_name model_tag
	if [[ ${model_spec} =~ ^(.+):(.+)$ ]]; then
		model_name="${BASH_REMATCH[1]}"
		model_tag="${BASH_REMATCH[2]}"
	else
		log_error "Invalid model spec format: ${model_spec}"
		return 1
	fi

	# ç¡®å®šmanifestæ–‡ä»¶è·¯å¾„
	local manifest_file
	if [[ ${model_name} == hf.co/* ]]; then
		manifest_file="${OLLAMA_MODELS_DIR}/manifests/${model_name}/${model_tag}"
	elif [[ ${model_name} == *"/"* ]]; then
		local user_name="${model_name%/*}"
		local repo_name="${model_name#*/}"
		manifest_file="${OLLAMA_MODELS_DIR}/manifests/registry.ollama.ai/${user_name}/${repo_name}/${model_tag}"
	else
		manifest_file="${OLLAMA_MODELS_DIR}/manifests/registry.ollama.ai/library/${model_name}/${model_tag}"
	fi

	# æ£€æŸ¥manifestæ–‡ä»¶æ˜¯å¦å­˜åœ¨
	[[ ! -f ${manifest_file} ]] && return 1

	# å¦‚æœä¸éœ€è¦æ£€æŸ¥blobï¼ŒåªéªŒè¯manifestå­˜åœ¨å³å¯
	[[ ${check_blobs} == "false" ]] && return 0

	# è·å–blobæ–‡ä»¶åˆ—è¡¨å¹¶éªŒè¯
	local blob_files
	if [[ ${use_cache} == "true" ]]; then
		blob_files=$(get_model_blobs_cached "${model_spec}")
		[[ -z ${blob_files} ]] && return 1

		# æ£€æŸ¥æ¯ä¸ªblobæ–‡ä»¶
		while IFS= read -r blob_relative_path; do
			[[ -n ${blob_relative_path} && ! -f "${OLLAMA_MODELS_DIR}/${blob_relative_path}" ]] && return 1
		done <<<"${blob_files}"
	else
		blob_files=$(get_model_blob_paths "${manifest_file}" "${OLLAMA_MODELS_DIR}")
		[[ -z ${blob_files} ]] && return 1

		# æ£€æŸ¥æ¯ä¸ªblobæ–‡ä»¶
		while IFS= read -r blob_file; do
			[[ -n ${blob_file} && ! -f ${blob_file} ]] && return 1
		done <<<"${blob_files}"
	fi

	return 0
}

# å†…éƒ¨å‡½æ•°ï¼šéªŒè¯å¤‡ä»½ç›®æ ‡ï¼ˆç›®å½•å¤‡ä»½ï¼‰
_verify_backup_target() {
	local backup_target="$1"
	local model_spec="$2"
	local use_cache="$3"
	local check_blobs="$4"

	# æ£€æŸ¥ç›®å½•å¤‡ä»½
	if [[ -d ${backup_target} ]]; then
		# Verify directory structure
		if [[ -d "${backup_target}/manifests" ]] && [[ -d "${backup_target}/blobs" ]]; then
			# Verify MD5 checksum
			local md5_file="${backup_target}.md5"
			if [[ -f ${md5_file} ]]; then
				if verify_directory_md5 "${backup_target}" "${md5_file}"; then
					[[ -n ${VERBOSE} ]] && log_info "Directory backup MD5 checksum verified: ${backup_target}"
					return 0
				else
					log_error "Directory backup MD5 checksum failed: ${backup_target}"
					return 1
				fi
			else
				log_warning "MD5 checksum file not found: ${md5_file}"
				return 0 # No MD5 file is still considered valid, but a warning is logged
			fi
		else
			log_error "Invalid directory backup structure: ${backup_target}"
			return 1
		fi
	fi

	return 1
}

# å†…éƒ¨å‡½æ•°ï¼šéªŒè¯æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§

# å†…éƒ¨å‡½æ•°ï¼šéªŒè¯å¤‡ä»½æ–‡ä»¶ï¼ˆä¸šåŠ¡é€»è¾‘å®Œæ•´æ€§ï¼‰
_verify_backup_file() {
	local backup_file="$1"
	local use_detailed_check="$2"

	[[ ! -f ${backup_file} ]] && return 1

	# åŸºæœ¬taræ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥
	if ! docker run --rm --entrypoint="" -v "$(dirname "${backup_file}"):/data" hf_downloader:latest sh -c "
        cd /data && tar -tf '$(basename "${backup_file}")' >/dev/null 2>&1
    "; then
		return 1
	fi

	# å¦‚æœéœ€è¦è¯¦ç»†æ£€æŸ¥ï¼Œæ‰§è¡Œä¸šåŠ¡é€»è¾‘éªŒè¯
	[[ ${use_detailed_check} == "true" ]] && validate_model_business_integrity "${backup_file}"
}

# åˆ é™¤ä¸å®Œæ•´çš„å¤‡ä»½æ–‡ä»¶
remove_incomplete_backup() {
	local backup_base="$1"
	local backup_suffix="${2-}"

	log_verbose "Deleting incomplete backup: ${backup_base}${backup_suffix}"

	# åˆ é™¤ç›®å½•å¤‡ä»½
	local backup_dir="${backup_base}${backup_suffix}"
	if [[ -d ${backup_dir} ]]; then
		rm -rf "${backup_dir}"
		log_verbose "Backup directory deleted: ${backup_dir}"
	fi

	# åˆ é™¤MD5æ ¡éªŒæ–‡ä»¶
	local md5_file="${backup_dir}.md5"
	if [[ -f ${md5_file} ]]; then
		rm -f "${md5_file}"
		log_verbose "MD5 checksum file deleted: ${md5_file}"
	fi

	# åˆ é™¤å¤‡ä»½ä¿¡æ¯æ–‡ä»¶
	local info_file="${backup_base}${backup_suffix}_info.txt"
	if [[ -f ${info_file} ]]; then
		rm -f "${info_file}"
		log_verbose "Backup info file deleted: ${info_file}"
	fi
}

# å®‰å…¨çš„ä¸´æ—¶æ–‡ä»¶åˆ›å»º
create_temp_file() {
	local prefix="${1:-temp}"
	local temp_file
	temp_file=$(mktemp) || {
		log_error "Unable to create temporary file"
		return 1
	}
	echo "${temp_file}"
}

# åˆ›å»ºæ¨¡å‹å¤‡ä»½ç›®å½•
create_model_backup_dir() {
	local model_spec="$1"
	local base_backup_dir="$2"
	local model_safe_name
	model_safe_name=$(get_safe_model_name "${model_spec}")
	local model_backup_dir="${base_backup_dir}/${model_safe_name}"

	# åˆ›å»ºå¤‡ä»½ç›®å½•
	if ! mkdir -p "${model_backup_dir}"; then
		log_error "Unable to create backup directory: ${model_backup_dir}"
		return 1
	fi
	echo "${model_backup_dir}"
}

# ç”Ÿæˆå¤‡ä»½åŸºç¡€è·¯å¾„
get_backup_base_path() {
	local model_spec="$1"
	local backup_dir="$2"
	local suffix="${3-}"
	local model_safe_name
	model_safe_name=$(get_safe_model_name "${model_spec}")
	echo "${backup_dir}/${model_safe_name}${suffix}"
}

# å¤‡ä»½ä¿¡æ¯å’Œç®¡ç†å‡½æ•°

# åˆ›å»ºå¤‡ä»½ä¿¡æ¯æ–‡ä»¶
create_backup_info() {
	local model_spec="$1"
	local backup_base="$2"
	local backup_type="$3"   # "directory", "single" æˆ– "split"
	local _volume_count="$4" # Reserved for future use
	local backup_extension="${5:-original}"

	local info_file="${backup_base}_info.txt"
	local current_time
	current_time=$(date '+%Y-%m-%d %H:%M:%S %Z')
	local model_safe_name
	model_safe_name=$(get_safe_model_name "${model_spec}")

	# ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶åˆ›å»ºå¤‡ä»½ä¿¡æ¯
	local temp_info
	temp_info=$(mktemp)
	cat >"${temp_info}" <<EOF
================================================================================
                           æ¨¡å‹å¤‡ä»½ä¿¡æ¯
================================================================================

å¤‡ä»½åŸºæœ¬ä¿¡æ¯:
  æ¨¡å‹è§„æ ¼: ${model_spec}
  å¤‡ä»½åç§°: ${model_safe_name}
  å¤‡ä»½ç±»å‹: ${backup_type}
  åˆ›å»ºæ—¶é—´: ${current_time}

å¤‡ä»½æ–‡ä»¶ä¿¡æ¯:
EOF

	# æ ¹æ®å¤‡ä»½ç±»å‹æ·»åŠ å…·ä½“çš„æ–‡ä»¶ä¿¡æ¯å’ŒMD5
	if [[ ${backup_type} == "directory" ]]; then
		local backup_dir="${backup_base}_${backup_extension}"
		# å¯¹äºollamaå¤‡ä»½ï¼Œbackup_baseå·²ç»æ˜¯å®Œæ•´è·¯å¾„ï¼Œä¸éœ€è¦æ·»åŠ åç¼€
		if [[ ${backup_extension} == "ollama" ]]; then
			backup_dir="${backup_base}"
		fi
		local backup_size
		backup_size=$(get_file_size_human "${backup_dir}" || echo "æœªçŸ¥")
		local md5_file="${backup_dir}.md5"
		local md5_status="æœ‰æ•ˆ"

		if [[ ! -f ${md5_file} ]]; then
			md5_status="ç¼ºå¤±"
		fi

		cat >>"${temp_info}" <<EOF
  å¤‡ä»½æ–¹å¼: ç›®å½•å¤åˆ¶
  å¤‡ä»½ç›®å½•: $(basename "${backup_dir}")
  å¤‡ä»½å¤§å°: ${backup_size}
  MD5æ ¡éªŒæ–‡ä»¶: ${md5_status}

æ–‡ä»¶åˆ—è¡¨:
EOF

		# æ·»åŠ æ–‡ä»¶åˆ—è¡¨
		if [[ -d ${backup_dir} ]]; then
			find "${backup_dir}" -type f -exec basename {} \; | sort >>"${temp_info}"
		fi

		cat >>"${temp_info}" <<EOF

MD5æ ¡éªŒä¿¡æ¯:
EOF

		# æ·»åŠ MD5æ ¡éªŒä¿¡æ¯
		if [[ -f ${md5_file} ]]; then
			cat "${md5_file}" >>"${temp_info}"
		else
			{
				echo "  MD5æ ¡éªŒæ–‡ä»¶åˆ›å»ºå¤±è´¥æˆ–ä¸å­˜åœ¨"
				echo "  æ–‡ä»¶è·¯å¾„: ${md5_file}"
				echo "  å»ºè®®: é‡æ–°è¿è¡Œå¤‡ä»½ä»¥ç”ŸæˆMD5æ ¡éªŒæ–‡ä»¶"
			} >>"${temp_info}"
		fi

		cat >>"${temp_info}" <<EOF

æ¢å¤å‘½ä»¤:
  # ä½¿ç”¨omo.shæ¢å¤
  ./omo.sh --restore "$(basename "${backup_dir}")"
  
  # æ‰‹åŠ¨æ¢å¤ï¼ˆOllamaæ¨¡å‹ï¼‰
  cp -r "$(basename "${backup_dir}")/manifests/"* "\$OLLAMA_MODELS_DIR/manifests/"
  cp "$(basename "${backup_dir}")/blobs/"* "\$OLLAMA_MODELS_DIR/blobs/"
  

EOF
	else
		log_error "ä¸æ”¯æŒçš„å¤‡ä»½ç±»å‹: ${backup_type}"
		rm -f "${temp_info}"
		return 1
	fi

	cat >>"${temp_info}" <<EOF
================================================================================
                               éªŒè¯ä¿¡æ¯
================================================================================

å¤‡ä»½éªŒè¯:
1. æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§:
   - ä½¿ç”¨MD5æ ¡éªŒæ–‡ä»¶éªŒè¯æ¯ä¸ªæ–‡ä»¶çš„å®Œæ•´æ€§
   - md5sum -c $(basename "${backup_dir}.md5")

2. æ£€æŸ¥å¤‡ä»½ç»“æ„:
   - ç¡®ä¿å¤‡ä»½ç›®å½•åŒ…å«å®Œæ•´çš„æ–‡ä»¶ç»“æ„
   - å¯¹äºOllamaæ¨¡å‹: manifests/ å’Œ blobs/ ç›®å½•

å¤‡ä»½ç‰¹æ€§:
   - ç›´æ¥å¤åˆ¶: æå¿«çš„å¤‡ä»½å’Œæ¢å¤é€Ÿåº¦ï¼Œæ— éœ€å‹ç¼©/è§£å‹ç¼©
   - MD5æ ¡éªŒ: ç¡®ä¿æ–‡ä»¶å®Œæ•´æ€§å’Œä¸€è‡´æ€§
   - ç®€åŒ–ç®¡ç†: å¤‡ä»½æ–‡ä»¶å¯ç›´æ¥è®¿é—®å’Œæ£€æŸ¥

ä½¿ç”¨è¯´æ˜:
- æ­¤å¤‡ä»½åŒ…å«æ¨¡å‹çš„å®Œæ•´æ–‡ä»¶ç»“æ„
- æ¢å¤åå¯ç›´æ¥ä½¿ç”¨ï¼Œæ— éœ€é¢å¤–å¤„ç†
- æ”¯æŒå¢é‡å¤‡ä»½å’Œå·®å¼‚æ£€æŸ¥

ç”Ÿæˆæ—¶é—´: ${current_time}
================================================================================
EOF

	# ç›´æ¥å†™å…¥ä¿¡æ¯æ–‡ä»¶
	if mv "${temp_info}" "${info_file}"; then
		log_verbose_success "Backup info file created: $(basename "${info_file}")"
	else
		log_error "Unable to write backup info file: ${info_file}"
		rm -f "${temp_info}"
		return 1
	fi
}

# åˆ—å‡ºå·²å®‰è£…çš„Ollamaæ¨¡å‹åŠè¯¦ç»†ä¿¡æ¯
list_installed_models() {
	log_info "æ‰«æå·²å®‰è£…çš„æ¨¡å‹..."

	# åˆå§‹åŒ–ç¼“å­˜ä»¥æé«˜å®Œæ•´æ€§æ£€æŸ¥æ€§èƒ½
	ensure_cache_initialized

	# æ£€æŸ¥Ollamaæ¨¡å‹ç›®å½•æ˜¯å¦å­˜åœ¨
	if [[ ! -d ${OLLAMA_MODELS_DIR} ]]; then
		log_error "Ollamaæ¨¡å‹ç›®å½•ä¸å­˜åœ¨: ${OLLAMA_MODELS_DIR}"
		return 1
	fi

	local manifests_base_dir="${OLLAMA_MODELS_DIR}/manifests"

	# æ£€æŸ¥manifestsåŸºç¡€ç›®å½•æ˜¯å¦å­˜åœ¨
	if [[ ! -d ${manifests_base_dir} ]]; then
		log_warning "æœªå‘ç°å·²å®‰è£…çš„æ¨¡å‹"
		return 0
	fi

	echo ""
	echo "=================================================================================="
	echo "                             å·²å®‰è£…çš„Ollamaæ¨¡å‹"
	echo "=================================================================================="
	echo ""

	local model_count=0
	local total_size=0
	local total_version_count=0

	# é€’å½’æŸ¥æ‰¾æ‰€æœ‰ manifest æ–‡ä»¶
	local manifest_files=()
	while IFS= read -r -d '' manifest_file; do
		manifest_files+=("${manifest_file}")
	done < <(find "${manifests_base_dir}" -type f -print0 2>/dev/null || true)

	# æŒ‰æ¨¡å‹ç»„ç»‡ manifest æ–‡ä»¶
	declare -A model_manifests

	for manifest_file in "${manifest_files[@]}"; do
		# æå–ç›¸å¯¹äº manifests_base_dir çš„è·¯å¾„
		local relative_path="${manifest_file#"${manifests_base_dir}"/}"

		# æ ¹æ®è·¯å¾„ç»“æ„æå–æ¨¡å‹åå’Œç‰ˆæœ¬
		local model_name=""
		local version=""
		local full_model_path=""

		if [[ ${relative_path} =~ ^registry\.ollama\.ai/library/([^/]+)/(.+)$ ]]; then
			# ä¼ ç»Ÿ Ollama æ¨¡å‹: registry.ollama.ai/library/model_name/version
			model_name="${BASH_REMATCH[1]}"
			version="${BASH_REMATCH[2]}"
			full_model_path="registry.ollama.ai/library/${model_name}"
		elif [[ ${relative_path} =~ ^hf\.co/([^/]+)/([^/]+)/(.+)$ ]]; then
			# HF-GGUF æ¨¡å‹: hf.co/user/repo/version
			local user="${BASH_REMATCH[1]}"
			local repo="${BASH_REMATCH[2]}"
			version="${BASH_REMATCH[3]}"
			model_name="hf.co/${user}/${repo}"
			full_model_path="hf.co/${user}/${repo}"
		else
			# å…¶ä»–æœªçŸ¥æ ¼å¼ï¼Œå°è¯•é€šç”¨è§£æ
			local path_parts
			IFS='/' read -ra path_parts <<<"${relative_path}"
			if [[ ${#path_parts[@]} -ge 2 ]]; then
				version="${path_parts[-1]}"
				unset 'path_parts[-1]'
				model_name=$(
					IFS='/'
					echo "${path_parts[*]}"
				)
				full_model_path="${model_name}"
			else
				continue
			fi
		fi

		# å°† manifest æ·»åŠ åˆ°å¯¹åº”æ¨¡å‹ç»„
		if [[ -n ${model_name} && -n ${version} ]]; then
			local key="${model_name}"
			if [[ -z ${model_manifests[${key}]-} ]]; then
				model_manifests[${key}]="${manifest_file}|${version}|${full_model_path}"
			else
				model_manifests[${key}]="${model_manifests[${key}]};;${manifest_file}|${version}|${full_model_path}"
			fi
		fi
	done

	# æ˜¾ç¤ºæ¯ä¸ªæ¨¡å‹çš„ä¿¡æ¯
	for model_name in "${!model_manifests[@]}"; do
		local model_data="${model_manifests[${model_name}]}"

		# è§£æç¬¬ä¸€ä¸ªæ¡ç›®ä»¥è·å–è·¯å¾„ä¿¡æ¯
		local first_entry="${model_data%%;*}"
		local full_model_path="${first_entry##*|}"
		local model_dir="${manifests_base_dir}/${full_model_path}"

		echo "ğŸ“¦ æ¨¡å‹: ${model_name}"
		[[ ${VERBOSE} == "true" ]] && echo "   â”œâ”€ ä½ç½®: ${model_dir}"

		local version_count=0

		# å¤„ç†æ‰€æœ‰ç‰ˆæœ¬
		IFS=';;' read -ra entries <<<"${model_data}"
		for entry in "${entries[@]}"; do
			IFS='|' read -r manifest_file version _ <<<"${entry}"

			if [[ ! -f ${manifest_file} ]]; then
				continue
			fi

			# æ£€æŸ¥æ¨¡å‹å®Œæ•´æ€§ï¼ˆä½¿ç”¨ç¼“å­˜ä¼˜åŒ–ï¼‰
			local integrity_status=""
			local check_model_spec="${model_name}:${version}"
			if verify_integrity "model" "${check_model_spec}" "use_cache:true,check_blobs:true"; then
				integrity_status=" âœ“(å®Œæ•´)"
			else
				integrity_status=" âš ï¸(ä¸å®Œæ•´)"
			fi

			echo "   â”œâ”€ ç‰ˆæœ¬: ${version}${integrity_status}"

			# è¯»å–manifestæ–‡ä»¶è·å–blobä¿¡æ¯
			if [[ ${VERBOSE} == "true" ]] && [[ -f ${manifest_file} ]]; then
				local manifest_content
				if manifest_content=$(cat "${manifest_file}" 2>/dev/null); then
					# manifestæ˜¯JSONæ ¼å¼ï¼Œè§£æè·å–æ‰€æœ‰å±‚çš„å¤§å°
					local total_model_size=0
					local blob_count=0
					local model_type="æœªçŸ¥"

					# å°è¯•ä»JSONä¸­æå–æ¨¡å‹ç±»å‹
					if echo "${manifest_content}" | grep -q "application/vnd.ollama.image.model"; then
						model_type="Ollamaæ¨¡å‹"
					fi

					# æå–configå¤§å°
					local config_size
					if config_size=$(echo "${manifest_content}" | grep -o '"config":{[^}]*"size":[0-9]*' | grep -o '[0-9]*$' 2>/dev/null); then
						total_model_size=$((total_model_size + config_size))
						blob_count=$((blob_count + 1))
					fi

					# æå–æ‰€æœ‰layersçš„å¤§å°
					local layer_sizes
					if layer_sizes=$(echo "${manifest_content}" | grep -o '"size":[0-9]*' | grep -o '[0-9]*' 2>/dev/null); then
						while IFS= read -r size; do
							if [[ -n ${size} && ${size} -gt 0 ]]; then
								total_model_size=$((total_model_size + size))
								blob_count=$((blob_count + 1))
							fi
						done <<<"${layer_sizes}"
					fi

					# æ ¼å¼åŒ–å¤§å°æ˜¾ç¤º
					local human_size
					human_size=$(format_bytes "${total_model_size}")

					echo "   â”œâ”€ å¤§å°: ${human_size}"

					total_size=$((total_size + total_model_size))
				fi
			fi

			version_count=$((version_count + 1))
		done

		echo "   â””â”€ ç‰ˆæœ¬æ•°é‡: ${version_count}"
		echo ""
		model_count=$((model_count + 1))
		total_version_count=$((total_version_count + version_count))
	done

	# æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
	echo "=================================================================================="
	echo "ç»Ÿè®¡ä¿¡æ¯:"
	echo "  ğŸ“Š æ€»æ¨¡å‹æ•°: ${model_count}"
	echo "  ğŸ”¢ æ€»ç‰ˆæœ¬æ•°: ${total_version_count}"

	# æ ¼å¼åŒ–æ€»å¤§å°
	if [[ ${VERBOSE} == "true" ]]; then
		local total_human_size
		total_human_size=$(format_bytes "${total_size}")
		echo "  ğŸ’¾ å¤§å°: ${total_human_size}"
	fi
	echo "  ğŸ“ ç›®å½•: ${OLLAMA_MODELS_DIR}"

	# æ˜¾ç¤ºç£ç›˜ä½¿ç”¨æƒ…å†µ
	local disk_usage
	if disk_usage=$(du -sh "${OLLAMA_MODELS_DIR}" 2>/dev/null || true); then
		local disk_size
		disk_size=$(echo "${disk_usage}" | cut -f1)
		echo "  ğŸ—„ï¸ Disk usage: ${disk_size}"
	fi

	echo "=================================================================================="
	echo ""

	return 0
}

# å¤‡ä»½Ollamaæ¨¡å‹ï¼ˆç›´æ¥å¤åˆ¶ï¼‰
backup_ollama_model() {
	local model_spec="$1"
	local backup_dir="$2"

	# åˆå§‹åŒ–ç¼“å­˜ä»¥æé«˜å®Œæ•´æ€§æ£€æŸ¥æ€§èƒ½
	ensure_cache_initialized

	# è§£ææ¨¡å‹åç§°å’Œç‰ˆæœ¬
	local model_name model_version
	if ! parse_model_spec "${model_spec}" model_name model_version; then
		return 1
	fi

	log_verbose "å¤‡ä»½æ¨¡å‹: ${model_name}:${model_version}"
	local model_spec="${model_name}:${model_version}"
	if ! verify_integrity "model" "${model_spec}" "use_cache:true,check_blobs:true"; then
		log_error "æœ¬åœ°æ¨¡å‹ä¸å®Œæ•´ï¼Œå–æ¶ˆå¤‡ä»½æ“ä½œ"
		return 1
	fi

	# åˆ›å»ºå¤‡ä»½ç›®å½•å’Œç”Ÿæˆè·¯å¾„
	local model_backup_dir
	model_backup_dir=$(create_model_backup_dir "${model_spec}" "${backup_dir}") || return 1
	local model_safe_name
	model_safe_name=$(get_safe_model_name "${model_spec}")
	local backup_model_dir="${model_backup_dir}/${model_safe_name}"

	# æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å¤‡ä»½ç›®å½•
	if [[ -d ${backup_model_dir} ]]; then
		log_success "æ¨¡å‹å¤‡ä»½å·²å­˜åœ¨"
		return 0
	fi

	# ç¡®å®šmanifestæ–‡ä»¶è·¯å¾„
	local manifest_file
	if [[ ${model_name} == hf.co/* ]]; then
		# HuggingFace GGUFæ¨¡å‹ï¼Œå¦‚ hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF
		manifest_file="${OLLAMA_MODELS_DIR}/manifests/${model_name}/${model_version}"
	elif [[ ${model_name} == *"/"* ]]; then
		# ç”¨æˆ·åˆ†äº«çš„æ¨¡å‹ï¼Œå¦‚ lrs33/bce-embedding-base_v1
		local user_name="${model_name%/*}"
		local repo_name="${model_name#*/}"
		manifest_file="${OLLAMA_MODELS_DIR}/manifests/registry.ollama.ai/${user_name}/${repo_name}/${model_version}"
	else
		# å®˜æ–¹æ¨¡å‹
		manifest_file="${OLLAMA_MODELS_DIR}/manifests/registry.ollama.ai/library/${model_name}/${model_version}"
	fi

	# æ£€æŸ¥manifestæ–‡ä»¶æ˜¯å¦å­˜åœ¨
	if [[ ! -f ${manifest_file} ]]; then
		log_error "æ¨¡å‹ä¸å­˜åœ¨: ${model_spec}"
		return 1
	fi

	# è·å–blobæ–‡ä»¶è·¯å¾„
	local blob_files
	blob_files=$(get_model_blob_paths "${manifest_file}" "${OLLAMA_MODELS_DIR}")

	if [[ -z ${blob_files} ]]; then
		log_error "æœªæ‰¾åˆ°æ¨¡å‹ç›¸å…³çš„blobæ–‡ä»¶"
		return 1
	fi

	# åˆ›å»ºå¤‡ä»½ç›®å½•ç»“æ„
	mkdir -p "${backup_model_dir}/manifests"
	mkdir -p "${backup_model_dir}/blobs"

	log_verbose "å¼€å§‹å¤åˆ¶æ–‡ä»¶..."

	# å¤åˆ¶manifestæ–‡ä»¶
	local manifest_rel_path="${manifest_file#"${OLLAMA_MODELS_DIR}"/manifests/}"
	local manifest_backup_dir
	manifest_backup_dir="${backup_model_dir}/manifests/$(dirname "${manifest_rel_path}")"
	mkdir -p "${manifest_backup_dir}"
	if ! cp "${manifest_file}" "${manifest_backup_dir}/"; then
		log_error "å¤åˆ¶manifestæ–‡ä»¶å¤±è´¥: ${manifest_file}"
		rm -rf "${backup_model_dir}"
		return 1
	fi

	# å¤åˆ¶blobæ–‡ä»¶
	while IFS= read -r blob_file; do
		if [[ -f ${blob_file} ]]; then
			local blob_name
			blob_name=$(basename "${blob_file}")
			if ! cp "${blob_file}" "${backup_model_dir}/blobs/"; then
				log_error "å¤åˆ¶blobæ–‡ä»¶å¤±è´¥: ${blob_file}"
				rm -rf "${backup_model_dir}"
				return 1
			fi
		fi
	done <<<"${blob_files}"

	# è®¡ç®—MD5æ ¡éªŒ
	log_verbose "è®¡ç®—MD5æ ¡éªŒå€¼..."
	local md5_file="${backup_model_dir}.md5"
	if calculate_directory_md5 "${backup_model_dir}" "${md5_file}"; then
		log_verbose "MD5æ ¡éªŒæ–‡ä»¶å·²åˆ›å»º: ${md5_file}"
	else
		log_warning "MD5æ ¡éªŒæ–‡ä»¶åˆ›å»ºå¤±è´¥"
	fi

	# åˆ›å»ºå¤‡ä»½ä¿¡æ¯æ–‡ä»¶
	create_backup_info "${model_spec}" "${backup_model_dir}" "directory" 1 "ollama"

	log_verbose_success "æ¨¡å‹å¤‡ä»½å®Œæˆ: ${model_spec}"
	return 0
}

# æ™ºèƒ½åˆ é™¤æ¨¡å‹ï¼ˆè‡ªåŠ¨è¯†åˆ«æ¨¡å‹ç±»å‹ï¼‰
remove_model_smart() {
	local model_input="$1"
	local force_delete="${2:-false}"

	log_info "åˆ é™¤æ¨¡å‹: ${model_input}"

	# æ£€æŸ¥è¾“å…¥æ ¼å¼ï¼Œåˆ¤æ–­æ˜¯ä»€ä¹ˆç±»å‹çš„æ¨¡å‹
	if [[ ${model_input} =~ ^([^:]+):(.+)$ ]]; then
		local model_name="${BASH_REMATCH[1]}"
		local model_tag_or_quant="${BASH_REMATCH[2]}"

		# å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯Ollamaæ¨¡å‹ï¼ˆç›´æ¥æ ¼å¼ï¼šmodel:tagï¼‰
		if check_ollama_model "${model_name}" "${model_tag_or_quant}"; then
			if remove_ollama_model "${model_input}" "${force_delete}"; then
				return 0
			else
				return 1
			fi
		fi

		# Model not found
		log_error "Model not found or unsupported format: ${model_input}"
		return 1

	else
		log_error "æ¨¡å‹æ ¼å¼é”™è¯¯ï¼Œåº”ä¸º 'æ¨¡å‹å:ç‰ˆæœ¬' æˆ– 'æ¨¡å‹å:é‡åŒ–ç±»å‹'"
		log_error "ä¾‹å¦‚: 'llama2:7b' æˆ– 'microsoft/DialoGPT-small:q4_0'"
		return 1
	fi
}

# æ£€æµ‹å¤‡ä»½æ–‡ä»¶ç±»å‹

# æ¢å¤Ollamaæ¨¡å‹ï¼ˆç›®å½•å¤‡ä»½ï¼‰
restore_ollama_model() {
	local backup_dir="$1"
	local force_restore="$2"

	log_info "æ¢å¤æ¨¡å‹: $(basename "${backup_dir}")"

	# æ£€æŸ¥å¤‡ä»½ç›®å½•æ˜¯å¦å­˜åœ¨
	if [[ ! -d ${backup_dir} ]]; then
		log_error "å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: ${backup_dir}"
		return 1
	fi

	# æ£€æŸ¥å¤‡ä»½ç›®å½•ç»“æ„
	if [[ ! -d "${backup_dir}/manifests" ]] || [[ ! -d "${backup_dir}/blobs" ]]; then
		log_error "å¤‡ä»½æ–‡ä»¶æŸåæˆ–æ ¼å¼é”™è¯¯"
		return 1
	fi

	# MD5æ ¡éªŒ
	local md5_file="${backup_dir}.md5"
	if [[ -f ${md5_file} ]]; then
		log_info "æ ¡éªŒå¤‡ä»½æ–‡ä»¶..."
		if verify_directory_md5 "${backup_dir}" "${md5_file}"; then
			log_verbose_success "MD5æ ¡éªŒé€šè¿‡"
		else
			log_error "å¤‡ä»½æ–‡ä»¶æ ¡éªŒå¤±è´¥ï¼Œå¯èƒ½å·²æŸå"
			if [[ ${force_restore} != "true" ]]; then
				return 1
			fi
			log_warning "å¼ºåˆ¶æ¢å¤æ¨¡å¼ï¼Œç»§ç»­æ“ä½œ..."
		fi
	else
		log_warning "è·³è¿‡å®Œæ•´æ€§æ ¡éªŒ"
	fi

	# æ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶è¦†ç›–
	if [[ ${force_restore} != "true" ]]; then
		log_info "æ£€æŸ¥æ¨¡å‹å†²çª..."
		local conflicts_found=false

		# æ£€æŸ¥manifestså†²çª
		if find "${backup_dir}/manifests" -type f 2>/dev/null | while read -r manifest_file; do
			local rel_path="${manifest_file#"${backup_dir}"/manifests/}"
			local target_file="${OLLAMA_MODELS_DIR}/manifests/${rel_path}"
			if [[ -f ${target_file} ]]; then
				echo "conflict"
				break
			fi
		done | grep -q "conflict"; then
			conflicts_found=true
		fi

		# æ£€æŸ¥blobså†²çª
		if find "${backup_dir}/blobs" -type f 2>/dev/null | while read -r blob_file; do
			local blob_name
			blob_name=$(basename "${blob_file}")
			local target_file="${OLLAMA_MODELS_DIR}/blobs/${blob_name}"
			if [[ -f ${target_file} ]]; then
				echo "conflict"
				break
			fi
		done | grep -q "conflict"; then
			conflicts_found=true
		fi

		if [[ ${conflicts_found} == "true" ]]; then
			log_error "æ£€æµ‹åˆ°æ–‡ä»¶å†²çªï¼Œä½¿ç”¨ --force å¼ºåˆ¶è¦†ç›–"
			return 1
		fi
	fi

	# åˆ›å»ºOllamaç›®å½•å¹¶è®¾ç½®æƒé™
	if ! mkdir -p "${OLLAMA_MODELS_DIR}/manifests" "${OLLAMA_MODELS_DIR}/blobs"; then
		log_error "Unable toåˆ›å»ºOllamaç›®å½•"
		return 1
	fi

	# å¤åˆ¶manifests
	log_verbose "æ¢å¤æ¨¡å‹ä¿¡æ¯..."
	if ! cp -r "${backup_dir}/manifests/"* "${OLLAMA_MODELS_DIR}/manifests/"; then
		log_error "manifestæ–‡ä»¶å¤åˆ¶å¤±è´¥"
		return 1
	fi

	# å¤åˆ¶blobs
	log_verbose "æ¢å¤æ¨¡å‹æ•°æ®..."
	if ! cp "${backup_dir}/blobs/"* "${OLLAMA_MODELS_DIR}/blobs/"; then
		log_error "blobæ–‡ä»¶å¤åˆ¶å¤±è´¥"
		return 1
	fi

	log_verbose_success "æ¨¡å‹æ¢å¤å®Œæˆ"
	return 0
}

# è‡ªåŠ¨è¯†åˆ«å¤‡ä»½ç±»å‹å¹¶æ¢å¤
# æ‰¹é‡å¤‡ä»½æ¨¡å‹ï¼ˆæ ¹æ®models.listæ–‡ä»¶ï¼‰
backup_models_from_list() {
	local models_file="$1"
	local backup_dir="$2"

	log_verbose "æ‰¹é‡å¤‡ä»½æ¨¡å‹..."
	log_verbose "æ¨¡å‹åˆ—è¡¨æ–‡ä»¶: ${models_file}"
	log_verbose "å¤‡ä»½ç›®å½•: ${backup_dir}"

	# è§£ææ¨¡å‹åˆ—è¡¨
	local models=()
	parse_models_list "${models_file}" models

	if [[ ${#models[@]} -eq 0 ]]; then
		log_warning "æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡å‹è¿›è¡Œå¤‡ä»½"
		return 1
	fi

	# åˆ›å»ºå¤‡ä»½ç›®å½•
	mkdir -p "${backup_dir}"

	local total_models=${#models[@]}
	local processed=0
	local success=0
	local failed=0

	log_verbose "å…±æ‰¾åˆ° ${total_models} ä¸ªæ¨¡å‹è¿›è¡Œå¤‡ä»½"

	# é¢„å…ˆåˆå§‹åŒ–Ollamaç¼“å­˜ï¼Œé¿å…æ¯ä¸ªæ¨¡å‹éƒ½é‡æ–°åˆå§‹åŒ–
	local has_ollama_models=false
	for model in "${models[@]}"; do
		if [[ ${model} =~ ^ollama: ]] || [[ ${model} =~ ^hf-gguf: ]]; then
			has_ollama_models=true
			break
		fi
	done

	if [[ ${has_ollama_models} == "true" ]]; then
		log_verbose "æ£€æµ‹åˆ°Ollamaæ¨¡å‹ï¼Œé¢„å…ˆåˆå§‹åŒ–æ¨¡å‹ç¼“å­˜..."
		if ! init_ollama_cache; then
			log_error "Ollamaç¼“å­˜åˆå§‹åŒ–å¤±è´¥ï¼Œå¯èƒ½å½±å“å¤‡ä»½æ€§èƒ½"
		fi
	fi

	for model in "${models[@]}"; do
		((processed++))
		log_info "å¤‡ä»½æ¨¡å‹ [${processed}/${total_models}]: ${model}"

		# è§£ææ¨¡å‹æ¡ç›®
		if [[ ${model} =~ ^ollama:([^:]+):(.+)$ ]]; then
			local model_name="${BASH_REMATCH[1]}"
			local model_tag="${BASH_REMATCH[2]}"
			local model_spec="${model_name}:${model_tag}"

			# æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
			if check_ollama_model "${model_name}" "${model_tag}"; then
				if backup_ollama_model "${model_spec}" "${backup_dir}"; then
					((success++))
				else
					((failed++))
				fi
			else
				((failed++))
			fi

		elif [[ ${model} =~ ^hf-gguf:(.+)$ ]]; then
			local model_full_name="${BASH_REMATCH[1]}"

			# è§£æHuggingFace GGUFæ¨¡å‹åç§°
			if [[ ${model_full_name} =~ ^(.+):(.+)$ ]]; then
				local model_name="${BASH_REMATCH[1]}"
				local model_tag="${BASH_REMATCH[2]}"
			else
				local model_name="${model_full_name}"
				local model_tag="latest"
			fi

			local model_spec="${model_name}:${model_tag}"

			# æ£€æŸ¥HF GGUFæ¨¡å‹æ˜¯å¦å­˜åœ¨
			if check_hf_gguf_model "${model_name}" "${model_tag}"; then
				if backup_ollama_model "${model_spec}" "${backup_dir}"; then
					((success++))
				else
					((failed++))
				fi
			else
				((failed++))
			fi

		else
			log_error "æ— æ•ˆçš„æ¨¡å‹æ¡ç›®æ ¼å¼: ${model}"
			((failed++))
		fi

		echo "" # æ·»åŠ ç©ºè¡Œåˆ†éš”
	done

	# æ˜¾ç¤ºå¤‡ä»½æ€»ç»“
	log_verbose_success "æ‰¹é‡å¤‡ä»½å®Œæˆ (${success}/${total_models})"
	if [[ ${failed} -gt 0 ]]; then
		log_warning "å¤‡ä»½å¤±è´¥: ${failed}"
		return 1
	fi

	# æ˜¾ç¤ºå¤‡ä»½ç›®å½•ä¿¡æ¯
	if [[ ${VERBOSE} == "true" ]] && [[ -d ${backup_dir} ]]; then
		# åªç»Ÿè®¡é¡¶çº§æ¨¡å‹ç›®å½•ï¼Œæ’é™¤å­ç›®å½•
		local backup_count
		backup_count=$(find "${backup_dir}" -maxdepth 1 -type d ! -path "${backup_dir}" | wc -l)
		local total_size
		total_size=$(du -sh "${backup_dir}" 2>/dev/null | cut -f1)
		log_info "å¤‡ä»½ç›®å½•ä¸‹å…±æœ‰: ${backup_count} ä¸ªæ¨¡å‹ï¼Œæ€»å¤§å°: ${total_size}"
	fi

	# æ¸…ç†å®Œæ•´æ€§æ£€æŸ¥ç¼“å­˜
	clear_integrity_cache

	if [[ ${failed} -eq 0 ]]; then
		log_verbose_success "å…¨éƒ¨æ¨¡å‹å¤‡ä»½å®Œæˆ"
		return 0
	else
		log_warning "éƒ¨åˆ†æ¨¡å‹å¤‡ä»½å¤±è´¥"
		return 1
	fi
}

# æ‰¹é‡åˆ é™¤æ¨¡å‹ï¼ˆæ ¹æ®models.listæ–‡ä»¶ï¼‰
remove_models_from_list() {
	local models_file="$1"
	local force_delete="${2:-false}"

	log_verbose "Batch deleting models..."
	log_verbose "Model list file: ${models_file}"
	log_info "Force delete mode: ${force_delete}"

	# è§£ææ¨¡å‹åˆ—è¡¨
	local models=()
	parse_models_list "${models_file}" models

	if [[ ${#models[@]} -eq 0 ]]; then
		log_warning "No models found for deletion"
		return 1
	fi

	local total_models=${#models[@]}
	local processed=0
	local success=0
	local failed=0

	log_verbose "å…±æ‰¾åˆ° ${total_models} ä¸ªæ¨¡å‹è¿›è¡Œåˆ é™¤"

	# å¦‚æœä¸æ˜¯å¼ºåˆ¶åˆ é™¤ï¼Œæ˜¾ç¤ºè¦åˆ é™¤çš„æ¨¡å‹åˆ—è¡¨å¹¶è¯·æ±‚ç¡®è®¤
	if [[ ${force_delete} != "true" ]]; then
		log_warning "The following models will be deleted:"
		for model in "${models[@]}"; do
			if [[ ${model} =~ ^ollama:([^:]+):(.+)$ ]]; then
				local model_name="${BASH_REMATCH[1]}"
				local model_tag="${BASH_REMATCH[2]}"
				echo "  - Ollamaæ¨¡å‹: ${model_name}:${model_tag}"
			elif [[ ${model} =~ ^hf-gguf:(.+)$ ]]; then
				local model_full_name="${BASH_REMATCH[1]}"
				echo "  - HuggingFace GGUFæ¨¡å‹: ${model_full_name}"
			fi
		done
		echo ""
		echo -n "Confirm deletion of all these models? [y/N]: "
		read -r confirm
		if [[ ${confirm} != "y" && ${confirm} != "Y" ]]; then
			log_info "Cancelled batch delete operation"
			return 2 # ç‰¹æ®Šé€€å‡ºç è¡¨ç¤ºç”¨æˆ·å–æ¶ˆ
		fi
		echo ""
	fi

	for model in "${models[@]}"; do
		((processed++))
		log_info "Deleting model [${processed}/${total_models}]: ${model}"

		# è§£ææ¨¡å‹æ¡ç›®
		if [[ ${model} =~ ^ollama:([^:]+):(.+)$ ]]; then
			local model_name="${BASH_REMATCH[1]}"
			local model_tag="${BASH_REMATCH[2]}"
			local model_spec="${model_name}:${model_tag}"

			log_verbose "åˆ é™¤Ollamaæ¨¡å‹: ${model_spec}"

			if remove_ollama_model "${model_spec}" "true"; then
				((success++))
				log_verbose_success "Ollamaæ¨¡å‹åˆ é™¤æˆåŠŸ: ${model_spec}"
			else
				((failed++))
				log_error "Ollamaæ¨¡å‹åˆ é™¤å¤±è´¥: ${model_spec}"
			fi

		elif [[ ${model} =~ ^hf-gguf:(.+)$ ]]; then
			local model_full_name="${BASH_REMATCH[1]}"

			# è§£æHuggingFace GGUFæ¨¡å‹åç§°
			if [[ ${model_full_name} =~ ^(.+):(.+)$ ]]; then
				local model_name="${BASH_REMATCH[1]}"
				local model_tag="${BASH_REMATCH[2]}"
			else
				local model_name="${model_full_name}"
				local model_tag="latest"
			fi

			local model_spec="${model_name}:${model_tag}"
			log_verbose "åˆ é™¤HuggingFace GGUFæ¨¡å‹: ${model_spec}"

			if remove_ollama_model "${model_spec}" "true"; then
				((success++))
				log_verbose_success "HuggingFace GGUFæ¨¡å‹åˆ é™¤æˆåŠŸ: ${model_spec}"
			else
				((failed++))
				log_error "HuggingFace GGUFæ¨¡å‹åˆ é™¤å¤±è´¥: ${model_spec}"
			fi

		else
			log_error "æ— æ•ˆçš„æ¨¡å‹æ¡ç›®æ ¼å¼: ${model}"
			((failed++))
		fi

		echo "" # æ·»åŠ ç©ºè¡Œåˆ†éš”
	done

	# æ˜¾ç¤ºåˆ é™¤æ€»ç»“
	log_verbose_success "æ‰¹é‡åˆ é™¤å®Œæˆ (${success}/${total_models})"
	if [[ ${failed} -gt 0 ]]; then
		log_warning "åˆ é™¤å¤±è´¥: ${failed}"
	fi

	if [[ ${failed} -eq 0 ]]; then
		log_verbose_success "å…¨éƒ¨æ¨¡å‹åˆ é™¤å®Œæˆ"
		return 0
	else
		log_warning "éƒ¨åˆ†æ¨¡å‹åˆ é™¤å¤±è´¥"
		return 1
	fi
}

# æ£€æŸ¥Ollamaä¸­æ˜¯å¦å­˜åœ¨æŒ‡å®šæ¨¡å‹

# æ£€æŸ¥Ollamaä¸­æ˜¯å¦å­˜åœ¨æŒ‡å®šæ¨¡å‹ï¼ˆé€šç”¨å‡½æ•°ï¼‰

# æ£€æŸ¥Ollamaæ¨¡å‹åœ¨backupsç›®å½•ä¸­æ˜¯å¦æœ‰å¤‡ä»½
check_ollama_backup_exists() {
	local model_name="$1"
	local model_tag="$2"

	# ä½¿ç”¨ä¸get_safe_model_nameç›¸åŒçš„é€»è¾‘ç”Ÿæˆå®‰å…¨åç§°
	local model_spec="${model_name}:${model_tag}"
	local model_safe_name
	model_safe_name=$(get_safe_model_name "${model_spec}")
	local backup_parent_dir="${BACKUP_OUTPUT_DIR}/${model_safe_name}"
	local backup_model_dir="${backup_parent_dir}/${model_safe_name}"

	# æ£€æŸ¥å¤‡ä»½ç›®å½•æ˜¯å¦å­˜åœ¨
	if [[ -d ${backup_model_dir} ]]; then
		# æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„ç›®å½•å¤‡ä»½ç»“æ„
		if [[ -d "${backup_model_dir}/manifests" ]] && [[ -d "${backup_model_dir}/blobs" ]]; then
			echo "${backup_parent_dir}"
			return 0
		fi
	fi

	return 1
}

# å°è¯•ä»å¤‡ä»½æ¢å¤Ollamaæ¨¡å‹
try_restore_ollama_from_backup() {
	local model_name="$1"
	local model_tag="$2"

	log_verbose "æ£€æŸ¥Ollamaæ¨¡å‹å¤‡ä»½: ${model_name}:${model_tag}"

	local backup_dir
	if backup_dir=$(check_ollama_backup_exists "${model_name}" "${model_tag}"); then
		log_verbose_success "æ‰¾åˆ°Ollamaæ¨¡å‹å¤‡ä»½: ${backup_dir}"

		# ä½¿ç”¨ä¸get_safe_model_nameç›¸åŒçš„é€»è¾‘ç”Ÿæˆå®‰å…¨åç§°
		local model_spec="${model_name}:${model_tag}"
		local model_safe_name
		model_safe_name=$(get_safe_model_name "${model_spec}")

		# æŸ¥æ‰¾å¤‡ä»½ç›®å½•ï¼ˆæ–°çš„ç›´æ¥å¤åˆ¶æ ¼å¼ï¼‰
		local backup_model_dir="${backup_dir}/${model_safe_name}"
		if [[ -d ${backup_model_dir} ]]; then
			# æ¢å¤æ¨¡å‹
			log_info "æ­£åœ¨ä»å¤‡ä»½æ¢å¤æ¨¡å‹..."
			if restore_ollama_model "${backup_model_dir}" "true"; then
				log_success "ä»å¤‡ä»½æˆåŠŸæ¢å¤æ¨¡å‹: ${model_name}:${model_tag}"
				return 0
			else
				log_warning "ä»å¤‡ä»½æ¢å¤æ¨¡å‹å¤±è´¥ï¼Œå°†å°è¯•é‡æ–°ä¸‹è½½"
				return 1
			fi
		else
			log_error "æœªæ‰¾åˆ°æœ‰æ•ˆçš„å¤‡ä»½ç›®å½•: ${backup_model_dir}"
			return 1
		fi
	else
		log_verbose "æœªæ‰¾åˆ°Ollamaæ¨¡å‹å¤‡ä»½"
		return 1
	fi
}

# å¤„ç†å•ä¸ªæ¨¡å‹
process_model() {
	local model_entry="$1"
	local force_download="$2"
	local check_only="$3"

	# è§£ææ¨¡å‹æ¡ç›®
	local -A model_info
	if ! parse_model_entry "${model_entry}" model_info; then
		log_error "æ— æ•ˆçš„æ¨¡å‹æ¡ç›®æ ¼å¼: ${model_entry}"
		return 1
	fi

	log_verbose "å¤„ç†æ¨¡å‹: ${model_info[display]}"

	# æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
	if [[ ${force_download} != "true" ]] && check_model_exists model_info; then
		log_success "æ¨¡å‹å·²å­˜åœ¨"
		return 0
	fi

	# æ¨¡å‹ä¸å­˜åœ¨æˆ–å¼ºåˆ¶ä¸‹è½½
	if [[ ${check_only} == "true" ]]; then
		log_warning "éœ€è¦ä¸‹è½½: ${model_info[display]}"
		return 0
	fi

	# å°è¯•ä»å¤‡ä»½æ¢å¤
	if try_restore_model model_info; then
		log_success "ä»å¤‡ä»½æ¢å¤æˆåŠŸ"
		# æ¸…é™¤ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°æ£€æŸ¥
		OLLAMA_CACHE_INITIALIZED="false"
		OLLAMA_MODELS_CACHE=""
		return 0
	fi

	# æ‰§è¡Œä¸‹è½½
	if download_model model_info; then
		log_success "æ¨¡å‹ä¸‹è½½å®Œæˆ"
		# æ¸…é™¤ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°æ£€æŸ¥
		OLLAMA_CACHE_INITIALIZED="false"
		OLLAMA_MODELS_CACHE=""
		return 0
	else
		log_error "æ¨¡å‹å¤„ç†å¤±è´¥: ${model_info[display]}"
		return 1
	fi
}

# ä¸»å‡½æ•°
main() {
	# è·å–ä¸»æœºæ—¶åŒº
	HOST_TIMEZONE=$(get_host_timezone)

	# æ£€æŸ¥å‚æ•° - æ”¯æŒhelpåœ¨ä»»ä½•ä½ç½®
	for arg in "$@"; do
		if [[ ${arg} == "--help" || ${arg} == "-h" ]]; then
			show_help
			exit 0
		fi
	done

	# é»˜è®¤å€¼
	CHECK_ONLY="true"
	FORCE_DOWNLOAD="false"
	BACKUP_MODEL=""
	BACKUP_ALL="false"
	LIST_MODELS="false"
	RESTORE_FILE=""
	GENERATE_COMPOSE="false"
	FORCE_RESTORE="false"
	REMOVE_MODEL=""
	REMOVE_ALL="false"

	# è§£æå‘½ä»¤è¡Œå‚æ•°
	while [[ $# -gt 0 ]]; do
		case $1 in
		--models-file)
			MODELS_FILE="$2"
			shift 2
			;;
		--ollama-dir)
			# å¤„ç†ç”¨æˆ·æŒ‡å®šçš„Ollamaç›®å½•
			local user_ollama_dir="$2"
			user_ollama_dir="${user_ollama_dir%/}" # ç§»é™¤æœ«å°¾æ–œæ 

			# è®¾ç½®æ•°æ®ç›®å½•å’Œæ¨¡å‹ç›®å½•
			if [[ ${user_ollama_dir} == */models ]]; then
				OLLAMA_MODELS_DIR="${user_ollama_dir}"
				OLLAMA_DATA_DIR="${user_ollama_dir%/models}"
			else
				OLLAMA_DATA_DIR="${user_ollama_dir}"
				OLLAMA_MODELS_DIR="${user_ollama_dir}/models"
			fi
			shift 2
			;;
		--backup)
			BACKUP_MODEL="$2"
			shift 2
			;;
		--backup-all)
			BACKUP_ALL="true"
			shift
			;;
		--list)
			LIST_MODELS="true"
			shift
			;;
		--restore)
			RESTORE_FILE="$2"
			shift 2
			;;
		--remove)
			REMOVE_MODEL="$2"
			shift 2
			;;
		--remove-all)
			REMOVE_ALL="true"
			shift
			;;
		--backup-dir)
			BACKUP_OUTPUT_DIR="$2"
			shift 2
			;;
		--check-only)
			CHECK_ONLY="true"
			shift
			;;
		--install)
			CHECK_ONLY="false"
			shift
			;;
		--force-download)
			FORCE_DOWNLOAD="true"
			CHECK_ONLY="false" # å¼ºåˆ¶ä¸‹è½½æ—¶åº”è¯¥å®é™…æ‰§è¡Œä¸‹è½½
			shift
			;;
		--force)
			FORCE_RESTORE="true"
			shift
			;;
		--verbose)
			VERBOSE="true"
			shift
			;;
		--generate-compose)
			GENERATE_COMPOSE="true"
			shift
			;;
		*)
			log_error "æœªçŸ¥å‚æ•°: $1"
			show_help
			exit 1
			;;
		esac
	done

	# æ˜¾ç¤ºå½“å‰ä»»åŠ¡ï¼ˆç®€åŒ–ï¼‰
	local current_task=""
	if [[ -n ${BACKUP_MODEL} ]]; then
		current_task="Backup model: ${BACKUP_MODEL}"
	elif [[ ${BACKUP_ALL} == "true" ]]; then
		current_task="Batch backup all models"
	elif [[ -n ${RESTORE_FILE} ]]; then
		current_task="Restore model: ${RESTORE_FILE}"
	elif [[ -n ${REMOVE_MODEL} ]]; then
		current_task="Remove model: ${REMOVE_MODEL}"
	elif [[ ${REMOVE_ALL} == "true" ]]; then
		current_task="Batch remove all models"
	elif [[ ${LIST_MODELS} == "true" ]]; then
		current_task="List installed models"
	elif [[ ${GENERATE_COMPOSE} == "true" ]]; then
		current_task="Generate Docker Compose configuration"
	elif [[ ${CHECK_ONLY} == "true" ]]; then
		current_task="Check model status"
	else
		current_task="Install/download models"
	fi

	log_info "ğŸš€ Task: ${current_task}"
	log_verbose "Model list file: ${MODELS_FILE}"
	log_verbose "Ollama directory: ${OLLAMA_MODELS_DIR}"
	[[ -n ${BACKUP_OUTPUT_DIR} ]] && log_verbose "Backup directory: ${BACKUP_OUTPUT_DIR}"

	# åˆå§‹åŒ–è·¯å¾„
	init_paths

	# ç¡®ä¿Ollamaç›®å½•å­˜åœ¨
	if [[ ! -d ${OLLAMA_MODELS_DIR} ]]; then
		log_verbose "åˆ›å»ºOllamaæ¨¡å‹ç›®å½•..."
		if ! mkdir -p "${OLLAMA_MODELS_DIR}" 2>/dev/null; then
			log_warning "æ— æ³•åˆ›å»ºOllamaæ¨¡å‹ç›®å½•ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨"
		fi
	fi

	# æ‰§è¡Œç‰¹å®šä»»åŠ¡å¹¶é€€å‡º
	if [[ -n ${BACKUP_MODEL} ]]; then
		execute_task "model backup" backup_single_model "${BACKUP_MODEL}" "${BACKUP_OUTPUT_DIR}"
	elif [[ ${BACKUP_ALL} == "true" ]]; then
		execute_task "batch backup" backup_models_from_list "${MODELS_FILE}" "${BACKUP_OUTPUT_DIR}"
	elif [[ ${LIST_MODELS} == "true" ]]; then
		execute_task "model list" list_installed_models
	elif [[ ${GENERATE_COMPOSE} == "true" ]]; then
		execute_task "Docker configuration generation" generate_docker_compose
	elif [[ -n ${RESTORE_FILE} ]]; then
		execute_task "model restore" restore_model "${RESTORE_FILE}" "${FORCE_RESTORE}"
	elif [[ -n ${REMOVE_MODEL} ]]; then
		execute_task "model removal" remove_model_smart "${REMOVE_MODEL}" "${FORCE_RESTORE}"
	elif [[ ${REMOVE_ALL} == "true" ]]; then
		execute_task "batch delete" remove_models_from_list "${MODELS_FILE}" "${FORCE_RESTORE}"
	fi

	# æ£€æŸ¥ä¾èµ–
	check_dependencies

	# è§£ææ¨¡å‹åˆ—è¡¨
	local models=()
	parse_models_list "${MODELS_FILE}" models

	if [[ ${#models[@]} -eq 0 ]]; then
		log_warning "æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡å‹ï¼Œé€€å‡º"
		exit 0
	fi

	# HuggingFace GGUF models are downloaded directly through Ollama, no Docker images needed

	# å¤„ç†æ¯ä¸ªæ¨¡å‹
	local total_models=${#models[@]}
	local processed=0
	local failed=0

	for model in "${models[@]}"; do
		processed=$((processed + 1))
		log_verbose "å¤„ç†æ¨¡å‹ [${processed}/${total_models}]: ${model}"

		# å¤„ç†å•ä¸ªæ¨¡å‹é”™è¯¯ï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
		if ! process_model "${model}" "${FORCE_DOWNLOAD}" "${CHECK_ONLY}"; then
			failed=$((failed + 1))
		fi
	done

	# æ˜¾ç¤ºæ€»ç»“
	log_info "=== å¤„ç†å®Œæˆ ==="
	log_info "æ€»æ¨¡å‹æ•°: ${total_models}"
	log_info "å·²å¤„ç†: ${processed}"
	if [[ ${failed} -gt 0 ]]; then
		log_warning "å¤±è´¥: ${failed}"
	else
		log_success "å…¨éƒ¨æˆåŠŸå®Œæˆ"
	fi

	if [[ ${CHECK_ONLY} == "true" ]]; then
		log_info "æ£€æŸ¥æ¨¡å¼å®Œæˆï¼Œæœªæ‰§è¡Œå®é™…ä¸‹è½½"
	fi
}

# ==================================================================================
#                           Docker Composeç”ŸæˆåŠŸèƒ½
# ==================================================================================

# ç”Ÿæˆdocker-compose.yamlæ–‡ä»¶
update_existing_compose() {
	local output_file="$1"
	local custom_models="$2"
	local default_model="$3"

	log_info "æ›´æ–°ç°æœ‰docker-compose.yamlæ–‡ä»¶ä¸­çš„CUSTOM_MODELSé…ç½®"

	# åˆ›å»ºå¤‡ä»½
	local backup_file
	backup_file="${output_file}.backup.$(date +%Y%m%d_%H%M%S)"
	cp "${output_file}" "${backup_file}"
	log_info "å·²å¤‡ä»½æ‰€æœ‰æ–‡ä»¶: ${backup_file}"

	# ä½¿ç”¨Pythonè„šæœ¬æ›´æ–°CUSTOM_MODELSç¯å¢ƒå˜é‡
	if grep -q "CUSTOM_MODELS=" "${output_file}"; then
		# ä½¿ç”¨Pythonæ¥ç²¾ç¡®å¤„ç†YAMLæ–‡ä»¶ä¸­çš„å¤šè¡ŒCUSTOM_MODELS
		# ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶å­˜å‚¨å¤šè¡Œå†…å®¹
		local temp_models_file
		temp_models_file=$(mktemp)
		echo "${custom_models}" >"${temp_models_file}"

		# ä½¿ç”¨çº¯shellå®ç°æ›¿æ¢åŠŸèƒ½
		update_docker_compose_models() {
			local file_path="$1"
			local models_file="$2"
			local default_model="$3"

			# è¯»å–æ–°çš„æ¨¡å‹é…ç½®
			local new_models
			new_models=$(cat "${models_file}")

			# åˆ›å»ºä¸´æ—¶æ–‡ä»¶
			local temp_file
			temp_file=$(mktemp)

			# ç®€å•æ›¿æ¢CUSTOM_MODELSè¡Œ
			if grep -q 'CUSTOM_MODELS=' "${file_path}"; then
				# ä½¿ç”¨sedè¿›è¡Œç®€å•çš„è¡Œæ›¿æ¢
				sed "s|CUSTOM_MODELS=.*|CUSTOM_MODELS=${new_models}\"|" "${file_path}" >"${temp_file}"
				cp "${temp_file}" "${file_path}"
			fi

			# å¤„ç†DEFAULT_MODELæ›¿æ¢
			sed -E "s|(^[[:space:]]*-[[:space:]]*DEFAULT_MODEL=)[^[:space:]#]*(.*)|\\1${default_model}  # è‡ªåŠ¨è®¾ç½®ä¸ºmodels.listç¬¬ä¸€ä¸ªæ¨¡å‹|" "${file_path}" >"${temp_file}"
			cp "${temp_file}" "${file_path}"

			# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
			rm -f "${temp_file}"
			return 0
		}

		if update_docker_compose_models "${output_file}" "${temp_models_file}" "${default_model}"; then
			echo "SUCCESS"
		else
			echo "ERROR: Failed to update docker-compose.yaml"
			exit 1
		fi

		# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
		rm -f "${temp_models_file}"

		log_success "æˆåŠŸæ›´æ–°docker-compose.yamlä¸­çš„CUSTOM_MODELSé…ç½®"
		log_info "æ›´æ–°å†…å®¹: ${custom_models}"
	else
		log_error "æœªåœ¨docker-compose.yamlä¸­æ‰¾åˆ°CUSTOM_MODELSé…ç½®"
		return 1
	fi

	return 0
}

generate_docker_compose() {
	local output_file="${1:-./docker-compose.yaml}"
	local models_file="${MODELS_FILE:-./models.list}"

	# æ£€æŸ¥æ¨¡å‹åˆ—è¡¨æ–‡ä»¶æ˜¯å¦å­˜åœ¨
	if [[ ! -f ${models_file} ]]; then
		log_error "æ¨¡å‹åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: ${models_file}"
		return 1
	fi

	# æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨docker-compose.yamlæ–‡ä»¶
	if [[ -f ${output_file} ]]; then
		log_info "æ£€æµ‹åˆ°ç°æœ‰docker-compose.yamlæ–‡ä»¶ï¼Œå°†æ›´æ–°CUSTOM_MODELSé…ç½®"

		# ç”ŸæˆCUSTOM_MODELSå†…å®¹
		local custom_models_content
		custom_models_content=$(generate_custom_models_list "${models_file}")

		if [[ -z ${custom_models_content} ]]; then
			log_warning "æœªæ‰¾åˆ°æ¿€æ´»çš„æ¨¡å‹ï¼Œå°†ç”Ÿæˆé»˜è®¤é…ç½®"
			custom_models_content="-all"
		fi

		# æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„æ¨¡å‹
		if [[ ${custom_models_content} == "-all" ]]; then
			log_error "é”™è¯¯: models.list ä¸­æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹é…ç½®"
			log_error "è¯·ç¡®ä¿ models.list ä¸­è‡³å°‘æœ‰ä¸€ä¸ªæœªè¢«æ³¨é‡Šçš„æ¨¡å‹é…ç½®"
			return 1
		fi

		# è‡ªåŠ¨æ£€æµ‹é»˜è®¤æ¨¡å‹
		local default_model
		default_model=$(detect_default_model "${models_file}")

		[[ -n ${VERBOSE} ]] && log_info "Generated CUSTOM_MODELS content"
		[[ -n ${VERBOSE} ]] && log_info "Detected default model: ${default_model}"

		# æ›´æ–°ç°æœ‰æ–‡ä»¶
		update_existing_compose "${output_file}" "${custom_models_content}" "${default_model}"
	else
		log_info "Generating docker-compose.yaml based on model list: ${models_file}"

		# ç”ŸæˆCUSTOM_MODELSå†…å®¹
		local custom_models_content
		custom_models_content=$(generate_custom_models_list "${models_file}")

		if [[ -z ${custom_models_content} ]]; then
			log_warning "æœªæ‰¾åˆ°æ¿€æ´»çš„æ¨¡å‹ï¼Œå°†ç”Ÿæˆé»˜è®¤é…ç½®"
			custom_models_content="-all"
		fi

		# è‡ªåŠ¨æ£€æµ‹é»˜è®¤æ¨¡å‹
		local default_model
		default_model=$(detect_default_model "${models_file}")

		# æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„æ¨¡å‹ (CUSTOM_MODELSåªæœ‰-allè¯´æ˜æ²¡æœ‰æ¿€æ´»çš„æ¨¡å‹)
		if [[ ${custom_models_content} == "-all" ]]; then
			log_error "é”™è¯¯: models.list ä¸­æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹é…ç½®"
			log_error "è¯·ç¡®ä¿ models.list ä¸­è‡³å°‘æœ‰ä¸€ä¸ªæœªè¢«æ³¨é‡Šçš„æ¨¡å‹é…ç½®"
			return 1
		fi

		[[ -n ${VERBOSE} ]] && log_info "Generated CUSTOM_MODELS content"
		[[ -n ${VERBOSE} ]] && log_info "Detected default model: ${default_model}"

		# ç”Ÿæˆdocker-compose.yamlå†…å®¹
		generate_compose_content "${output_file}" "${custom_models_content}" "${default_model}"
	fi
}

# ç”ŸæˆCUSTOM_MODELSåˆ—è¡¨
generate_custom_models_list() {
	local models_file="$1"
	local custom_models_entries=()

	# æ·»åŠ  -all ä½œä¸ºç¬¬ä¸€ä¸ªæ¡ç›®ï¼ˆéšè—æ‰€æœ‰é»˜è®¤æ¨¡å‹ï¼‰
	custom_models_entries+=("-all")

	while IFS= read -r line || [[ -n ${line} ]]; do
		# è·³è¿‡æ³¨é‡Šè¡Œå’Œç©ºè¡Œ
		[[ ${line} =~ ^[[:space:]]*# ]] && continue
		[[ -z ${line// /} ]] && continue

		# è§£æè¡Œå†…å®¹
		read -r model_type model_spec _ <<<"${line}"

		case "${model_type}" in
		"ollama")
			if [[ -n ${model_spec} ]]; then
				local alias
				alias=$(generate_model_alias "${model_spec}" "ollama")
				local entry="+${model_spec}@OpenAI=${alias}"
				custom_models_entries+=("${entry}")
			fi
			;;
		"hf-gguf")
			if [[ -n ${model_spec} ]]; then
				local alias
				alias=$(generate_model_alias "${model_spec}" "hf-gguf")
				local entry="+${model_spec}@OpenAI=${alias}"
				custom_models_entries+=("${entry}")
			fi
			;;
		*)
			# å¿½ç•¥æœªçŸ¥çš„æ¨¡å‹ç±»å‹
			;;
		esac
	done <"${models_file}"

	# è¾“å‡ºCUSTOM_MODELSæ ¼å¼
	if [[ ${#custom_models_entries[@]} -gt 1 ]]; then
		printf '%s' "${custom_models_entries[0]}"
		for ((i = 1; i < ${#custom_models_entries[@]}; i++)); do
			printf ',\\\n        %s' "${custom_models_entries[i]}"
		done
	else
		echo "-all"
	fi
}

# ç”Ÿæˆç®€å•çš„æ¨¡å‹åˆ«å
generate_model_alias() {
	local model_spec="$1"
	local model_type="$2"

	# æ ¹æ®æ¨¡å‹ç±»å‹æå–å®é™…çš„æ¨¡å‹åç§°
	local model_name=""
	local model_version=""

	case "${model_type}" in
	"hf-gguf")
		# å¯¹äº hf-gguf æ¨¡å‹ï¼Œä»è·¯å¾„ä¸­æå–æ¨¡å‹åç§°
		# æ ¼å¼å¦‚: hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF:latest
		if [[ ${model_spec} =~ hf\.co/[^/]+/([^/:]+) ]]; then
			model_name="${BASH_REMATCH[1]}"
			# ç§»é™¤å¸¸è§çš„ GGUF åç¼€
			model_name=$(echo "${model_name}" | sed 's/-GGUF$//' | sed 's/_GGUF$//')
		fi
		;;
	*)
		# å¯¹äº ollama å’Œå…¶ä»–ç±»å‹ï¼Œä½¿ç”¨åŸºç¡€åç§°
		model_name="${model_spec%:*}"
		;;
	esac

	# ä»æ¨¡å‹è§„æ ¼ä¸­æå–ç‰ˆæœ¬ä¿¡æ¯
	if [[ ${model_spec} =~ :(.+)$ ]]; then
		model_version="${BASH_REMATCH[1]}"
	fi

	# å¦‚æœæ²¡æœ‰æå–åˆ°æ¨¡å‹åç§°ï¼Œä½¿ç”¨ç±»å‹ä½œä¸ºåå¤‡
	if [[ -z ${model_name} ]]; then
		model_name="${model_type}"
	fi

	# æ¸…ç†æ¨¡å‹åç§°å’Œç‰ˆæœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦
	local clean_name
	clean_name=$(echo "${model_name}" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9]/-/g' | sed 's/--*/-/g' | sed 's/^-\|-$//g')

	if [[ -n ${model_version} && ${model_version} != "latest" ]]; then
		local clean_version
		clean_version=$(echo "${model_version}" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9.]/-/g' | sed 's/--*/-/g' | sed 's/^-\|-$//g')
		echo "${clean_name}-${clean_version}"
	else
		echo "${clean_name}"
	fi
}

# æ£€æµ‹é»˜è®¤æ¨¡å‹
detect_default_model() {
	local models_file="$1"
	local first_active_model=""

	while IFS= read -r line || [[ -n ${line} ]]; do
		# è·³è¿‡æ³¨é‡Šè¡Œå’Œç©ºè¡Œ
		[[ ${line} =~ ^[[:space:]]*# ]] && continue
		[[ -z ${line// /} ]] && continue

		# è§£æè¡Œå†…å®¹
		read -r model_type model_spec _ <<<"${line}"

		# æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ¿€æ´»çš„æ¨¡å‹å¹¶ç”Ÿæˆå…¶åˆ«å
		if [[ -n ${model_spec} && -z ${first_active_model} ]]; then
			case "${model_type}" in
			"ollama")
				first_active_model=$(generate_model_alias "${model_spec}" "ollama")
				break
				;;
			"hf-gguf")
				first_active_model=$(generate_model_alias "${model_spec}" "hf-gguf")
				break
				;;
			*)
				# å¿½ç•¥æœªçŸ¥çš„æ¨¡å‹ç±»å‹
				;;
			esac
		fi
	done <"${models_file}"

	# å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ¿€æ´»çš„æ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤å€¼
	echo "${first_active_model:-qwen3-14b}"
}

# ç”Ÿæˆdocker-compose.yamlæ–‡ä»¶å†…å®¹
detect_gpus() {
	local gpu_indices=""

	if command -v nvidia-smi &>/dev/null; then
		gpu_indices=$(nvidia-smi --query-gpu=index --format=csv,noheader,nounits 2>/dev/null | tr '\n' ',' | sed 's/,$//')
		if [[ -n ${gpu_indices} ]]; then
			echo "${gpu_indices}"
		else
			echo "0,1,2,3"
		fi
	else
		echo "0,1,2,3"
	fi
}

generate_compose_content() {
	local output_file="$1"
	local custom_models="$2"
	local default_model="$3"

	local cuda_devices
	cuda_devices=$(detect_gpus)

	# è·å–ä¸»æœºæ—¶åŒº
	local host_timezone
	host_timezone=$(get_host_timezone)
	[[ -z ${host_timezone} ]] && host_timezone="UTC"

	# å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆ›å»ºå¤‡ä»½
	if [[ -f ${output_file} ]]; then
		local backup_file
		backup_file="${output_file}.backup.$(date +%Y%m%d_%H%M%S)"
		cp "${output_file}" "${backup_file}"
		log_info "å·²å¤‡ä»½æ‰€æœ‰æ–‡ä»¶: ${backup_file}"
	fi

	# ç”Ÿæˆdocker-compose.yamlå†…å®¹
	cat >"${output_file}" <<EOF
services:
  ollama:
    image: ${DOCKER_IMAGE_OLLAMA}
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ./ollama:/root/.ollama
    networks:
      - llms-tools-network
    environment:
      # Ollamaä¼˜åŒ–é…ç½®
      - CUDA_VISIBLE_DEVICES=${cuda_devices} # è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU
      - OLLAMA_NEW_ENGINE=1 # æ–°çš„å¼•æ“, ollamarunner
      - OLLAMA_SCHED_SPREAD=1 # å¯ç”¨å¤šGPUè´Ÿè½½å‡è¡¡
      - OLLAMA_KEEP_ALIVE=5m # æ¨¡å‹åœ¨å†…å­˜ä¸­ä¿æŒåŠ è½½çš„æ—¶é•¿, åˆ†é’Ÿ
      - OLLAMA_NUM_PARALLEL=3 # å¹¶å‘è¯·æ±‚æ•°
      - OLLAMA_FLASH_ATTENTION=1 # flash attention, ç”¨äºä¼˜åŒ–æ³¨æ„åŠ›è®¡ç®—, é™ä½æ˜¾å­˜ä½¿ç”¨
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    command: [ "serve"]
    restart: unless-stopped

  one-api:
    image: ${DOCKER_IMAGE_ONE_API}
    container_name: one-api
    volumes:
      - ./one-api:/data
    networks:
      - llms-tools-network
    ports:
      - "3001:3001"
    depends_on:
      - ollama
    environment:
      - TZ=${host_timezone}
      - SESSION_SECRET=random_string
    command: [ "--port", "3001" ]
    restart: unless-stopped

  prompt-optimizer:
    image: ${DOCKER_IMAGE_PROMPT_OPTIMIZER}
    container_name: prompt-optimizer
    ports:
      - "8501:80"
    environment:
      - VITE_CUSTOM_API_BASE_URL=http://YOUR_SERVER_IP:3001/v1  # ä¿®æ”¹ä¸ºä½ çš„æœåŠ¡å™¨IPåœ°å€
      - VITE_CUSTOM_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx  # ä¿®æ”¹ä¸ºä½ çš„APIå¯†é’¥
      - VITE_CUSTOM_API_MODEL=${default_model}  # è‡ªåŠ¨è®¾ç½®ä¸ºmodels.listç¬¬ä¸€ä¸ªæ¨¡å‹
      - ACCESS_USERNAME=admin  # ä¿®æ”¹ä¸ºä½ çš„ç”¨æˆ·å
      - ACCESS_PASSWORD=xxxxxxxxxxxxxxxxxxxxxx  # ä¿®æ”¹ä¸ºä½ çš„å¯†ç 
    networks:
      - llms-tools-network
    depends_on:
      - one-api
    restart: unless-stopped

  chatgpt-next-web:
    image: ${DOCKER_IMAGE_CHATGPT_NEXT_WEB}
    container_name: chatgpt-next-web
    ports:
      - "3000:3000"
    environment:
      - OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx  # ä¿®æ”¹ä¸ºä½ çš„OpenAI APIå¯†é’¥
      - BASE_URL=http://one-api:3001
      - PROXY_URL=
      - "CUSTOM_MODELS=${custom_models}"
      - DEFAULT_MODEL=${default_model}  # è‡ªåŠ¨è®¾ç½®ä¸ºmodels.listç¬¬ä¸€ä¸ªæ¨¡å‹
      - CODE=xxxxxxxxxxxxxxxxxxxxxx  # ä¿®æ”¹ä¸ºä½ çš„è®¿é—®å¯†ç 
    networks:
      - llms-tools-network
    depends_on:
      - one-api
    restart: unless-stopped

networks:
  llms-tools-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.28.0/24
EOF

	log_success "æˆåŠŸç”Ÿæˆdocker-compose.yamlæ–‡ä»¶: ${output_file}"
	log_info "åŒ…å«æ¨¡å‹é…ç½®: ${custom_models}"
	log_info "é»˜è®¤æ¨¡å‹: ${default_model}"
	log_info "æ£€æµ‹åˆ°GPUè®¾å¤‡: ${cuda_devices}"
	echo ""
	log_info "âš ï¸  é‡è¦æç¤º: ç”Ÿæˆçš„é…ç½®æ–‡ä»¶ä¸­åŒ…å«å ä½ç¬¦ï¼Œè¯·æ ¹æ®ä»¥ä¸‹è¯´æ˜ä¿®æ”¹ï¼š"
	log_info "== å¿…é¡»ä¿®æ”¹çš„é…ç½® =="
	log_info "1. VITE_CUSTOM_API_BASE_URL: å°† YOUR_SERVER_IP æ›¿æ¢ä¸ºå®é™…æœåŠ¡å™¨IPåœ°å€"
	log_info "2. VITE_CUSTOM_API_KEY: æ›¿æ¢ä¸º one-api ä¸­çš„æœ‰æ•ˆAPIå¯†é’¥"
	log_info "3. ACCESS_USERNAME/ACCESS_PASSWORD: è®¾ç½® prompt-optimizer çš„ç™»å½•å‡­æ®"
	log_info "4. OPENAI_API_KEY: æ›¿æ¢ä¸º one-api ä¸­çš„æœ‰æ•ˆAPIå¯†é’¥"
	log_info "5. CODE: è®¾ç½® ChatGPT-Next-Web çš„è®¿é—®å¯†ç "
	log_info "6. VITE_CUSTOM_API_MODEL/DEFAULT_MODEL: å·²è‡ªåŠ¨è®¾ç½®ä¸º ${default_model}ï¼Œå¯æ ¹æ®éœ€è¦ä¿®æ”¹"
	echo ""
	log_info "== å¯é€‰ä¿®æ”¹çš„é…ç½® =="
	log_info "â€¢ ç«¯å£æ˜ å°„: å¦‚éœ€é¿å…ç«¯å£å†²çªï¼Œå¯ä¿®æ”¹ ports éƒ¨åˆ†çš„ä¸»æœºç«¯å£"
	log_info "  - Ollama: 11434 -> è‡ªå®šä¹‰ç«¯å£"
	log_info "  - One-API: 3001 -> è‡ªå®šä¹‰ç«¯å£"
	log_info "  - Prompt-Optimizer: 8501 -> è‡ªå®šä¹‰ç«¯å£"
	log_info "  - ChatGPT-Next-Web: 3000 -> è‡ªå®šä¹‰ç«¯å£"
	log_info "â€¢ Dockeré•œåƒ: å¦‚éœ€ä½¿ç”¨ç‰¹å®šç‰ˆæœ¬ï¼Œå¯ä¿®æ”¹ image éƒ¨åˆ†çš„æ ‡ç­¾"
	log_info "â€¢ ç½‘ç»œé…ç½®: å¯ä¿®æ”¹ subnet ä»¥é¿å…IPåœ°å€å†²çª"
	echo ""
	log_info "é…ç½®å®Œæˆåè¿è¡Œ: docker compose up -d æ¥å¯åŠ¨æœåŠ¡"

	return 0
}

# åªæœ‰åœ¨ç›´æ¥è¿è¡Œè„šæœ¬æ—¶æ‰æ‰§è¡Œmainå‡½æ•°
if [[ ${BASH_SOURCE[0]:-$0} == "${0}" ]]; then
	main "$@"
fi
